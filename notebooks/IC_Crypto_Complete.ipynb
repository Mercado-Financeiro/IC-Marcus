{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ofpjqul08a",
   "source": "# ğŸš€ PRD Implementation - Componentes Alinhados com PRD Marcus\n\nEsta seÃ§Ã£o implementa os componentes crÃ­ticos especificados no PRD:\n1. **Great Expectations** - ValidaÃ§Ã£o de qualidade de dados\n2. **Binance Public Data** - IngestÃ£o otimizada de dados histÃ³ricos  \n3. **Vectorbt** - Backtesting vetorizado com custos reais\n4. **CalibraÃ§Ã£o de Probabilidades** - Isotonic/Sigmoid calibration\n5. **Threshold por EV** - OtimizaÃ§Ã£o baseada em Expected Value\n6. **Wavelets** - Features de decomposiÃ§Ã£o multi-resoluÃ§Ã£o",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "g690435wpjv",
   "source": "# Importar novos mÃ³dulos PRD\nimport sys\nfrom pathlib import Path\n\n# Adicionar path do projeto\nproject_root = Path.cwd().parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Imports dos novos mÃ³dulos PRD\ntry:\n    # Data validation\n    from scripts.validate.ge_checks import CryptoDataValidator, validate_crypto_data\n    print(\"âœ… Great Expectations importado\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Great Expectations nÃ£o disponÃ­vel: {e}\")\n    \ntry:\n    # Binance data fetcher\n    from scripts.fetch.binance_klines import BinanceKlinesFetcher\n    print(\"âœ… Binance Klines Fetcher importado\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Binance Fetcher nÃ£o disponÃ­vel: {e}\")\n    \ntry:\n    # Vectorbt backtesting\n    from src.backtest.vectorbt_runner import VectorbtBacktester, backtest_strategy\n    print(\"âœ… Vectorbt Backtester importado\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Vectorbt nÃ£o disponÃ­vel: {e}\")\n    \ntry:\n    # Calibration and EV threshold\n    from src.models.calibration import (\n        ProbabilityCalibrator, \n        EVThresholdOptimizer,\n        calibrate_and_optimize\n    )\n    print(\"âœ… CalibraÃ§Ã£o e EV Threshold importados\")\nexcept ImportError as e:\n    print(f\"âš ï¸ CalibraÃ§Ã£o nÃ£o disponÃ­vel: {e}\")\n    \ntry:\n    # Wavelet features\n    from src.features.wavelets import (\n        WaveletFeatures,\n        MultiResolutionAnalysis,\n        add_wavelet_features\n    )\n    print(\"âœ… Wavelet Features importado\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Wavelets nÃ£o disponÃ­vel: {e}\")\n\nprint(\"\\nğŸ“Š MÃ³dulos PRD carregados com sucesso!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "k05sc2alsr",
   "source": "## 1. Data Validation com Great Expectations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v4m9410uko",
   "source": "# ValidaÃ§Ã£o de dados com Great Expectations\ndef validate_data_quality(df: pd.DataFrame, symbol: str = \"BTCUSDT\", timeframe: str = \"15m\"):\n    \"\"\"\n    Valida qualidade dos dados OHLCV conforme PRD.\n    \"\"\"\n    print(f\"ğŸ” Validando dados para {symbol} {timeframe}...\")\n    \n    try:\n        # Validar com Great Expectations\n        is_valid, report = validate_crypto_data(df, symbol, timeframe)\n        \n        # Mostrar relatÃ³rio\n        print(\"\\nğŸ“Š RelatÃ³rio de ValidaÃ§Ã£o:\")\n        print(f\"  - Status: {'âœ… PASSOU' if is_valid else 'âŒ FALHOU'}\")\n        print(f\"  - Linhas: {len(df)}\")\n        print(f\"  - PerÃ­odo: {df.index.min()} atÃ© {df.index.max()}\")\n        \n        # Checks estruturais\n        structural = report.get('structural_checks', {})\n        print(f\"\\n  Checks Estruturais:\")\n        print(f\"    - Duplicatas: {structural.get('duplicates', 0)}\")\n        print(f\"    - Gaps: {structural.get('gaps', 0)}\")\n        print(f\"    - MonotÃ´nico: {structural.get('is_monotonic', False)}\")\n        \n        # ValidaÃ§Ã£o GE\n        validation = report.get('validation_results', {})\n        if validation:\n            print(f\"\\n  Great Expectations:\")\n            print(f\"    - Sucesso: {validation.get('success', False)}\")\n            print(f\"    - Falhas: {validation.get('failed_expectations', 0)}/{validation.get('total_expectations', 0)}\")\n        \n        # Mostrar falhas especÃ­ficas se houver\n        if not is_valid and 'failures' in report:\n            print(\"\\n  âš ï¸ Falhas EspecÃ­ficas:\")\n            for failure in report['failures'][:5]:  # Mostrar atÃ© 5 falhas\n                print(f\"    - {failure['expectation']}\")\n                \n        return is_valid, report\n        \n    except Exception as e:\n        print(f\"âŒ Erro na validaÃ§Ã£o: {e}\")\n        return False, {}\n\n# Testar com dados existentes se disponÃ­vel\nif 'df_15m' in locals():\n    is_valid, validation_report = validate_data_quality(df_15m.head(1000))\nelse:\n    print(\"âš ï¸ Carregue dados primeiro (df_15m) para testar validaÃ§Ã£o\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rqaahdmcsk9",
   "source": "## 2. Wavelet Features para DecomposiÃ§Ã£o Multi-ResoluÃ§Ã£o",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7achk2nvfsr",
   "source": "# Adicionar Wavelet Features aos dados\ndef add_wavelet_features_to_data(df: pd.DataFrame, optimize_memory: bool = True):\n    \"\"\"\n    Adiciona features de wavelet para decomposiÃ§Ã£o e denoising.\n    PRD: Wavelets como extensÃ£o para features.\n    \"\"\"\n    print(\"ğŸŒŠ Adicionando Wavelet Features...\")\n    \n    # Copiar dataframe\n    df_wavelets = df.copy()\n    \n    # Inicializar extrator de wavelets\n    wf = WaveletFeatures(\n        wavelet='db4',  # Daubechies 4 - bom para dados financeiros\n        levels=4,        # 4 nÃ­veis de decomposiÃ§Ã£o\n        denoising=True,  # Incluir denoising\n        feature_mode='statistics'  # EstatÃ­sticas dos coeficientes\n    )\n    \n    # Adicionar features para cada coluna OHLC\n    for col in ['open', 'high', 'low', 'close']:\n        if col in df.columns:\n            print(f\"  - Processando {col}...\")\n            \n            # Extrair features com janela rolante\n            wavelet_feats = wf.transform_rolling(\n                df[col],\n                window=64,  # ~16 horas em dados de 15min\n                min_periods=32\n            )\n            \n            # Adicionar ao dataframe\n            for feat_col in wavelet_feats.columns:\n                new_col_name = f'{col}_{feat_col}'\n                df_wavelets[new_col_name] = wavelet_feats[feat_col]\n    \n    # Multi-resoluÃ§Ã£o para close\n    print(\"  - AnÃ¡lise multi-resoluÃ§Ã£o...\")\n    mra = MultiResolutionAnalysis(\n        wavelets=['db4', 'sym4'],  # MÃºltiplos wavelets\n        max_level=3\n    )\n    \n    # Aplicar com janela rolante (mais eficiente)\n    window_size = 64\n    for i in range(window_size, len(df)):\n        window_data = df['close'].iloc[i-window_size:i]\n        mra_features = mra.analyze(window_data)\n        \n        # Adicionar features\n        for col in mra_features.columns:\n            if col not in df_wavelets.columns:\n                df_wavelets[col] = np.nan\n            df_wavelets.loc[df.index[i-1], col] = mra_features[col].values[0]\n    \n    # Preencher NaNs iniciais\n    df_wavelets.fillna(method='ffill', inplace=True)\n    \n    # OtimizaÃ§Ã£o de memÃ³ria\n    if optimize_memory:\n        # Converter para float32\n        float_cols = df_wavelets.select_dtypes(include=['float64']).columns\n        df_wavelets[float_cols] = df_wavelets[float_cols].astype(np.float32)\n    \n    # EstatÃ­sticas\n    n_wavelet_features = len([c for c in df_wavelets.columns if 'wavelet' in c or 'db4' in c or 'sym4' in c])\n    print(f\"\\nâœ… Adicionadas {n_wavelet_features} features de wavelet\")\n    print(f\"   MemÃ³ria: {df_wavelets.memory_usage().sum() / 1024**2:.2f} MB\")\n    \n    return df_wavelets\n\n# Exemplo de uso (descomente se tiver dados)\n# if 'df_15m' in locals():\n#     df_with_wavelets = add_wavelet_features_to_data(df_15m.head(500))\n#     print(f\"Features totais: {len(df_with_wavelets.columns)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "og4mfu2xtbf",
   "source": "## 3. CalibraÃ§Ã£o de Probabilidades e Threshold por EV",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dji31wrm21g",
   "source": "# Pipeline de CalibraÃ§Ã£o e OtimizaÃ§Ã£o de Threshold\nclass PRDCalibrationPipeline:\n    \"\"\"\n    Pipeline completo de calibraÃ§Ã£o e threshold optimization conforme PRD.\n    \"\"\"\n    \n    def __init__(self, fee_bps: float = 10.0, slippage_bps: float = 10.0):\n        self.fee_bps = fee_bps\n        self.slippage_bps = slippage_bps\n        self.calibrator = None\n        self.optimizer = None\n        self.results = {}\n        \n    def fit_and_optimize(self, y_prob: np.ndarray, y_true: np.ndarray, \n                         calibration_method: str = 'isotonic',\n                         plot: bool = True):\n        \"\"\"\n        Calibra probabilidades e otimiza threshold por EV.\n        \"\"\"\n        print(\"ğŸ¯ Pipeline de CalibraÃ§Ã£o e OtimizaÃ§Ã£o PRD\")\n        print(\"=\" * 50)\n        \n        # Step 1: CalibraÃ§Ã£o\n        print(\"\\n1ï¸âƒ£ Calibrando probabilidades...\")\n        self.calibrator = ProbabilityCalibrator(method=calibration_method)\n        y_prob_cal = self.calibrator.fit_transform(y_prob, y_true)\n        \n        # MÃ©tricas de calibraÃ§Ã£o\n        print(f\"   Brier Score:\")\n        print(f\"     Antes: {self.calibrator.metrics['brier_score_before']:.4f}\")\n        print(f\"     Depois: {self.calibrator.metrics['brier_score_after']:.4f}\")\n        print(f\"     Melhoria: {(self.calibrator.metrics['brier_score_before'] - self.calibrator.metrics['brier_score_after']) / self.calibrator.metrics['brier_score_before'] * 100:.1f}%\")\n        \n        # Step 2: OtimizaÃ§Ã£o de Threshold\n        print(\"\\n2ï¸âƒ£ Otimizando threshold por Expected Value...\")\n        self.optimizer = EVThresholdOptimizer(\n            fee_bps=self.fee_bps,\n            slippage_bps=self.slippage_bps,\n            win_return=0.01,  # 1% retorno mÃ©dio\n            loss_return=0.01   # 1% perda mÃ©dia\n        )\n        \n        optimal_threshold, threshold_metrics = self.optimizer.optimize_threshold(\n            y_prob_cal, y_true\n        )\n        \n        # Resultados\n        best_metrics = threshold_metrics[threshold_metrics['threshold'] == optimal_threshold].iloc[0]\n        \n        print(f\"\\n   ğŸ“Š Threshold Ã“timo: {optimal_threshold:.3f}\")\n        print(f\"   ğŸ’° EV por Trade: {best_metrics['ev_per_trade']:.6f}\")\n        print(f\"   ğŸ“ˆ Precision: {best_metrics['precision']:.3f}\")\n        print(f\"   ğŸ“‰ Recall: {best_metrics['recall']:.3f}\")\n        print(f\"   ğŸ¯ F1 Score: {best_metrics['f1']:.3f}\")\n        print(f\"   ğŸ”„ NÃºmero de Trades: {best_metrics['n_trades']:.0f}\")\n        print(f\"   ğŸ’µ Lucro LÃ­quido Estimado: {best_metrics['net_profit']:.4f}\")\n        \n        # ComparaÃ§Ã£o com threshold padrÃ£o (0.5)\n        default_metrics = threshold_metrics[threshold_metrics['threshold'] == 0.5].iloc[0]\n        ev_improvement = (best_metrics['ev_per_trade'] - default_metrics['ev_per_trade']) / abs(default_metrics['ev_per_trade']) * 100 if default_metrics['ev_per_trade'] != 0 else 0\n        \n        print(f\"\\n   ğŸ”„ ComparaÃ§Ã£o com Threshold 0.5:\")\n        print(f\"      EV Melhoria: {ev_improvement:.1f}%\")\n        print(f\"      Trades: {best_metrics['n_trades']:.0f} vs {default_metrics['n_trades']:.0f}\")\n        \n        # Plots\n        if plot:\n            self._plot_results(y_prob, y_prob_cal, y_true, threshold_metrics)\n        \n        # Guardar resultados\n        self.results = {\n            'y_prob_original': y_prob,\n            'y_prob_calibrated': y_prob_cal,\n            'optimal_threshold': optimal_threshold,\n            'threshold_metrics': threshold_metrics,\n            'calibration_metrics': self.calibrator.metrics,\n            'best_metrics': best_metrics.to_dict()\n        }\n        \n        return y_prob_cal, optimal_threshold\n    \n    def _plot_results(self, y_prob, y_prob_cal, y_true, threshold_metrics):\n        \"\"\"Gera visualizaÃ§Ãµes dos resultados.\"\"\"\n        \n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        \n        # 1. Calibration curve\n        ax = axes[0, 0]\n        from sklearn.calibration import calibration_curve\n        \n        fraction_pos, mean_pred = calibration_curve(y_true, y_prob, n_bins=10)\n        fraction_pos_cal, mean_pred_cal = calibration_curve(y_true, y_prob_cal, n_bins=10)\n        \n        ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfeita')\n        ax.plot(mean_pred, fraction_pos, 'o-', label='Original', alpha=0.7)\n        ax.plot(mean_pred_cal, fraction_pos_cal, 's-', label='Calibrada', alpha=0.7)\n        ax.set_xlabel('Probabilidade MÃ©dia Predita')\n        ax.set_ylabel('FraÃ§Ã£o de Positivos')\n        ax.set_title('Curva de CalibraÃ§Ã£o')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # 2. Distribution comparison\n        ax = axes[0, 1]\n        ax.hist(y_prob, bins=30, alpha=0.5, label='Original', density=True)\n        ax.hist(y_prob_cal, bins=30, alpha=0.5, label='Calibrada', density=True)\n        ax.set_xlabel('Probabilidade')\n        ax.set_ylabel('Densidade')\n        ax.set_title('DistribuiÃ§Ã£o de Probabilidades')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # 3. EV vs Threshold\n        ax = axes[1, 0]\n        ax.plot(threshold_metrics['threshold'], threshold_metrics['ev_per_trade'], 'b-', linewidth=2)\n        ax.axvline(self.optimizer.optimal_threshold, color='r', linestyle='--', \n                   label=f'Ã“timo: {self.optimizer.optimal_threshold:.3f}', linewidth=2)\n        ax.axhline(0, color='k', linestyle='-', alpha=0.3)\n        ax.set_xlabel('Threshold')\n        ax.set_ylabel('EV por Trade')\n        ax.set_title('Expected Value vs Threshold')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # 4. Trade metrics\n        ax = axes[1, 1]\n        ax2 = ax.twinx()\n        \n        l1 = ax.plot(threshold_metrics['threshold'], threshold_metrics['n_trades'], \n                     'orange', label='NÃºmero de Trades', linewidth=2)\n        l2 = ax2.plot(threshold_metrics['threshold'], threshold_metrics['precision'], \n                      'green', label='Precision', linewidth=2)\n        \n        ax.axvline(self.optimizer.optimal_threshold, color='r', linestyle='--', alpha=0.5)\n        ax.set_xlabel('Threshold')\n        ax.set_ylabel('NÃºmero de Trades', color='orange')\n        ax2.set_ylabel('Precision', color='green')\n        ax.set_title('Trade Count e Precision')\n        \n        # Combinar legendas\n        lns = l1 + l2\n        labs = [l.get_label() for l in lns]\n        ax.legend(lns, labs, loc='upper right')\n        ax.grid(True, alpha=0.3)\n        \n        plt.suptitle('Pipeline de CalibraÃ§Ã£o e OtimizaÃ§Ã£o PRD', fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n# Exemplo de uso\ndef test_calibration_pipeline():\n    \"\"\"Testa o pipeline com dados sintÃ©ticos.\"\"\"\n    \n    # Gerar dados de exemplo\n    np.random.seed(42)\n    n_samples = 1000\n    \n    # Labels verdadeiros (30% positivos)\n    y_true = np.random.binomial(1, 0.3, n_samples)\n    \n    # Probabilidades nÃ£o calibradas (com viÃ©s sistemÃ¡tico)\n    y_prob = np.clip(\n        y_true * 0.6 + np.random.beta(2, 5, n_samples) * 0.4 + 0.1,\n        0, 1\n    )\n    \n    # Executar pipeline\n    pipeline = PRDCalibrationPipeline(fee_bps=10, slippage_bps=10)\n    y_prob_cal, optimal_threshold = pipeline.fit_and_optimize(\n        y_prob, y_true, \n        calibration_method='isotonic',\n        plot=True\n    )\n    \n    return pipeline\n\n# Executar teste\nprint(\"ğŸ§ª Testando Pipeline de CalibraÃ§Ã£o PRD...\")\ntest_pipeline = test_calibration_pipeline()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6sxv0ng6hfb",
   "source": "## 4. Backtesting Vetorizado com Vectorbt",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nnlmritcqhm",
   "source": "# Backtesting com Vectorbt - PRD Section 11\nclass PRDVectorbtBacktest:\n    \"\"\"\n    Backtester vetorizado conforme especificaÃ§Ãµes do PRD.\n    \"\"\"\n    \n    def __init__(self, fee_bps: float = 10.0, slippage_bps: float = 10.0):\n        self.fee_bps = fee_bps\n        self.slippage_bps = slippage_bps\n        self.backtester = VectorbtBacktester(\n            fee_bps=fee_bps,\n            slippage_bps=slippage_bps,\n            init_cash=10000.0,\n            size_type='percent',\n            size=0.95  # Usar 95% do capital disponÃ­vel\n        )\n        \n    def run_complete_backtest(self, prices: pd.DataFrame, predictions: np.ndarray, \n                              threshold: float = 0.5, plot: bool = True):\n        \"\"\"\n        Executa backtest completo com mÃ©tricas PRD.\n        \"\"\"\n        print(\"ğŸ“ˆ Executando Backtest Vetorizado (Vectorbt)\")\n        print(\"=\" * 50)\n        \n        # Converter predictions para Series se necessÃ¡rio\n        if isinstance(predictions, np.ndarray):\n            predictions = pd.Series(predictions, index=prices.index[:len(predictions)])\n        \n        # Alinhar dados\n        common_idx = prices.index.intersection(predictions.index)\n        prices_aligned = prices.loc[common_idx]\n        predictions_aligned = predictions.loc[common_idx]\n        \n        # Preparar sinais\n        signals = self.backtester.prepare_signals(\n            predictions_aligned, \n            threshold, \n            prices_aligned\n        )\n        \n        # Executar backtest\n        portfolio = self.backtester.run_backtest(\n            prices_aligned, \n            signals, \n            column='close'\n        )\n        \n        # Calcular mÃ©tricas\n        metrics = self.backtester.calculate_metrics(portfolio)\n        \n        # Exibir resultados\n        self._display_results(metrics)\n        \n        # Plots\n        if plot:\n            self._plot_backtest(portfolio, signals, prices_aligned)\n            \n        return portfolio, metrics\n    \n    def walk_forward_analysis(self, prices: pd.DataFrame, predictions: pd.Series,\n                             window_size: int = 1000, test_size: int = 200,\n                             threshold: float = 0.5):\n        \"\"\"\n        Walk-forward analysis conforme PRD.\n        \"\"\"\n        print(\"\\nğŸš¶ Walk-Forward Analysis\")\n        print(\"=\" * 50)\n        \n        portfolios, metrics_df = self.backtester.walk_forward_backtest(\n            prices, predictions, window_size, test_size, threshold\n        )\n        \n        # EstatÃ­sticas agregadas\n        print(f\"\\nğŸ“Š Resultados Walk-Forward ({len(portfolios)} janelas):\")\n        print(f\"   Sharpe MÃ©dio: {metrics_df['sharpe_ratio'].mean():.2f} Â± {metrics_df['sharpe_ratio'].std():.2f}\")\n        print(f\"   Retorno MÃ©dio: {metrics_df['total_return'].mean():.2%} Â± {metrics_df['total_return'].std():.2%}\")\n        print(f\"   MaxDD MÃ©dio: {metrics_df['max_drawdown'].mean():.2%}\")\n        print(f\"   Win Rate MÃ©dio: {metrics_df['win_rate'].mean():.2%}\")\n        print(f\"   EV/Trade MÃ©dio: {metrics_df['ev_after_costs'].mean():.4f}\")\n        print(f\"   Total de Trades: {metrics_df['n_trades'].sum():.0f}\")\n        \n        # Plot evoluÃ§Ã£o\n        fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n        \n        # Sharpe ao longo do tempo\n        ax = axes[0, 0]\n        ax.plot(range(1, len(metrics_df) + 1), metrics_df['sharpe_ratio'], 'b-o')\n        ax.axhline(metrics_df['sharpe_ratio'].mean(), color='r', linestyle='--', alpha=0.7)\n        ax.set_xlabel('Janela')\n        ax.set_ylabel('Sharpe Ratio')\n        ax.set_title('EvoluÃ§Ã£o do Sharpe Ratio')\n        ax.grid(True, alpha=0.3)\n        \n        # Retornos\n        ax = axes[0, 1]\n        ax.bar(range(1, len(metrics_df) + 1), metrics_df['total_return'])\n        ax.axhline(0, color='k', linestyle='-', alpha=0.3)\n        ax.set_xlabel('Janela')\n        ax.set_ylabel('Retorno Total')\n        ax.set_title('Retornos por Janela')\n        ax.grid(True, alpha=0.3)\n        \n        # EV por trade\n        ax = axes[1, 0]\n        ax.plot(range(1, len(metrics_df) + 1), metrics_df['ev_after_costs'], 'g-s')\n        ax.axhline(0, color='k', linestyle='-', alpha=0.3)\n        ax.axhline(metrics_df['ev_after_costs'].mean(), color='r', linestyle='--', alpha=0.7)\n        ax.set_xlabel('Janela')\n        ax.set_ylabel('EV apÃ³s Custos')\n        ax.set_title('Expected Value por Trade')\n        ax.grid(True, alpha=0.3)\n        \n        # NÃºmero de trades\n        ax = axes[1, 1]\n        ax.bar(range(1, len(metrics_df) + 1), metrics_df['n_trades'], color='orange')\n        ax.set_xlabel('Janela')\n        ax.set_ylabel('NÃºmero de Trades')\n        ax.set_title('Atividade de Trading')\n        ax.grid(True, alpha=0.3)\n        \n        plt.suptitle('Walk-Forward Analysis - PRD Compliant', fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        \n        return portfolios, metrics_df\n    \n    def _display_results(self, metrics: dict):\n        \"\"\"Exibe resultados formatados.\"\"\"\n        \n        print(f\"\\nğŸ“Š MÃ©tricas do Backtest:\")\n        print(f\"   Retorno Total: {metrics['total_return']:.2%}\")\n        print(f\"   Retorno Anualizado: {metrics['annual_return']:.2%}\")\n        print(f\"   Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n        print(f\"   Sortino Ratio: {metrics['sortino_ratio']:.2f}\")\n        print(f\"   Calmar Ratio: {metrics['calmar_ratio']:.2f}\")\n        print(f\"   Max Drawdown: {metrics['max_drawdown']:.2%}\")\n        \n        print(f\"\\nğŸ’° MÃ©tricas de Trading:\")\n        print(f\"   NÃºmero de Trades: {metrics['n_trades']:.0f}\")\n        print(f\"   Win Rate: {metrics['win_rate']:.2%}\")\n        print(f\"   Profit Factor: {metrics['profit_factor']:.2f}\")\n        print(f\"   Avg Win: {metrics['avg_win']:.2%}\")\n        print(f\"   Avg Loss: {metrics['avg_loss']:.2%}\")\n        \n        print(f\"\\nğŸ“ˆ Expected Value (PRD Critical):\")\n        print(f\"   EV por Trade: {metrics['ev_per_trade']:.4f}\")\n        print(f\"   EV apÃ³s Custos: {metrics['ev_after_costs']:.4f}\")\n        print(f\"   Total de Fees: {metrics['total_fees']:.2f}\")\n        print(f\"   Fee por Trade: {metrics['fee_per_trade']:.4f}\")\n        \n        print(f\"\\nğŸ’µ Resultado Final:\")\n        print(f\"   Capital Inicial: $10,000\")\n        print(f\"   Capital Final: ${metrics['final_value']:.2f}\")\n        print(f\"   Lucro Total: ${metrics['total_profit']:.2f}\")\n        \n    def _plot_backtest(self, portfolio, signals, prices):\n        \"\"\"Gera plots do backtest.\"\"\"\n        \n        # Criar figura com subplots\n        fig, axes = plt.subplots(3, 1, figsize=(14, 10), height_ratios=[2, 1, 1])\n        \n        # 1. Equity curve\n        ax = axes[0]\n        equity = portfolio.value()\n        equity.plot(ax=ax, label='Portfolio Value', linewidth=2)\n        ax.axhline(10000, color='k', linestyle='--', alpha=0.3, label='Initial Capital')\n        ax.set_ylabel('Portfolio Value ($)')\n        ax.set_title('Equity Curve')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # 2. Drawdown\n        ax = axes[1]\n        drawdown = portfolio.drawdown()\n        drawdown.plot(ax=ax, color='red', linewidth=1.5)\n        ax.fill_between(drawdown.index, 0, drawdown, color='red', alpha=0.3)\n        ax.set_ylabel('Drawdown (%)')\n        ax.set_title('Underwater Plot')\n        ax.grid(True, alpha=0.3)\n        \n        # 3. Signals e preÃ§o\n        ax = axes[2]\n        ax2 = ax.twinx()\n        \n        # PreÃ§o\n        prices['close'].plot(ax=ax, color='blue', alpha=0.5, linewidth=1)\n        ax.set_ylabel('Price', color='blue')\n        \n        # Sinais\n        if 'signal' in signals.columns:\n            ax2.plot(signals.index, signals['signal'], 'g-', alpha=0.7, linewidth=0.5)\n            ax2.fill_between(signals.index, 0, signals['signal'], alpha=0.3, color='green')\n            ax2.set_ylabel('Signal', color='green')\n            ax2.set_ylim([-0.1, 1.1])\n        \n        ax.set_xlabel('Date')\n        ax.set_title('Price and Signals')\n        ax.grid(True, alpha=0.3)\n        \n        plt.suptitle('Vectorbt Backtest Results - PRD Compliant', fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n# Teste do backtester\ndef test_vectorbt_backtest():\n    \"\"\"Testa backtester com dados sintÃ©ticos.\"\"\"\n    \n    # Gerar dados de exemplo\n    np.random.seed(42)\n    n_points = 1000\n    dates = pd.date_range('2024-01-01', periods=n_points, freq='15T')\n    \n    # Simular preÃ§os com tendÃªncia\n    returns = np.random.randn(n_points) * 0.002 + 0.0001  # Leve tendÃªncia positiva\n    close_prices = 100 * np.exp(np.cumsum(returns))\n    \n    prices = pd.DataFrame({\n        'open': close_prices * (1 + np.random.randn(n_points) * 0.001),\n        'high': close_prices * (1 + np.abs(np.random.randn(n_points)) * 0.002),\n        'low': close_prices * (1 - np.abs(np.random.randn(n_points)) * 0.002),\n        'close': close_prices,\n        'volume': np.random.uniform(1000, 10000, n_points)\n    }, index=dates)\n    \n    # Simular predictions com algum sinal\n    predictions = np.random.beta(2.5, 2, n_points)  # Ligeiramente enviesado para positivo\n    \n    # Executar backtest\n    backtest = PRDVectorbtBacktest(fee_bps=10, slippage_bps=10)\n    portfolio, metrics = backtest.run_complete_backtest(\n        prices, predictions, threshold=0.5, plot=True\n    )\n    \n    return portfolio, metrics\n\n# Executar teste\nprint(\"ğŸ§ª Testando Vectorbt Backtest...\")\ntest_portfolio, test_metrics = test_vectorbt_backtest()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "88f2f3bb",
   "metadata": {},
   "source": [
    "# IC Crypto Complete - Pipeline ML para Trading de Criptomoedas\n",
    "\n",
    "Pipeline completo de Machine Learning para trading de criptomoedas com:\n",
    "- Labeling adaptativo baseado em volatilidade\n",
    "- MÃºltiplos horizontes de prediÃ§Ã£o (15m, 30m, 60m, 120m)\n",
    "- Features especÃ­ficas para mercado 24/7\n",
    "- Backtest realista com custos e execuÃ§Ã£o t+1\n",
    "- OtimizaÃ§Ã£o Bayesiana com Optuna\n",
    "- CalibraÃ§Ã£o de probabilidades\n",
    "- MLflow tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528cac0",
   "metadata": {},
   "source": [
    "## 1. Setup Completo - Imports e ConfiguraÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33823ff1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Optuna nÃ£o disponÃ­vel\n",
      "âš ï¸ TA-Lib nÃ£o disponÃ­vel\n",
      "âš ï¸ Imports locais nÃ£o disponÃ­veis - usando implementaÃ§Ãµes do notebook\n",
      "âœ… Imports concluÃ­dos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# ========================== IMPORTS ORGANIZADOS ==========================\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import hashlib\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "# Configurar warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report, roc_curve,\n",
    "    precision_recall_curve, brier_score_loss, balanced_accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"âš ï¸ XGBoost nÃ£o disponÃ­vel\")\n",
    "\n",
    "# Optuna\n",
    "OPTUNA_AVAILABLE = False\n",
    "OPTUNA_XGBOOST_INTEGRATION = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.pruners import MedianPruner, SuccessiveHalvingPruner, HyperbandPruner\n",
    "    from optuna.samplers import TPESampler\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"âœ… Optuna core disponÃ­vel\")\n",
    "    \n",
    "    # Tentar carregar integraÃ§Ã£o XGBoost separadamente\n",
    "    try:\n",
    "        from optuna.integration import XGBoostPruningCallback\n",
    "        OPTUNA_XGBOOST_INTEGRATION = True\n",
    "        print(\"âœ… Optuna XGBoost integration disponÃ­vel\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Optuna XGBoost integration nÃ£o disponÃ­vel: {e}\")\n",
    "        print(\"   Pipeline continuarÃ¡ com otimizaÃ§Ã£o bÃ¡sica\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Optuna nÃ£o disponÃ­vel: {e}\")\n",
    "    print(\"   Pipeline continuarÃ¡ com hiperparÃ¢metros padrÃ£o\")\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    TORCH_AVAILABLE = True\n",
    "    \n",
    "    # Configurar determinismo do PyTorch\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"âš ï¸ PyTorch nÃ£o disponÃ­vel\")\n",
    "\n",
    "# MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.xgboost\n",
    "    import mlflow.pytorch\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"âš ï¸ MLflow nÃ£o disponÃ­vel\")\n",
    "\n",
    "# SHAP\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"âš ï¸ SHAP nÃ£o disponÃ­vel\")\n",
    "\n",
    "# Data APIs\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    print(\"âš ï¸ yfinance nÃ£o disponÃ­vel\")\n",
    "\n",
    "try:\n",
    "    import ccxt\n",
    "    CCXT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CCXT_AVAILABLE = False\n",
    "    print(\"âš ï¸ CCXT nÃ£o disponÃ­vel\")\n",
    "\n",
    "# Technical Analysis\n",
    "try:\n",
    "    import ta\n",
    "    TA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TA_AVAILABLE = False\n",
    "    print(\"âš ï¸ TA-Lib nÃ£o disponÃ­vel\")\n",
    "\n",
    "# Data Validation\n",
    "try:\n",
    "    import pandera as pa\n",
    "    from pandera import Column, DataFrameSchema, Check\n",
    "    PANDERA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PANDERA_AVAILABLE = False\n",
    "    print(\"âš ï¸ Pandera nÃ£o disponÃ­vel\")\n",
    "\n",
    "# VisualizaÃ§Ã£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"âš ï¸ Plotly nÃ£o disponÃ­vel\")\n",
    "\n",
    "# Imports locais do projeto (quando disponÃ­veis)\n",
    "try:\n",
    "    from src.data.binance_loader import CryptoDataLoader\n",
    "    from src.data.splits import PurgedKFold as ImportedPurgedKFold\n",
    "    from src.features.engineering import FeatureEngineer as BaseFeatureEngineer\n",
    "    from src.models.xgb_optuna import XGBoostOptuna as ImportedXGBoostOptuna\n",
    "    from src.backtest.engine import BacktestEngine as ImportedBacktestEngine, BacktestConfig\n",
    "    LOCAL_IMPORTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LOCAL_IMPORTS_AVAILABLE = False\n",
    "    print(\"âš ï¸ Imports locais nÃ£o disponÃ­veis - usando implementaÃ§Ãµes do notebook\")\n",
    "\n",
    "# ConfiguraÃ§Ãµes de visualizaÃ§Ã£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "print(\"âœ… Imports concluÃ­dos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4826e",
   "metadata": {},
   "source": [
    "## 2. Fallbacks para Bibliotecas Opcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791abc0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FallbackPruningCallback:\n",
    "    \"\"\"Fallback para XGBoostPruningCallback quando optuna-integration nÃ£o disponÃ­vel\"\"\"\n",
    "    def __init__(self, trial, metric_name):\n",
    "        self.trial = trial\n",
    "        self.metric_name = metric_name\n",
    "        \n",
    "    def __call__(self, env):\n",
    "        \"\"\"Callback compatÃ­vel com XGBoost\"\"\"\n",
    "        # Extrair mÃ©trica de validaÃ§Ã£o\n",
    "        if hasattr(env, 'evaluation_result_list') and env.evaluation_result_list:\n",
    "            # Formato: [('validation_0', 'aucpr', value)]\n",
    "            for eval_name, metric, value in env.evaluation_result_list:\n",
    "                if metric in self.metric_name or self.metric_name in metric:\n",
    "                    self.trial.report(value, env.iteration)\n",
    "                    if self.trial.should_prune():\n",
    "                        raise optuna.TrialPruned()\n",
    "        return False\n",
    "\n",
    "def create_xgb_pruning_callback(trial, metric_name):\n",
    "    \"\"\"Factory function para criar callback apropriado\"\"\"\n",
    "    if OPTUNA_XGBOOST_INTEGRATION:\n",
    "        from optuna.integration import XGBoostPruningCallback\n",
    "        return XGBoostPruningCallback(trial, metric_name)\n",
    "    elif OPTUNA_AVAILABLE:\n",
    "        return FallbackPruningCallback(trial, metric_name)\n",
    "    else:\n",
    "        # Retorna callback dummy que nÃ£o faz nada\n",
    "        return lambda env: False\n",
    "\n",
    "def safe_optuna_study(direction='maximize', sampler=None, pruner=None):\n",
    "    \"\"\"Criar study do Optuna com fallbacks\"\"\"\n",
    "    if OPTUNA_AVAILABLE:\n",
    "        return optuna.create_study(\n",
    "            direction=direction,\n",
    "            sampler=sampler or optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=pruner or optuna.pruners.MedianPruner()\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: usar grid search simples\n",
    "        print(\"âš ï¸ Usando grid search bÃ¡sico no lugar do Optuna\")\n",
    "        return None\n",
    "\n",
    "def safe_mlflow_log(func_name, *args, **kwargs):\n",
    "    \"\"\"Helper para fazer log no MLflow de forma segura\"\"\"\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        func = getattr(mlflow, func_name)\n",
    "        return func(*args, **kwargs)\n",
    "    return None\n",
    "\n",
    "def safe_mlflow_start_run(**kwargs):\n",
    "    \"\"\"Helper para iniciar run do MLflow de forma segura\"\"\"\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        return mlflow.start_run(**kwargs)\n",
    "    return None\n",
    "\n",
    "def safe_mlflow_end_run():\n",
    "    \"\"\"Helper para finalizar run do MLflow de forma segura\"\"\"\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        return mlflow.end_run()\n",
    "    return None\n",
    "\n",
    "print(\"âœ… Fallbacks configurados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480505f",
   "metadata": {},
   "source": [
    "## 3. ConfiguraÃ§Ã£o DeterminÃ­stica do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f8f09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ambiente configurado para determinismo com seed=42\n",
      "   PYTHONHASHSEED=42\n"
     ]
    }
   ],
   "source": [
    "def setup_deterministic_environment(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Configura ambiente para reprodutibilidade total\n",
    "    \"\"\"\n",
    "    # Python built-in\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Scikit-learn (se aplicÃ¡vel)\n",
    "    os.environ['SKLEARN_SEED'] = str(seed)\n",
    "    \n",
    "    # PyTorch (se disponÃ­vel)\n",
    "    if TORCH_AVAILABLE:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            # Para operaÃ§Ãµes determinÃ­sticas em GPU\n",
    "            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    \n",
    "    # TensorFlow (se disponÃ­vel)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    print(f\"âœ… Ambiente configurado para determinismo com seed={seed}\")\n",
    "    print(f\"   PYTHONHASHSEED={os.environ.get('PYTHONHASHSEED', 'not set')}\")\n",
    "    if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "        print(f\"   CUDA determinÃ­stico: {torch.backends.cudnn.deterministic}\")\n",
    "        print(f\"   CUBLAS_WORKSPACE_CONFIG={os.environ.get('CUBLAS_WORKSPACE_CONFIG', 'not set')}\")\n",
    "    \n",
    "    return seed\n",
    "\n",
    "# Aplicar configuraÃ§Ã£o determinÃ­stica\n",
    "GLOBAL_SEED = setup_deterministic_environment(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82713",
   "metadata": {},
   "source": [
    "## 3. ConfiguraÃ§Ãµes Globais do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666b0857",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DiretÃ³rios criados\n",
      "âœ… MLflow configurado: artifacts/mlruns\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class ProjectConfig:\n",
    "    \"\"\"ConfiguraÃ§Ãµes globais do projeto\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    data_path: str = \"data\"\n",
    "    artifacts_path: str = \"artifacts\"\n",
    "    models_path: str = \"artifacts/models\"\n",
    "    reports_path: str = \"artifacts/reports\"\n",
    "    mlflow_tracking_uri: str = \"artifacts/mlruns\"\n",
    "    \n",
    "    # Data\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    timeframe: str = \"15m\"\n",
    "    start_date: str = \"2023-01-01\"\n",
    "    end_date: str = \"2024-01-01\"\n",
    "    \n",
    "    # Funding periods por sÃ­mbolo (em minutos)\n",
    "    # Fonte: https://www.binance.com/en/support/faq/detail/360033525031\n",
    "    # PadrÃ£o: 480 min (8h) para maioria dos contratos perpÃ©tuos\n",
    "    # Alguns contratos especÃ­ficos usam 60 min (1h) ou 240 min (4h)\n",
    "    FUNDING_MIN_BY_SYMBOL: Dict[str, int] = field(default_factory=lambda: {\n",
    "        \"BTCUSDT\": 480,    # 8 horas\n",
    "        \"ETHUSDT\": 480,    # 8 horas  \n",
    "        \"BNBUSDT\": 480,    # 8 horas\n",
    "        \"SOLUSDT\": 480,    # 8 horas\n",
    "        \"XRPUSDT\": 480,    # 8 horas\n",
    "        \"ADAUSDT\": 480,    # 8 horas\n",
    "        # Adicionar outros sÃ­mbolos conforme necessÃ¡rio\n",
    "        # Alguns contratos exÃ³ticos podem ter 60 ou 240 minutos\n",
    "    })\n",
    "    \n",
    "    # Horizontes de prediÃ§Ã£o (em barras de 15min)\n",
    "    horizons: Dict[str, int] = None\n",
    "    \n",
    "    # Features\n",
    "    feature_windows: List[int] = None\n",
    "    volatility_estimators: List[str] = None\n",
    "    \n",
    "    # Model\n",
    "    test_size: float = 0.2\n",
    "    val_size: float = 0.2\n",
    "    cv_splits: int = 5\n",
    "    embargo_bars: int = 10\n",
    "    \n",
    "    # Trading\n",
    "    initial_capital: float = 100000\n",
    "    fee_bps: float = 5  # basis points\n",
    "    slippage_bps: float = 10\n",
    "    max_leverage: float = 1.0\n",
    "    funding_period_minutes: int = 480  # perÃ­odo de funding em minutos (8 horas por padrÃ£o)\n",
    "    \n",
    "    # Optimization\n",
    "    n_trials_optuna: int = 50\n",
    "    optuna_timeout: int = 3600  # seconds\n",
    "    \n",
    "    # MLflow\n",
    "    experiment_name: str = \"crypto_ml_trading\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Inicializar valores padrÃ£o para campos mutÃ¡veis\"\"\"\n",
    "        if self.horizons is None:\n",
    "            # Calcular horizonte de funding dinamicamente\n",
    "            funding_horizon_bars = self.funding_period_minutes // 15  # converter para barras de 15min\n",
    "            self.horizons = {\n",
    "                '15m': 1,   # 15 minutos\n",
    "                '30m': 2,   # 30 minutos  \n",
    "                '60m': 4,   # 1 hora\n",
    "                '120m': 8,  # 2 horas\n",
    "                '240m': 16, # 4 horas\n",
    "                f'{self.funding_period_minutes}m': funding_horizon_bars  # funding cycle dinÃ¢mico\n",
    "            }\n",
    "        \n",
    "        if self.feature_windows is None:\n",
    "            self.feature_windows = [5, 10, 20, 50, 100, 200]\n",
    "        \n",
    "        if self.volatility_estimators is None:\n",
    "            self.volatility_estimators = ['atr', 'garman_klass', 'yang_zhang', 'parkinson']\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Criar estrutura de diretÃ³rios\"\"\"\n",
    "        for path in [self.data_path, self.artifacts_path, self.models_path, self.reports_path]:\n",
    "            Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        print(\"âœ… DiretÃ³rios criados\")\n",
    "\n",
    "# Instanciar configuraÃ§Ã£o global\n",
    "config = ProjectConfig()\n",
    "config.create_directories()\n",
    "\n",
    "# Configurar MLflow\n",
    "if MLFLOW_AVAILABLE:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(config.mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(config.experiment_name)\n",
    "    print(f\"âœ… MLflow configurado: {config.mlflow_tracking_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ff321",
   "metadata": {},
   "source": [
    "## 4. Classes de Estimadores de Volatilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daebc14c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe VolatilityEstimators definida\n"
     ]
    }
   ],
   "source": [
    "class VolatilityEstimators:\n",
    "    \"\"\"\n",
    "    ImplementaÃ§Ã£o de diversos estimadores de volatilidade para mercados 24/7\n",
    "    ReferÃªncia: Sinclair (2008) - Volatility Trading\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def atr(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"Average True Range - robusto para gaps\"\"\"\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close']\n",
    "        \n",
    "        tr1 = high - low\n",
    "        tr2 = abs(high - close.shift())\n",
    "        tr3 = abs(low - close.shift())\n",
    "        \n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=window).mean()\n",
    "        \n",
    "        # Normalizar como proporÃ§Ã£o do preÃ§o (retorno implÃ­cito)\n",
    "        return atr / close\n",
    "    \n",
    "    @staticmethod\n",
    "    def garman_klass(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Garman-Klass estimator (1980)\n",
    "        Usa OHLC, ~8x mais eficiente que close-to-close\n",
    "        \"\"\"\n",
    "        log_hl = np.log(df['high'] / df['low'])\n",
    "        log_co = np.log(df['close'] / df['open'])\n",
    "        \n",
    "        gk = np.sqrt(\n",
    "            0.5 * log_hl**2 - \n",
    "            (2 * np.log(2) - 1) * log_co**2\n",
    "        )\n",
    "        \n",
    "        # Normalizar para escala comparÃ¡vel ao ATR (retorno fracionÃ¡rio)\n",
    "        gk_mean = gk.rolling(window=window).mean()\n",
    "        return gk_mean.clip(lower=1e-8)  # Evitar divisÃ£o por zero\n",
    "    \n",
    "    @staticmethod  \n",
    "    def yang_zhang(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Yang-Zhang estimator (2000)\n",
    "        Melhor estimador para drift e gaps\n",
    "        \"\"\"\n",
    "        log_ho = np.log(df['high'] / df['open'])\n",
    "        log_lo = np.log(df['low'] / df['open'])\n",
    "        log_co = np.log(df['close'] / df['open'])\n",
    "        \n",
    "        log_oc = np.log(df['open'] / df['close'].shift())\n",
    "        log_oc_mean = log_oc.rolling(window=window).mean()\n",
    "        \n",
    "        log_cc = np.log(df['close'] / df['close'].shift())\n",
    "        log_cc_mean = log_cc.rolling(window=window).mean()\n",
    "        \n",
    "        # Volatilidade overnight\n",
    "        vol_overnight = (log_oc - log_oc_mean)**2\n",
    "        vol_overnight = vol_overnight.rolling(window=window).mean()\n",
    "        \n",
    "        # Volatilidade close-to-close\n",
    "        vol_cc = (log_cc - log_cc_mean)**2\n",
    "        vol_cc = vol_cc.rolling(window=window).mean()\n",
    "        \n",
    "        # Volatilidade Rogers-Satchell\n",
    "        rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
    "        vol_rs = rs.rolling(window=window).mean()\n",
    "        \n",
    "        # Combinar com pesos Ã³timos\n",
    "        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "        yz = np.sqrt(vol_overnight + k * vol_cc + (1 - k) * vol_rs)\n",
    "        \n",
    "        # Normalizar para escala comparÃ¡vel (retorno fracionÃ¡rio)\n",
    "        # Yang-Zhang jÃ¡ estÃ¡ em escala de log-retorno, clipar para evitar zero\n",
    "        return yz.clip(lower=1e-8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parkinson(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Parkinson estimator (1980)\n",
    "        Usa high-low, ~5x mais eficiente que close-to-close\n",
    "        \"\"\"\n",
    "        log_hl = np.log(df['high'] / df['low'])\n",
    "        park = log_hl / (2 * np.sqrt(np.log(2)))\n",
    "        \n",
    "        # Normalizar para escala comparÃ¡vel (retorno fracionÃ¡rio)\n",
    "        park_mean = park.rolling(window=window).mean()\n",
    "        return park_mean.clip(lower=1e-8)  # Evitar divisÃ£o por zero\n",
    "    \n",
    "    @staticmethod\n",
    "    def realized_volatility(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"Volatilidade realizada clÃ¡ssica\"\"\"\n",
    "        returns = np.log(df['close'] / df['close'].shift())\n",
    "        return returns.rolling(window=window).std()\n",
    "\n",
    "print(\"âœ… Classe VolatilityEstimators definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14e03f",
   "metadata": {},
   "source": [
    "## 5. Sistema de Labeling Adaptativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4091405",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe AdaptiveLabeler definida\n"
     ]
    }
   ],
   "source": [
    "class AdaptiveLabeler:\n",
    "    \"\"\"\n",
    "    Sistema de rotulagem adaptativo baseado em volatilidade\n",
    "    Sistema mais robusto e interpretÃ¡vel para mercados 24/7\n",
    "    Suporta mÃºltiplos horizontes alinhados com timeframe de 15m\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 horizon_bars: int = 4,  # 1h em dados de 15min\n",
    "                 k: float = 1.0,  # Multiplicador do threshold\n",
    "                 vol_estimator: str = 'atr',  # Estimador de volatilidade\n",
    "                 neutral_zone: bool = True):  # Usar zona neutra\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            horizon_bars: Janela futura para calcular retorno\n",
    "            k: Multiplicador do threshold (hiperparÃ¢metro a otimizar)\n",
    "            vol_estimator: 'atr', 'garman_klass', 'yang_zhang', 'parkinson'\n",
    "            neutral_zone: Se True, cria zona morta entre thresholds\n",
    "        \"\"\"\n",
    "        self.horizon_bars = horizon_bars\n",
    "        self.k = k\n",
    "        self.vol_estimator = vol_estimator\n",
    "        self.neutral_zone = neutral_zone\n",
    "        self.volatility_estimators = VolatilityEstimators()\n",
    "        \n",
    "        # Mapeamento de horizontes em minutos para bars de 15m\n",
    "        # Calcular horizonte de funding dinamicamente\n",
    "        funding_period_minutes = getattr(self, 'funding_period_minutes', 480)\n",
    "        funding_horizon_bars = funding_period_minutes // 15  # converter para barras de 15min\n",
    "        \n",
    "        self.horizon_map = {\n",
    "            '15m': 1,   # 15 minutos = 1 bar\n",
    "            '30m': 2,   # 30 minutos = 2 bars\n",
    "            '60m': 4,   # 60 minutos = 4 bars\n",
    "            '120m': 8,  # 120 minutos = 8 bars\n",
    "            '240m': 16, # 240 minutos = 16 bars\n",
    "            f'{funding_period_minutes}m': funding_horizon_bars  # funding cycle dinÃ¢mico\n",
    "        }\n",
    "    \n",
    "    def calculate_volatility(self, df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "        \"\"\"Calcula volatilidade usando estimador selecionado\"\"\"\n",
    "        estimator_map = {\n",
    "            'atr': self.volatility_estimators.atr,\n",
    "            'garman_klass': self.volatility_estimators.garman_klass,\n",
    "            'yang_zhang': self.volatility_estimators.yang_zhang,\n",
    "            'parkinson': self.volatility_estimators.parkinson,\n",
    "            'realized': self.volatility_estimators.realized_volatility\n",
    "        }\n",
    "        \n",
    "        if self.vol_estimator not in estimator_map:\n",
    "            raise ValueError(f\"Estimador {self.vol_estimator} nÃ£o suportado\")\n",
    "        \n",
    "        return estimator_map[self.vol_estimator](df, window)\n",
    "    \n",
    "    def calculate_adaptive_threshold(self, df: pd.DataFrame, \n",
    "                                    window: int = 20) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calcula threshold adaptativo baseado em volatilidade\n",
    "        \n",
    "        Returns:\n",
    "            Series com threshold adaptativo para cada barra\n",
    "        \"\"\"\n",
    "        volatility = self.calculate_volatility(df, window)\n",
    "        \n",
    "        # Ajustar threshold baseado na volatilidade e horizonte\n",
    "        # Horizonte maior = threshold maior\n",
    "        horizon_adjustment = np.sqrt(self.horizon_bars)\n",
    "        \n",
    "        threshold = self.k * volatility * horizon_adjustment\n",
    "        \n",
    "        # Aplicar limite mÃ­nimo e mÃ¡ximo\n",
    "        threshold = threshold.clip(lower=0.001, upper=0.10)\n",
    "        \n",
    "        return threshold\n",
    "    \n",
    "    def create_labels(self, df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Cria labels baseados em threshold adaptativo\n",
    "        \n",
    "        Returns:\n",
    "            Series com labels: 1 (long), 0 (neutral), -1 (short)\n",
    "        \"\"\"\n",
    "        # Calcular retorno futuro\n",
    "        future_return = (\n",
    "            df['close'].shift(-self.horizon_bars) / df['close'] - 1\n",
    "        )\n",
    "        \n",
    "        # Calcular threshold adaptativo\n",
    "        threshold = self.calculate_adaptive_threshold(df, window)\n",
    "        \n",
    "        # Criar labels\n",
    "        labels = pd.Series(index=df.index, dtype=float)\n",
    "        \n",
    "        if self.neutral_zone:\n",
    "            # Com zona neutra: -1, 0, 1\n",
    "            labels[future_return > threshold] = 1  # Long\n",
    "            labels[future_return < -threshold] = -1  # Short  \n",
    "            labels[(future_return >= -threshold) & (future_return <= threshold)] = 0  # Neutral\n",
    "        else:\n",
    "            # Sem zona neutra: -1, 1\n",
    "            labels[future_return > 0] = 1  # Long\n",
    "            labels[future_return <= 0] = -1  # Short\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def get_label_distribution(self, labels: pd.Series) -> Dict:\n",
    "        \"\"\"Retorna distribuiÃ§Ã£o dos labels\"\"\"\n",
    "        counts = labels.value_counts()\n",
    "        proportions = labels.value_counts(normalize=True)\n",
    "        \n",
    "        return {\n",
    "            'counts': counts.to_dict(),\n",
    "            'proportions': proportions.to_dict(),\n",
    "            'total': len(labels.dropna()),\n",
    "            'balance_ratio': counts.min() / counts.max() if len(counts) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def optimize_k_for_horizon(self, df: pd.DataFrame, X: pd.DataFrame,\n",
    "                               horizon: str, cv_splits: int = 3,\n",
    "                               metric: str = 'f1',\n",
    "                               k_range: Tuple[float, float] = (0.5, 2.0)) -> float:\n",
    "        \"\"\"\n",
    "        Otimiza o multiplicador k para um horizonte especÃ­fico\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com OHLC\n",
    "            X: Features\n",
    "            horizon: Horizonte alvo ('15m', '30m', etc)\n",
    "            cv_splits: NÃºmero de splits para validaÃ§Ã£o\n",
    "            metric: MÃ©trica para otimizaÃ§Ã£o ('f1', 'pr_auc')\n",
    "            k_range: Range de valores de k para testar\n",
    "            \n",
    "        Returns:\n",
    "            float: k Ã³timo para o horizonte\n",
    "        \"\"\"\n",
    "        # Imports jÃ¡ feitos no inÃ­cio do arquivo, nÃ£o precisam ser repetidos\n",
    "        \n",
    "        # Configurar horizonte\n",
    "        self.horizon_bars = self.horizon_map[horizon]\n",
    "        \n",
    "        best_k = self.k\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        # Testar diferentes valores de k\n",
    "        k_values = np.linspace(k_range[0], k_range[1], 20)\n",
    "        \n",
    "        for k in k_values:\n",
    "            self.k = k\n",
    "            \n",
    "            # Criar labels com k atual\n",
    "            labels = self.create_labels(df)\n",
    "            \n",
    "            # Remover NaN\n",
    "            mask = ~(labels.isna() | X.isna().any(axis=1))\n",
    "            X_clean = X[mask]\n",
    "            y_clean = labels[mask]\n",
    "            \n",
    "            # Converter para binÃ¡rio se necessÃ¡rio\n",
    "            if metric in ['f1', 'pr_auc']:\n",
    "                y_clean = (y_clean > 0).astype(int)\n",
    "            \n",
    "            # ValidaÃ§Ã£o cruzada temporal\n",
    "            tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_clean):\n",
    "                X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
    "                y_train, y_val = y_clean.iloc[train_idx], y_clean.iloc[val_idx]\n",
    "                \n",
    "                # Modelo simples para avaliaÃ§Ã£o rÃ¡pida\n",
    "                model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                if metric == 'f1':\n",
    "                    y_pred = model.predict(X_val)\n",
    "                    score = f1_score(y_val, y_pred, average='weighted')\n",
    "                elif metric == 'pr_auc':\n",
    "                    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "                    score = average_precision_score(y_val, y_proba)\n",
    "                else:\n",
    "                    raise ValueError(f\"MÃ©trica {metric} nÃ£o suportada\")\n",
    "                \n",
    "                scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_k = k\n",
    "            \n",
    "            print(f\"k={k:.2f}: {metric}={avg_score:.4f}\")\n",
    "        \n",
    "        print(f\"âœ… k Ã³timo para {horizon}: {best_k:.3f}\")\n",
    "        \n",
    "        return best_k\n",
    "    \n",
    "    def optimize_k_multi_horizon(self, df: pd.DataFrame, X: pd.DataFrame,\n",
    "                                 horizons: List[str] = None,\n",
    "                                 cv_splits: int = 3,\n",
    "                                 metric: str = 'pr_auc') -> Dict:\n",
    "        \"\"\"\n",
    "        Otimiza k para mÃºltiplos horizontes simultaneamente\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com OHLC\n",
    "            X: Features\n",
    "            horizons: Lista de horizontes para otimizar\n",
    "            cv_splits: NÃºmero de splits para CV\n",
    "            metric: MÃ©trica para otimizaÃ§Ã£o ('f1', 'pr_auc')\n",
    "            \n",
    "        Returns:\n",
    "            Dict com k Ã³timo para cada horizonte\n",
    "        \"\"\"\n",
    "        if horizons is None:\n",
    "            horizons = ['15m', '30m', '60m', '120m']\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for horizon in horizons:\n",
    "            print(f\"\\nOtimizando k para horizonte {horizon}...\")\n",
    "            optimal_k = self.optimize_k_for_horizon(\n",
    "                df, X, horizon, cv_splits, metric\n",
    "            )\n",
    "            results[horizon] = optimal_k\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize_k(self, df: pd.DataFrame, X: pd.DataFrame, \n",
    "                   cv_splits: int = 5, metric: str = 'f1') -> float:\n",
    "        \"\"\"\n",
    "        Otimiza o multiplicador k usando validaÃ§Ã£o cruzada temporal\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com OHLC\n",
    "            X: Features para treino\n",
    "            cv_splits: NÃºmero de splits temporais\n",
    "            metric: 'f1' ou 'balanced_accuracy'\n",
    "            \n",
    "        Returns:\n",
    "            float: k Ã³timo\n",
    "        \"\"\"\n",
    "        # Imports jÃ¡ feitos no inÃ­cio do arquivo, nÃ£o precisam ser repetidos\n",
    "        \n",
    "        best_k = self.k\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        # Range de k para testar\n",
    "        k_values = np.linspace(0.5, 2.0, 20)\n",
    "        \n",
    "        for k in k_values:\n",
    "            self.k = k\n",
    "            \n",
    "            # Criar labels com k atual\n",
    "            labels = self.create_labels(df)\n",
    "            \n",
    "            # Remover NaN\n",
    "            mask = ~(labels.isna() | X.isna().any(axis=1))\n",
    "            X_clean = X[mask]\n",
    "            y_clean = labels[mask]\n",
    "            \n",
    "            # Converter para binÃ¡rio (up/down)\n",
    "            y_binary = (y_clean > 0).astype(int)\n",
    "            \n",
    "            # Time Series CV\n",
    "            tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_clean):\n",
    "                X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
    "                y_train, y_val = y_binary.iloc[train_idx], y_binary.iloc[val_idx]\n",
    "                \n",
    "                # Modelo simples para teste rÃ¡pido\n",
    "                clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_val)\n",
    "                \n",
    "                if metric == 'f1':\n",
    "                    score = f1_score(y_val, y_pred, average='weighted')\n",
    "                else:\n",
    "                    score = balanced_accuracy_score(y_val, y_pred)\n",
    "                \n",
    "                scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_k = k\n",
    "            \n",
    "            print(f\"k={k:.2f}: {metric}={avg_score:.4f}\")\n",
    "        \n",
    "        self.k = best_k\n",
    "        return best_k\n",
    "\n",
    "print(\"âœ… Classe AdaptiveLabeler definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37658733",
   "metadata": {},
   "source": [
    "## 6. Features para Mercado Cripto 24/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a940e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe Crypto24x7Features definida\n"
     ]
    }
   ],
   "source": [
    "# Resolver dinÃ¢mico de funding period\n",
    "def resolve_funding_minutes(symbol: str, timestamp: pd.Timestamp = None) -> int:\n",
    "    \"\"\"\n",
    "    Resolve o perÃ­odo de funding dinamicamente baseado no sÃ­mbolo e data.\n",
    "    \n",
    "    Regras (fonte: Binance Support):\n",
    "    - PadrÃ£o: 480 min (8 horas) para maioria dos contratos perpÃ©tuos\n",
    "    - Condicional: 60 min (1 hora) quando funding rate atinge cap/floor\n",
    "    - Especial: Alguns contratos podem ter 240 min (4 horas)\n",
    "    \n",
    "    Args:\n",
    "        symbol: SÃ­mbolo do contrato (ex: BTCUSDT)\n",
    "        timestamp: Data/hora para verificar regras especÃ­ficas do perÃ­odo\n",
    "        \n",
    "    Returns:\n",
    "        PerÃ­odo de funding em minutos\n",
    "    \"\"\"\n",
    "    # DicionÃ¡rio base de funding por sÃ­mbolo\n",
    "    # Fonte: https://www.binance.com/en/support/faq/detail/360033525031\n",
    "    FUNDING_PERIODS = {\n",
    "        # Majors - 8 horas padrÃ£o\n",
    "        \"BTCUSDT\": 480,\n",
    "        \"ETHUSDT\": 480,\n",
    "        \"BNBUSDT\": 480,\n",
    "        \"XRPUSDT\": 480,\n",
    "        \"ADAUSDT\": 480,\n",
    "        \"SOLUSDT\": 480,\n",
    "        \"DOTUSDT\": 480,\n",
    "        \"DOGEUSDT\": 480,\n",
    "        \n",
    "        # Contratos com perÃ­odo especial (exemplos)\n",
    "        # Adicionar conforme documentaÃ§Ã£o da exchange\n",
    "    }\n",
    "    \n",
    "    # Obter perÃ­odo base do sÃ­mbolo\n",
    "    base_period = FUNDING_PERIODS.get(symbol, 480)  # Default 8h\n",
    "    \n",
    "    # TODO: Implementar lÃ³gica condicional\n",
    "    # Se funding rate atingir cap/floor, pode mudar para 1h temporariamente\n",
    "    # Isso requer acesso ao funding rate atual da exchange\n",
    "    \n",
    "    # Log da decisÃ£o\n",
    "    if timestamp:\n",
    "        print(f\"ğŸ“Š Funding period para {symbol} em {timestamp}: {base_period} min ({base_period/60:.1f}h)\")\n",
    "    \n",
    "    return base_period\n",
    "\n",
    "class Crypto24x7Features:\n",
    "    \"\"\"\n",
    "    Features especÃ­ficas para mercado cripto 24/7\n",
    "    Inclui calendÃ¡rio, sessÃµes regionais e funding\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cria features de calendÃ¡rio 24/7\n",
    "        \n",
    "        Crypto nÃ£o tem fechamento, mas tem padrÃµes:\n",
    "        - HorÃ¡rios de maior volume (overlaps de mercados)\n",
    "        - Dias da semana\n",
    "        - Fim de mÃªs (rebalanceamento de portfolios)\n",
    "        \"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Extrair componentes temporais\n",
    "        features['hour'] = df.index.hour\n",
    "        features['day_of_week'] = df.index.dayofweek\n",
    "        features['day_of_month'] = df.index.day\n",
    "        features['week_of_year'] = df.index.isocalendar().week\n",
    "        features['month'] = df.index.month\n",
    "        features['quarter'] = df.index.quarter\n",
    "        \n",
    "        # Features cÃ­clicas (encoding circular)\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)\n",
    "        features['dow_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)\n",
    "        features['dow_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)\n",
    "        features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)\n",
    "        features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)\n",
    "        \n",
    "        # PerÃ­odos especiais\n",
    "        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)\n",
    "        features['is_month_end'] = (df.index.day >= 28).astype(int)\n",
    "        features['is_quarter_end'] = ((features['month'] % 3 == 0) & \n",
    "                                      (features['is_month_end'] == 1)).astype(int)\n",
    "        \n",
    "        # HorÃ¡rio combinado (0-167 para hora da semana)\n",
    "        features['hour_of_week'] = features['day_of_week'] * 24 + features['hour']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_session_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identifica sessÃµes de trading regionais\n",
    "        \n",
    "        Principais sessÃµes (UTC):\n",
    "        - Asia: 00:00 - 09:00\n",
    "        - Europe: 07:00 - 16:00  \n",
    "        - Americas: 13:00 - 22:00\n",
    "        \"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        hour = df.index.hour\n",
    "        \n",
    "        # SessÃµes principais\n",
    "        features['session_asia'] = ((hour >= 0) & (hour < 9)).astype(int)\n",
    "        features['session_europe'] = ((hour >= 7) & (hour < 16)).astype(int)\n",
    "        features['session_americas'] = ((hour >= 13) & (hour < 22)).astype(int)\n",
    "        \n",
    "        # Overlaps (maior volume/volatilidade)\n",
    "        features['overlap_asia_europe'] = ((hour >= 7) & (hour < 9)).astype(int)\n",
    "        features['overlap_europe_americas'] = ((hour >= 13) & (hour < 16)).astype(int)\n",
    "        \n",
    "        # Contagem de sessÃµes ativas\n",
    "        features['active_sessions'] = (\n",
    "            features['session_asia'] + \n",
    "            features['session_europe'] + \n",
    "            features['session_americas']\n",
    "        )\n",
    "        \n",
    "        # PerÃ­odo de baixa atividade\n",
    "        features['low_activity'] = (features['active_sessions'] == 0).astype(int)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_funding_features(df: pd.DataFrame, \n",
    "                               features: pd.DataFrame = None,\n",
    "                               funding_period_minutes: int = 480) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Features relacionadas ao funding rate (perpetual futures)\n",
    "        \n",
    "        Default Ã© 480 minutos (8 horas) - padrÃ£o da maioria dos contratos perpetuais\n",
    "        Alguns contratos especÃ­ficos usam 60 minutos (1 hora) - ajustar por sÃ­mbolo\n",
    "        \"\"\"\n",
    "        if features is None:\n",
    "            features = pd.DataFrame(index=df.index)\n",
    "        else:\n",
    "            features = features.copy()\n",
    "        \n",
    "        # Converter perÃ­odo de funding para barras (15min cada)\n",
    "        funding_period_bars = funding_period_minutes // 15  # minutos / 15 = barras\n",
    "        \n",
    "        # Identificar proximidade ao funding\n",
    "        hour = df.index.hour\n",
    "        minute = df.index.minute\n",
    "        \n",
    "        # Minutos atÃ© prÃ³ximo funding\n",
    "        minutes_in_day = hour * 60 + minute\n",
    "        \n",
    "        # Gerar funding times dinamicamente baseado no perÃ­odo\n",
    "        funding_times = list(range(0, 1440, funding_period_minutes))\n",
    "        \n",
    "        # Calcular minutos atÃ© prÃ³ximo funding\n",
    "        features['minutes_to_funding'] = [\n",
    "            min(((ft - m) % 1440) for ft in funding_times) \n",
    "            for m in minutes_in_day\n",
    "        ]\n",
    "        \n",
    "        features['bars_to_funding'] = features['minutes_to_funding'] / 15\n",
    "        \n",
    "        # Proximidade ao funding (decai exponencialmente)\n",
    "        features['funding_proximity'] = np.exp(-features['bars_to_funding'] / 10)\n",
    "        \n",
    "        # Ã‰ hora de funding?\n",
    "        features['is_funding_time'] = (features['minutes_to_funding'] == 0).astype(int)\n",
    "        \n",
    "        # Janela prÃ©-funding (proporÃ§Ã£o do perÃ­odo - 12.5% antes do funding)\n",
    "        pre_funding_minutes = min(60, funding_period_minutes // 8)  # MÃ¡ximo 1 hora, ou 1/8 do perÃ­odo\n",
    "        features['pre_funding_window'] = (features['minutes_to_funding'] <= pre_funding_minutes).astype(int)\n",
    "        \n",
    "        # Ciclo de funding (qual perÃ­odo estamos)\n",
    "        features['funding_cycle'] = (minutes_in_day // funding_period_minutes).astype(int)\n",
    "        \n",
    "        # Features cÃ­clicas para funding\n",
    "        features['funding_cycle_sin'] = np.sin(2 * np.pi * features['bars_to_funding'] / funding_period_bars)\n",
    "        features['funding_cycle_cos'] = np.cos(2 * np.pi * features['bars_to_funding'] / funding_period_bars)\n",
    "        \n",
    "        return features\n",
    "\n",
    "print(\"âœ… Classe Crypto24x7Features definida\")# %% [markdown]\n",
    "# ## 7. Pipeline Multi-Horizonte de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d802a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FunÃ§Ã£o run_multi_horizon_pipeline definida\n"
     ]
    }
   ],
   "source": [
    "def run_multi_horizon_pipeline(df: pd.DataFrame, \n",
    "                              features: pd.DataFrame,\n",
    "                              horizons: List[str] = ['15m', '30m', '60m', '120m'],\n",
    "                              test_size: float = 0.2,\n",
    "                              val_size: float = 0.2,\n",
    "                              n_trials: int = 20,  # Reduzido para otimizaÃ§Ã£o de memÃ³ria\n",
    "                              k_range: Tuple[float, float] = (0.5, 2.0)) -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline completo para treinar e avaliar modelos em mÃºltiplos horizontes\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com OHLC\n",
    "        features: Features preparadas\n",
    "        horizons: Lista de horizontes para avaliar\n",
    "        test_size: ProporÃ§Ã£o para teste\n",
    "        val_size: ProporÃ§Ã£o para validaÃ§Ã£o  \n",
    "        n_trials: NÃºmero de trials Optuna\n",
    "        k_range: Range para otimizaÃ§Ã£o do k\n",
    "        \n",
    "    Returns:\n",
    "        Dict com resultados para cada horizonte\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ INICIANDO PIPELINE MULTI-HORIZONTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Verificar disponibilidade de bibliotecas\n",
    "    if not XGB_AVAILABLE:\n",
    "        raise ImportError(\"XGBoost nÃ£o estÃ¡ disponÃ­vel\")\n",
    "    \n",
    "    # Informar sobre status das bibliotecas opcionais\n",
    "    if not OPTUNA_AVAILABLE:\n",
    "        print(\"âš ï¸ Optuna nÃ£o disponÃ­vel - usando hiperparÃ¢metros padrÃ£o\")\n",
    "    elif not OPTUNA_XGBOOST_INTEGRATION:\n",
    "        print(\"âš ï¸ Optuna XGBoost integration nÃ£o disponÃ­vel - usando otimizaÃ§Ã£o bÃ¡sica\")\n",
    "    \n",
    "    if not MLFLOW_AVAILABLE:\n",
    "        print(\"âš ï¸ MLflow nÃ£o disponÃ­vel - resultados nÃ£o serÃ£o tracked\")\n",
    "    \n",
    "    if not SHAP_AVAILABLE:\n",
    "        print(\"âš ï¸ SHAP nÃ£o disponÃ­vel - interpretabilidade limitada\")\n",
    "    \n",
    "    # Estrutura para armazenar resultados\n",
    "    results = {}\n",
    "    \n",
    "    # Configurar MLflow\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        experiment_name = f\"multi_horizon_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Split temporal dos dados\n",
    "    n_samples = len(df)\n",
    "    test_start = int(n_samples * (1 - test_size))\n",
    "    val_start = int(n_samples * (1 - test_size - val_size))\n",
    "    \n",
    "    train_idx = slice(0, val_start)\n",
    "    val_idx = slice(val_start, test_start)\n",
    "    test_idx = slice(test_start, n_samples)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Split dos dados:\")\n",
    "    print(f\"  Train: {val_start} samples ({val_start/n_samples:.1%})\")\n",
    "    print(f\"  Val:   {test_start - val_start} samples ({val_size:.1%})\")\n",
    "    print(f\"  Test:  {n_samples - test_start} samples ({test_size:.1%})\")\n",
    "    \n",
    "    # Adicionar features de funding cycle parametrizado por sÃ­mbolo\n",
    "    crypto_features = Crypto24x7Features()\n",
    "    \n",
    "    # Resolver perÃ­odo de funding dinamicamente\n",
    "    symbol = config.symbol if 'config' in locals() else \"BTCUSDT\"\n",
    "    last_timestamp = df.index[-1] if not df.empty else None\n",
    "    funding_period_minutes = resolve_funding_minutes(symbol, last_timestamp)\n",
    "    \n",
    "    # Log adicional se MLflow disponÃ­vel  \n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        mlflow.log_param(\"funding_period_minutes\", funding_period_minutes)\n",
    "        mlflow.log_param(\"funding_symbol\", symbol)\n",
    "    \n",
    "    features_with_funding = crypto_features.create_funding_features(\n",
    "        df, features, funding_period_minutes=funding_period_minutes\n",
    "    )\n",
    "    \n",
    "    # Processar cada horizonte\n",
    "    for horizon in horizons:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"â±ï¸ Processando horizonte: {horizon}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        run_context = None\n",
    "        if MLFLOW_AVAILABLE:\n",
    "            import mlflow\n",
    "            try:\n",
    "                run_context = mlflow.start_run(run_name=f\"horizon_{horizon}\", nested=True)\n",
    "            except Exception:\n",
    "                # Se der erro com nested, tentar sem ou finalizar run anterior\n",
    "                try:\n",
    "                    mlflow.end_run()\n",
    "                    run_context = mlflow.start_run(run_name=f\"horizon_{horizon}\")\n",
    "                except Exception:\n",
    "                    print(\"âš ï¸ Erro ao inicializar MLflow run - continuando sem tracking\")\n",
    "                    run_context = None\n",
    "        \n",
    "        try:\n",
    "            # Log do horizonte\n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_param(\"horizon\", horizon)\n",
    "                mlflow.log_param(\"n_trials\", n_trials)\n",
    "                mlflow.log_param(\"k_range\", k_range)\n",
    "            \n",
    "            # 1. Criar labels para este horizonte\n",
    "            labeler = AdaptiveLabeler(vol_estimator='yang_zhang')\n",
    "            horizon_bars = labeler.horizon_map[horizon]\n",
    "            \n",
    "            # Otimizar k para este horizonte\n",
    "            print(f\"\\nğŸ” Otimizando k para horizonte {horizon}...\")\n",
    "            optimal_k = labeler.optimize_k_for_horizon(\n",
    "                df[train_idx], \n",
    "                features_with_funding[train_idx],\n",
    "                horizon=horizon,\n",
    "                cv_splits=3,\n",
    "                metric='pr_auc'\n",
    "            )\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_metric(f\"optimal_k_{horizon}\", optimal_k)\n",
    "            \n",
    "            # Criar labels com k otimizado\n",
    "            labeler.k = optimal_k\n",
    "            labeler.horizon_bars = horizon_bars\n",
    "            labels = labeler.create_labels(df)\n",
    "            \n",
    "            # 2. Preparar dados\n",
    "            X_train = features_with_funding[train_idx]\n",
    "            y_train = labels[train_idx]\n",
    "            X_val = features_with_funding[val_idx]\n",
    "            y_val = labels[val_idx]\n",
    "            X_test = features_with_funding[test_idx]\n",
    "            y_test = labels[test_idx]\n",
    "            \n",
    "            # Remover NaN\n",
    "            mask_train = ~(X_train.isna().any(axis=1) | y_train.isna())\n",
    "            mask_val = ~(X_val.isna().any(axis=1) | y_val.isna())\n",
    "            mask_test = ~(X_test.isna().any(axis=1) | y_test.isna())\n",
    "            \n",
    "            X_train = X_train[mask_train]\n",
    "            y_train = y_train[mask_train]\n",
    "            X_val = X_val[mask_val]\n",
    "            y_val = y_val[mask_val]\n",
    "            X_test = X_test[mask_test]\n",
    "            y_test = y_test[mask_test]\n",
    "            \n",
    "            # Converter labels para binÃ¡rio (1: up, 0: down/neutral)\n",
    "            y_train_binary = (y_train > 0).astype(int)\n",
    "            y_val_binary = (y_val > 0).astype(int)\n",
    "            y_test_binary = (y_test > 0).astype(int)\n",
    "            \n",
    "            # Log distribuiÃ§Ã£o das classes\n",
    "            train_pos_pct = y_train_binary.mean()\n",
    "            val_pos_pct = y_val_binary.mean()\n",
    "            test_pos_pct = y_test_binary.mean()\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ DistribuiÃ§Ã£o das classes:\")\n",
    "            print(f\"  Train: {train_pos_pct:.2%} positivos\")\n",
    "            print(f\"  Val:   {val_pos_pct:.2%} positivos\")\n",
    "            print(f\"  Test:  {test_pos_pct:.2%} positivos\")\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                mlflow.log_metric(\"train_positive_pct\", train_pos_pct)\n",
    "                mlflow.log_metric(\"val_positive_pct\", val_pos_pct)\n",
    "            \n",
    "            # 3. XGBoost nÃ£o precisa de normalizaÃ§Ã£o (trees sÃ£o invariantes Ã  escala)\n",
    "            # Manter dados originais para melhor interpretabilidade\n",
    "            X_train_scaled = X_train\n",
    "            X_val_scaled = X_val\n",
    "            X_test_scaled = X_test\n",
    "            scaler = None  # XGBoost nÃ£o precisa\n",
    "            \n",
    "            # 4. OtimizaÃ§Ã£o com Optuna\n",
    "            print(f\"\\nğŸ¯ Otimizando XGBoost com Optuna...\")\n",
    "            \n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                    'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'scale_pos_weight': ((1 - train_pos_pct) / train_pos_pct) if train_pos_pct > 0 else 1.0,\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'eval_metric': 'aucpr',\n",
    "                    'tree_method': 'hist',\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                \n",
    "                # Treinar com early stopping e pruning\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                \n",
    "                # Usar callback apropriado baseado na disponibilidade\n",
    "                callbacks = []\n",
    "                if OPTUNA_AVAILABLE:\n",
    "                    callbacks.append(create_xgb_pruning_callback(trial, \"validation_0-aucpr\"))\n",
    "                \n",
    "                model.fit(\n",
    "                    X_train_scaled, y_train_binary,\n",
    "                    eval_set=[(X_val_scaled, y_val_binary)],\n",
    "                    verbose=False,\n",
    "                    early_stopping_rounds=200,\n",
    "                    callbacks=callbacks if callbacks else None\n",
    "                )\n",
    "                \n",
    "                # Salvar melhor iteraÃ§Ã£o no trial\n",
    "                if hasattr(model, 'best_iteration'):\n",
    "                    trial.set_user_attr('best_iteration', model.best_iteration)\n",
    "                \n",
    "                # Avaliar com PR-AUC\n",
    "                y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "                pr_auc = average_precision_score(y_val_binary, y_pred_proba)\n",
    "                \n",
    "                return pr_auc\n",
    "            \n",
    "            # Executar otimizaÃ§Ã£o\n",
    "            if OPTUNA_AVAILABLE:\n",
    "                study = safe_optuna_study(\n",
    "                    direction='maximize',\n",
    "                    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                    pruner=optuna.pruners.MedianPruner()\n",
    "                )\n",
    "                \n",
    "                study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "                \n",
    "                # Melhores parÃ¢metros\n",
    "                best_params = study.best_params\n",
    "                best_score = study.best_value\n",
    "            else:\n",
    "                # Fallback: usar hiperparÃ¢metros padrÃ£o otimizados\n",
    "                print(\"âš ï¸ Usando hiperparÃ¢metros padrÃ£o (Optuna nÃ£o disponÃ­vel)\")\n",
    "                best_params = {\n",
    "                    'max_depth': 6,\n",
    "                    'learning_rate': 0.05,\n",
    "                    'n_estimators': 200,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'gamma': 0.1,\n",
    "                    'reg_alpha': 0.1,\n",
    "                    'reg_lambda': 1.0,\n",
    "                    'min_child_weight': 1,\n",
    "                    'scale_pos_weight': ((1 - train_pos_pct) / train_pos_pct) if train_pos_pct > 0 else 1.0,\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'eval_metric': 'aucpr',\n",
    "                    'tree_method': 'hist',\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                \n",
    "                # Avaliar hiperparÃ¢metros padrÃ£o\n",
    "                temp_model = xgb.XGBClassifier(**best_params)\n",
    "                temp_model.fit(X_train_scaled, y_train_binary, eval_set=[(X_val_scaled, y_val_binary)], verbose=False, early_stopping_rounds=200)\n",
    "                y_pred_proba = temp_model.predict_proba(X_val_scaled)[:, 1]\n",
    "                best_score = average_precision_score(y_val_binary, y_pred_proba)\n",
    "            \n",
    "            # Recuperar melhor iteraÃ§Ã£o se disponÃ­vel (sÃ³ quando Optuna usado)\n",
    "            if OPTUNA_AVAILABLE and 'study' in locals():\n",
    "                best_iteration = study.best_trial.user_attrs.get('best_iteration')\n",
    "                if best_iteration is not None:\n",
    "                    best_params['n_estimators'] = int(best_iteration)\n",
    "                    print(f\"ğŸ“Š Usando melhor iteraÃ§Ã£o do early stopping: {best_iteration}\")\n",
    "            \n",
    "            best_params.update({\n",
    "                'scale_pos_weight': ((1 - train_pos_pct) / train_pos_pct) if train_pos_pct > 0 else 1.0,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'aucpr',\n",
    "                'tree_method': 'hist',\n",
    "                'random_state': 42\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nâœ… Melhor PR-AUC em validaÃ§Ã£o: {best_score:.4f}\")\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                mlflow.log_metric(f\"best_pr_auc_val_{horizon}\", best_score)\n",
    "                mlflow.log_params({f\"xgb_{k}_{horizon}\": v for k, v in best_params.items()})\n",
    "            \n",
    "            # 5. Treinar modelo final\n",
    "            print(f\"\\nğŸ‹ï¸ Treinando modelo final...\")\n",
    "            final_model = xgb.XGBClassifier(**best_params)\n",
    "            final_model.fit(\n",
    "                X_train_scaled, y_train_binary,\n",
    "                eval_set=[(X_val_scaled, y_val_binary)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200  # Manter early stopping no modelo final\n",
    "            )\n",
    "            \n",
    "            # 6. CalibraÃ§Ã£o de probabilidades\n",
    "            print(f\"\\nğŸ“ Calibrando probabilidades...\")\n",
    "            calibrator = CalibratedClassifierCV(\n",
    "                final_model, \n",
    "                method='isotonic',\n",
    "                cv='prefit'\n",
    "            )\n",
    "            calibrator.fit(X_val, y_val_binary)  # Usar dados originais\n",
    "            \n",
    "            # 7. Otimizar threshold no VALIDATION (nÃ£o no teste!)\n",
    "            print(f\"\\nğŸ” Otimizando threshold no conjunto de validaÃ§Ã£o...\")\n",
    "            \n",
    "            # PrediÃ§Ãµes calibradas no validation para escolher threshold\n",
    "            y_val_pred_cal = calibrator.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Otimizar threshold baseado em F1 no VALIDATION\n",
    "            precision, recall, thresholds = precision_recall_curve(\n",
    "                y_val_binary, y_val_pred_cal\n",
    "            )\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            best_threshold_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds[best_threshold_idx] if best_threshold_idx < len(thresholds) else 0.5\n",
    "            \n",
    "            print(f\"  Threshold Ã³timo (do validation): {best_threshold:.4f}\")\n",
    "            \n",
    "            # 8. AvaliaÃ§Ã£o em teste com threshold fixo\n",
    "            print(f\"\\nğŸ“Š Avaliando em conjunto de teste com threshold fixo...\")\n",
    "            \n",
    "            # PrediÃ§Ãµes nÃ£o calibradas\n",
    "            y_test_pred_raw = final_model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # PrediÃ§Ãµes calibradas\n",
    "            y_test_pred_cal = calibrator.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Aplicar threshold\n",
    "            y_test_pred_binary = (y_test_pred_cal >= best_threshold).astype(int)\n",
    "            \n",
    "            # MÃ©tricas finais\n",
    "            test_pr_auc = average_precision_score(y_test_binary, y_test_pred_cal)\n",
    "            test_f1 = f1_score(y_test_binary, y_test_pred_binary)\n",
    "            test_mcc = matthews_corrcoef(y_test_binary, y_test_pred_binary)\n",
    "            \n",
    "            # Matriz de confusÃ£o (com labels explÃ­citos para evitar erros)\n",
    "            cm = confusion_matrix(y_test_binary, y_test_pred_binary, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "            # MÃ©tricas adicionais\n",
    "            precision_score_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall_score_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ MÃ©tricas em teste para {horizon}:\")\n",
    "            print(f\"  PR-AUC:      {test_pr_auc:.4f}\")\n",
    "            print(f\"  F1 Score:    {test_f1:.4f}\")\n",
    "            print(f\"  MCC:         {test_mcc:.4f}\")\n",
    "            print(f\"  Precision:   {precision_score_val:.4f}\")\n",
    "            print(f\"  Recall:      {recall_score_val:.4f}\")\n",
    "            print(f\"  Specificity: {specificity:.4f}\")\n",
    "            print(f\"  Threshold:   {best_threshold:.4f}\")\n",
    "            \n",
    "            # Log mÃ©tricas no MLflow\n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_metrics({\n",
    "                    f\"test_pr_auc_{horizon}\": test_pr_auc,\n",
    "                    f\"test_f1_{horizon}\": test_f1,\n",
    "                    f\"test_mcc_{horizon}\": test_mcc,\n",
    "                    f\"test_precision_{horizon}\": precision_score_val,\n",
    "                    f\"test_recall_{horizon}\": recall_score_val,\n",
    "                    f\"test_specificity_{horizon}\": specificity,\n",
    "                    f\"best_threshold_{horizon}\": best_threshold\n",
    "                })\n",
    "            \n",
    "            # 8. AnÃ¡lise de importÃ¢ncia de features\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': final_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nğŸ” Top 10 features mais importantes:\")\n",
    "            for idx, row in feature_importance.head(10).iterrows():\n",
    "                print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "            \n",
    "            # Salvar resultados (incluindo Ã­ndices para alinhamento no backtest)\n",
    "            results[horizon] = {\n",
    "                'model': final_model,\n",
    "                'calibrator': calibrator,\n",
    "                'scaler': scaler,\n",
    "                'labeler': labeler,\n",
    "                'threshold': best_threshold,\n",
    "                'metrics': {\n",
    "                    'pr_auc': test_pr_auc,\n",
    "                    'f1': test_f1,\n",
    "                    'mcc': test_mcc,\n",
    "                    'precision': precision_score_val,\n",
    "                    'recall': recall_score_val,\n",
    "                    'specificity': specificity\n",
    "                },\n",
    "                'confusion_matrix': cm,\n",
    "                'feature_importance': feature_importance,\n",
    "                'predictions': {\n",
    "                    'raw': y_test_pred_raw,\n",
    "                    'calibrated': y_test_pred_cal,\n",
    "                    'binary': y_test_pred_binary\n",
    "                },\n",
    "                'labels': y_test_binary,\n",
    "                'optimal_k': optimal_k,\n",
    "                'test_indices': X_test.index  # Salvar Ã­ndices para backtest\n",
    "            }\n",
    "            \n",
    "            # Limpeza de memÃ³ria apÃ³s cada horizonte\n",
    "            del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "            del X_train_scaled, X_val_scaled, X_test_scaled\n",
    "            del y_train_binary, y_val_binary, y_test_binary\n",
    "            del y_test_pred_raw, y_test_pred_cal, y_test_pred_binary\n",
    "            if 'temp_model' in locals():\n",
    "                del temp_model\n",
    "            gc.collect()\n",
    "            print(f\"ğŸ§¹ MemÃ³ria limpa apÃ³s horizonte {horizon}\")\n",
    "            \n",
    "            # Salvar modelo\n",
    "            import joblib\n",
    "            model_path = f\"{config.models_path}/xgb_{horizon}_{experiment_name if MLFLOW_AVAILABLE else 'local'}.pkl\"\n",
    "            os.makedirs(config.models_path, exist_ok=True)\n",
    "            joblib.dump({\n",
    "                'model': final_model,\n",
    "                'calibrator': calibrator,\n",
    "                'scaler': scaler,\n",
    "                'threshold': best_threshold\n",
    "            }, model_path)\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_artifact(model_path)\n",
    "        \n",
    "        finally:\n",
    "            if MLFLOW_AVAILABLE and run_context:\n",
    "                import mlflow\n",
    "                mlflow.end_run()\n",
    "    \n",
    "    # 9. AnÃ¡lise comparativa entre horizontes\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ“Š ANÃLISE COMPARATIVA ENTRE HORIZONTES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        horizon: {\n",
    "            'PR-AUC': results[horizon]['metrics']['pr_auc'],\n",
    "            'F1': results[horizon]['metrics']['f1'],\n",
    "            'MCC': results[horizon]['metrics']['mcc'],\n",
    "            'Precision': results[horizon]['metrics']['precision'],\n",
    "            'Recall': results[horizon]['metrics']['recall'],\n",
    "            'Optimal_k': results[horizon]['optimal_k']\n",
    "        }\n",
    "        for horizon in horizons\n",
    "    }).T\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Tabela Comparativa:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Identificar melhor horizonte\n",
    "    best_horizon_pr_auc = comparison_df['PR-AUC'].idxmax()\n",
    "    best_horizon_f1 = comparison_df['F1'].idxmax()\n",
    "    \n",
    "    print(f\"\\nğŸ† Melhores horizontes:\")\n",
    "    print(f\"  Melhor PR-AUC: {best_horizon_pr_auc} ({comparison_df.loc[best_horizon_pr_auc, 'PR-AUC']:.4f})\")\n",
    "    print(f\"  Melhor F1:     {best_horizon_f1} ({comparison_df.loc[best_horizon_f1, 'F1']:.4f})\")\n",
    "    \n",
    "    # 10. AnÃ¡lise de correlaÃ§Ã£o entre prediÃ§Ãµes\n",
    "    print(f\"\\nğŸ”— CorrelaÃ§Ã£o entre prediÃ§Ãµes dos horizontes:\")\n",
    "    pred_matrix = pd.DataFrame({\n",
    "        horizon: results[horizon]['predictions']['calibrated']\n",
    "        for horizon in horizons\n",
    "    })\n",
    "    \n",
    "    corr_matrix = pred_matrix.corr()\n",
    "    print(corr_matrix.round(3))\n",
    "    \n",
    "    # Salvar comparaÃ§Ã£o\n",
    "    comparison_df.to_csv(f\"{config.reports_path}/horizon_comparison_{experiment_name if MLFLOW_AVAILABLE else 'local'}.csv\")\n",
    "    \n",
    "    # Limpeza final de memÃ³ria\n",
    "    del features_with_funding, pred_matrix, corr_matrix, comparison_df\n",
    "    gc.collect()\n",
    "    print(f\"\\nğŸ§¹ Pipeline finalizado - memÃ³ria limpa\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… FunÃ§Ã£o run_multi_horizon_pipeline definida\")# %% [markdown]\n",
    "# ## 8. Pipeline LSTM para SÃ©ries Temporais\n",
    "# \n",
    "# ImplementaÃ§Ã£o de LSTM com:\n",
    "# - OtimizaÃ§Ã£o Bayesiana via Optuna\n",
    "# - PR-AUC como mÃ©trica alvo\n",
    "# - CalibraÃ§Ã£o isotÃ´nica\n",
    "# - Threshold otimizado no validation\n",
    "# - Export para produÃ§Ã£o via TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "023d2d4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# FunÃ§Ãµes de preparaÃ§Ã£o de dados sequenciais\n",
    "def make_sequences(X_df: pd.DataFrame, y_series: pd.Series, seq_len: int):\n",
    "    \"\"\"\n",
    "    Cria sequÃªncias para LSTM a partir de dados tabulares\n",
    "    \n",
    "    Args:\n",
    "        X_df: Features\n",
    "        y_series: Labels\n",
    "        seq_len: Comprimento da sequÃªncia\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: Array de sequÃªncias [n_samples, seq_len, n_features]\n",
    "        y_seq: Array de labels\n",
    "        idx_seq: Ãndices pandas correspondentes\n",
    "    \"\"\"\n",
    "    X = X_df.values.astype(np.float32)\n",
    "    y = y_series.values.astype(np.int64)\n",
    "    idx = X_df.index\n",
    "\n",
    "    X_seq, y_seq, idx_seq = [], [], []\n",
    "    for t in range(seq_len, len(X)):\n",
    "        X_seq.append(X[t-seq_len:t])\n",
    "        y_seq.append(y[t])\n",
    "        idx_seq.append(idx[t])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq), pd.Index(idx_seq)\n",
    "\n",
    "def train_val_test_split_time(X_df: pd.DataFrame, y: pd.Series, n_splits: int = 5):\n",
    "    \"\"\"\n",
    "    Split temporal para treino/validaÃ§Ã£o/teste\n",
    "    \n",
    "    Args:\n",
    "        X_df: Features\n",
    "        y: Labels\n",
    "        n_splits: NÃºmero de splits para TimeSeriesSplit\n",
    "        \n",
    "    Returns:\n",
    "        tr_idx: Ãndices de treino\n",
    "        va_idx: Ãndices de validaÃ§Ã£o\n",
    "        te_idx: Ãndices de teste\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = list(tscv.split(X_df))\n",
    "    \n",
    "    # PenÃºltimo split para validaÃ§Ã£o\n",
    "    (tr_idx, va_idx) = splits[-2]\n",
    "    # Ãšltimo split para teste\n",
    "    (tr2_idx, te_idx) = splits[-1]\n",
    "    \n",
    "    # Treino = do inÃ­cio atÃ© fim do penÃºltimo split\n",
    "    tr_idx_full = np.arange(0, va_idx[-1] + 1)\n",
    "    \n",
    "    return tr_idx_full, va_idx, te_idx\n",
    "\n",
    "def build_lstm_tensors(X_df: pd.DataFrame, y: pd.Series, seq_len: int, \n",
    "                      tr_idx, va_idx, te_idx):\n",
    "    \"\"\"\n",
    "    Prepara tensores para LSTM com escalonamento\n",
    "    \n",
    "    Args:\n",
    "        X_df: Features\n",
    "        y: Labels\n",
    "        seq_len: Comprimento da sequÃªncia\n",
    "        tr_idx, va_idx, te_idx: Ãndices dos splits\n",
    "        \n",
    "    Returns:\n",
    "        Tupla com tensores de treino/val/test, Ã­ndices e scaler\n",
    "    \"\"\"\n",
    "    # Escalonar features com base no treino\n",
    "    scaler = StandardScaler()\n",
    "    X_train_df = X_df.iloc[tr_idx]\n",
    "    scaler.fit(X_train_df.values)\n",
    "    \n",
    "    # Aplicar escalonamento\n",
    "    Xs = pd.DataFrame(\n",
    "        scaler.transform(X_df.values), \n",
    "        index=X_df.index, \n",
    "        columns=X_df.columns\n",
    "    )\n",
    "    \n",
    "    # Gerar sequÃªncias\n",
    "    X_seq, y_seq, idx_seq = make_sequences(Xs, y, seq_len)\n",
    "    \n",
    "    # Mapear Ã­ndices originais para Ã­ndices da sequÃªncia\n",
    "    idx_map = pd.Series(range(len(idx_seq)), index=idx_seq)\n",
    "    \n",
    "    # Obter Ã­ndices das sequÃªncias para cada split\n",
    "    tr = idx_map[idx_seq.intersection(X_df.index[tr_idx])].dropna().astype(int).values\n",
    "    va = idx_map[idx_seq.intersection(X_df.index[va_idx])].dropna().astype(int).values\n",
    "    te = idx_map[idx_seq.intersection(X_df.index[te_idx])].dropna().astype(int).values\n",
    "    \n",
    "    # Separar tensores\n",
    "    Xtr, ytr = X_seq[tr], y_seq[tr]\n",
    "    Xva, yva = X_seq[va], y_seq[va]\n",
    "    Xte, yte = X_seq[te], y_seq[te]\n",
    "    \n",
    "    return (Xtr, ytr, Xva, yva, Xte, yte, idx_seq[te], scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7be7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo LSTM para classificaÃ§Ã£o binÃ¡ria\n",
    "if TORCH_AVAILABLE:\n",
    "    class LSTMClassifier(nn.Module):\n",
    "        \"\"\"\n",
    "        LSTM para classificaÃ§Ã£o binÃ¡ria de sÃ©ries temporais\n",
    "        \"\"\"\n",
    "        def __init__(self, in_dim: int, hidden: int, layers: int, dropout: float):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(\n",
    "                in_dim, hidden, \n",
    "                num_layers=layers, \n",
    "                batch_first=True, \n",
    "                dropout=dropout if layers > 1 else 0\n",
    "            )\n",
    "            self.head = nn.Linear(hidden, 1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x: [batch, seq, feat]\n",
    "            out, _ = self.lstm(x)\n",
    "            # Usar apenas Ãºltimo timestep\n",
    "            logit = self.head(out[:, -1, :])\n",
    "            return logit.squeeze(1)  # Retorna logits (sem sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3314238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunÃ§Ãµes de treinamento e avaliaÃ§Ã£o\n",
    "if TORCH_AVAILABLE:\n",
    "    def train_one_epoch(model, optimizer, loss_fn, loader, device):\n",
    "        \"\"\"\n",
    "        Treina modelo por uma Ã©poca\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            optimizer: Otimizador\n",
    "            loss_fn: FunÃ§Ã£o de perda\n",
    "            loader: DataLoader\n",
    "            device: Device (cuda/cpu)\n",
    "            \n",
    "        Returns:\n",
    "            Loss mÃ©dio da Ã©poca\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping para estabilidade\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            \n",
    "        return total_loss / len(loader.dataset)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_ap(model, loader, device):\n",
    "        \"\"\"\n",
    "        Avalia modelo com Average Precision (PR-AUC)\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            loader: DataLoader\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            ap: Average Precision score\n",
    "            probs: Probabilidades preditas\n",
    "            labels: Labels verdadeiros\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        probs_list, labels_list = [], []\n",
    "        \n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs_list.append(probs)\n",
    "            labels_list.append(yb.numpy())\n",
    "        \n",
    "        labels = np.concatenate(labels_list)\n",
    "        probs = np.concatenate(probs_list)\n",
    "        \n",
    "        ap = average_precision_score(labels, probs)\n",
    "        return ap, probs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d65e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo do Optuna para LSTM\n",
    "if TORCH_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    def objective_lstm(trial, X_train, y_train, X_val, y_val, seq_len, device):\n",
    "        \"\"\"\n",
    "        FunÃ§Ã£o objetivo para otimizaÃ§Ã£o com Optuna\n",
    "        \n",
    "        Args:\n",
    "            trial: Trial do Optuna\n",
    "            X_train, y_train: Dados de treino\n",
    "            X_val, y_val: Dados de validaÃ§Ã£o\n",
    "            seq_len: Comprimento da sequÃªncia\n",
    "            device: Device (cuda/cpu)\n",
    "            \n",
    "        Returns:\n",
    "            Best Average Precision obtido\n",
    "        \"\"\"\n",
    "        # HiperparÃ¢metros para otimizar\n",
    "        hidden = trial.suggest_categorical(\"hidden\", [64, 128, 256])\n",
    "        layers = trial.suggest_int(\"layers\", 1, 3)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "        \n",
    "        # Criar modelo\n",
    "        model = LSTMClassifier(\n",
    "            X_train.shape[-1], hidden, layers, dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        # BCEWithLogitsLoss com peso para desbalanceamento\n",
    "        pos_ratio = y_train.mean()\n",
    "        pos_ratio = float(pos_ratio if pos_ratio > 0 else 1e-6)\n",
    "        pos_weight = torch.tensor((1 - pos_ratio) / pos_ratio, device=device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Otimizador\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=wd\n",
    "        )\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32)\n",
    "        )\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(X_val, dtype=torch.float32),\n",
    "            torch.tensor(y_val, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=512, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Treinar com early stopping via pruning\n",
    "        best_ap = 0.0\n",
    "        patience_counter = 0\n",
    "        patience = 10\n",
    "        \n",
    "        for epoch in range(60):\n",
    "            # Treinar\n",
    "            train_loss = train_one_epoch(\n",
    "                model, optimizer, loss_fn, train_loader, device\n",
    "            )\n",
    "            \n",
    "            # Avaliar\n",
    "            ap, _, _ = eval_ap(model, val_loader, device)\n",
    "            \n",
    "            # Reportar ao Optuna\n",
    "            trial.report(ap, epoch)\n",
    "            \n",
    "            # Pruning\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "            # Track best\n",
    "            if ap > best_ap:\n",
    "                best_ap = ap\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "                \n",
    "        return best_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97efe3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de treinamento com Optuna\n",
    "if TORCH_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    def fit_lstm_with_optuna(Xtr, ytr, Xva, yva, n_trials=50, \n",
    "                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Treina LSTM com otimizaÃ§Ã£o Bayesiana\n",
    "        \n",
    "        Args:\n",
    "            Xtr, ytr: Dados de treino\n",
    "            Xva, yva: Dados de validaÃ§Ã£o\n",
    "            n_trials: NÃºmero de trials do Optuna\n",
    "            device: Device para treino\n",
    "            \n",
    "        Returns:\n",
    "            model: Modelo treinado\n",
    "            best_params: Melhores hiperparÃ¢metros\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ” OtimizaÃ§Ã£o Bayesiana com Optuna ({n_trials} trials)\")\n",
    "        \n",
    "        # Criar estudo\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner()\n",
    "        )\n",
    "        \n",
    "        # Otimizar\n",
    "        study.optimize(\n",
    "            lambda t: objective_lstm(t, Xtr, ytr, Xva, yva, Xtr.shape[1], device),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"âœ… Melhor AP em validaÃ§Ã£o: {study.best_value:.4f}\")\n",
    "        print(f\"ğŸ“Š Melhores parÃ¢metros: {best_params}\")\n",
    "        \n",
    "        # Re-treinar com melhores parÃ¢metros no conjunto completo (treino + val)\n",
    "        print(f\"\\nğŸ‹ï¸ Treinando modelo final...\")\n",
    "        \n",
    "        model = LSTMClassifier(\n",
    "            Xtr.shape[-1], \n",
    "            best_params['hidden'],\n",
    "            best_params['layers'],\n",
    "            best_params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Combinar treino e validaÃ§Ã£o\n",
    "        X_combined = np.concatenate([Xtr, Xva])\n",
    "        y_combined = np.concatenate([ytr, yva])\n",
    "        \n",
    "        # Loss com peso para manter balanceamento de classes\n",
    "        pos_ratio = y_combined.mean()\n",
    "        pos_ratio = float(pos_ratio if pos_ratio > 0 else 1e-6)\n",
    "        pos_weight = torch.tensor((1 - pos_ratio) / pos_ratio, device=device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=best_params['lr'],\n",
    "            weight_decay=best_params['weight_decay']\n",
    "        )\n",
    "        \n",
    "        combined_dataset = TensorDataset(\n",
    "            torch.tensor(X_combined, dtype=torch.float32),\n",
    "            torch.tensor(y_combined, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        combined_loader = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # Treinar por mais Ã©pocas (1.5x)\n",
    "        for epoch in range(90):\n",
    "            train_loss = train_one_epoch(\n",
    "                model, optimizer, loss_fn, combined_loader, device\n",
    "            )\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  Ã‰poca {epoch}: Loss = {train_loss:.4f}\")\n",
    "        \n",
    "        return model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57c7f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CalibraÃ§Ã£o e seleÃ§Ã£o de threshold\n",
    "if TORCH_AVAILABLE:\n",
    "    @torch.no_grad()\n",
    "    def calibrate_and_choose_threshold(model, Xva, yva, device):\n",
    "        \"\"\"\n",
    "        Calibra probabilidades e escolhe threshold Ã³timo\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM treinado\n",
    "            Xva, yva: Dados de validaÃ§Ã£o\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            calibrator: Calibrador isotÃ´nico\n",
    "            threshold: Threshold Ã³timo\n",
    "        \"\"\"\n",
    "        from sklearn.isotonic import IsotonicRegression\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Obter probabilidades\n",
    "        Xva_tensor = torch.tensor(Xva, dtype=torch.float32).to(device)\n",
    "        logits = model(Xva_tensor)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        # CalibraÃ§Ã£o isotÃ´nica\n",
    "        calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        probs_cal = calibrator.fit_transform(probs, yva)\n",
    "        \n",
    "        # Escolher threshold que maximiza F1\n",
    "        precision, recall, thresholds = precision_recall_curve(yva, probs_cal)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
    "        \n",
    "        # Ignorar Ãºltimo elemento (threshold = 1.0)\n",
    "        best_idx = np.nanargmax(f1_scores[:-1])\n",
    "        best_threshold = float(thresholds[best_idx]) if len(thresholds) > 0 else 0.5\n",
    "        best_f1 = f1_scores[best_idx]\n",
    "        \n",
    "        print(f\"\\nğŸ“ CalibraÃ§Ã£o completa\")\n",
    "        print(f\"  Threshold Ã³timo: {best_threshold:.3f}\")\n",
    "        print(f\"  F1 em validaÃ§Ã£o: {best_f1:.3f}\")\n",
    "        \n",
    "        # Calcular Brier score\n",
    "        from sklearn.metrics import brier_score_loss\n",
    "        brier_before = brier_score_loss(yva, probs)\n",
    "        brier_after = brier_score_loss(yva, probs_cal)\n",
    "        print(f\"  Brier Score: {brier_before:.4f} â†’ {brier_after:.4f}\")\n",
    "        \n",
    "        return calibrator, best_threshold\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_proba(model, X, device):\n",
    "        \"\"\"\n",
    "        Prediz probabilidades para conjunto de dados\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            X: Features\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            Probabilidades preditas\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        logits = model(X_tensor)\n",
    "        return torch.sigmoid(logits).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d80a7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AvaliaÃ§Ã£o no teste e geraÃ§Ã£o de sinais\n",
    "if TORCH_AVAILABLE:\n",
    "    def evaluate_on_test(model, Xte, yte, calibrator, threshold, test_index, device):\n",
    "        \"\"\"\n",
    "        Avalia modelo no conjunto de teste\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM treinado\n",
    "            Xte, yte: Dados de teste\n",
    "            calibrator: Calibrador isotÃ´nico\n",
    "            threshold: Threshold escolhido\n",
    "            test_index: Ãndices do teste\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            DicionÃ¡rio com mÃ©tricas e prediÃ§Ãµes\n",
    "        \"\"\"\n",
    "        # PrediÃ§Ãµes\n",
    "        probs = predict_proba(model, Xte, device)\n",
    "        probs_cal = calibrator.transform(probs)\n",
    "        preds = (probs_cal >= threshold).astype(int)\n",
    "        \n",
    "        # MÃ©tricas\n",
    "        ap = average_precision_score(yte, probs_cal)\n",
    "        f1 = f1_score(yte, preds)\n",
    "        acc = accuracy_score(yte, preds)\n",
    "        \n",
    "        # MCC e Brier\n",
    "        mcc = matthews_corrcoef(yte, preds)\n",
    "        brier = brier_score_loss(yte, probs_cal)\n",
    "        \n",
    "        # Confusion matrix - forÃ§ar shape 2x2 para evitar erro quando sÃ³ uma classe Ã© predita\n",
    "        cm = confusion_matrix(yte, preds, labels=[0, 1])\n",
    "        \n",
    "        # Desempacotar matriz de forma segura\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Fallback se algo der errado\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Sinais para backtest (-1 para short, +1 para long)\n",
    "        signals = pd.Series((preds * 2 - 1), index=test_index)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Resultados no Teste:\")\n",
    "        print(f\"  AP (PR-AUC): {ap:.4f}\")\n",
    "        print(f\"  F1 Score:    {f1:.4f}\")\n",
    "        print(f\"  Accuracy:    {acc:.4f}\")\n",
    "        print(f\"  MCC:         {mcc:.4f}\")\n",
    "        print(f\"  Brier Score: {brier:.4f}\")\n",
    "        print(f\"\\n  Confusion Matrix:\")\n",
    "        print(f\"    TN: {tn:4d}  FP: {fp:4d}\")\n",
    "        print(f\"    FN: {fn:4d}  TP: {tp:4d}\")\n",
    "        \n",
    "        return {\n",
    "            \"ap\": ap,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy\": acc,\n",
    "            \"mcc\": mcc,\n",
    "            \"brier\": brier,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"proba\": probs_cal,\n",
    "            \"pred\": preds,\n",
    "            \"signals\": signals\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "303d9b0f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Export para produÃ§Ã£o\n",
    "if TORCH_AVAILABLE:\n",
    "    def export_torchscript(model, in_dim, seq_len, path=\"lstm_model.pt\", device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Exporta modelo para TorchScript\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            in_dim: DimensÃ£o de entrada\n",
    "            seq_len: Comprimento da sequÃªncia\n",
    "            path: Caminho para salvar\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            Caminho do arquivo salvo\n",
    "        \"\"\"\n",
    "        model_cpu = model.to(device).eval()\n",
    "        \n",
    "        # Input dummy para tracing\n",
    "        dummy_input = torch.randn(1, seq_len, in_dim).to(device)\n",
    "        \n",
    "        # Trace e salvar\n",
    "        traced_model = torch.jit.trace(model_cpu, dummy_input)\n",
    "        traced_model.save(path)\n",
    "        \n",
    "        print(f\"âœ… Modelo exportado para: {path}\")\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b3323f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline LSTM definido e pronto para uso\n"
     ]
    }
   ],
   "source": [
    "# Pipeline completo LSTM\n",
    "def run_lstm_pipeline(X_df: pd.DataFrame, \n",
    "                     y_series: pd.Series,\n",
    "                     seq_len: int = 64,\n",
    "                     n_trials: int = 20,  # Reduzido para otimizaÃ§Ã£o de memÃ³ria\n",
    "                     device: str = None,\n",
    "                     horizon: str = \"lstm\") -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline completo para treinar LSTM com Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_df: DataFrame com features\n",
    "        y_series: Series com labels binÃ¡rias\n",
    "        seq_len: Comprimento das sequÃªncias (default: 64 = 16 horas em 15min)\n",
    "        n_trials: NÃºmero de trials do Optuna\n",
    "        device: Device para treino (None = auto-detectar)\n",
    "        horizon: Nome do horizonte para logging\n",
    "        \n",
    "    Returns:\n",
    "        DicionÃ¡rio com resultados compatÃ­vel com run_multi_horizon_backtest\n",
    "    \"\"\"\n",
    "    if not TORCH_AVAILABLE:\n",
    "        print(\"âš ï¸ PyTorch nÃ£o disponÃ­vel - pulando LSTM\")\n",
    "        return {}\n",
    "    \n",
    "    if not OPTUNA_AVAILABLE:\n",
    "        print(\"âš ï¸ Optuna nÃ£o disponÃ­vel - pulando otimizaÃ§Ã£o LSTM\")\n",
    "        return {}\n",
    "    \n",
    "    # Auto-detectar device\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ PIPELINE LSTM - Horizonte: {horizon}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“Š Dataset: {len(X_df)} amostras, {X_df.shape[1]} features\")\n",
    "    print(f\"âš™ï¸  Device: {device}\")\n",
    "    print(f\"ğŸ“ SequÃªncia: {seq_len} timesteps\")\n",
    "    \n",
    "    # 1. Split temporal\n",
    "    print(f\"\\n1ï¸âƒ£ Preparando splits temporais...\")\n",
    "    tr_idx, va_idx, te_idx = train_val_test_split_time(X_df, y_series)\n",
    "    print(f\"  Treino:    {len(tr_idx)} amostras\")\n",
    "    print(f\"  ValidaÃ§Ã£o: {len(va_idx)} amostras\")\n",
    "    print(f\"  Teste:     {len(te_idx)} amostras\")\n",
    "    \n",
    "    # 2. Preparar tensores\n",
    "    print(f\"\\n2ï¸âƒ£ Gerando sequÃªncias e escalonando features...\")\n",
    "    (Xtr, ytr, Xva, yva, Xte, yte, test_index, scaler) = build_lstm_tensors(\n",
    "        X_df, y_series, seq_len, tr_idx, va_idx, te_idx\n",
    "    )\n",
    "    print(f\"  SequÃªncias treino: {Xtr.shape}\")\n",
    "    print(f\"  SequÃªncias valid:  {Xva.shape}\")\n",
    "    print(f\"  SequÃªncias teste:  {Xte.shape}\")\n",
    "    \n",
    "    # 3. OtimizaÃ§Ã£o com Optuna\n",
    "    print(f\"\\n3ï¸âƒ£ OtimizaÃ§Ã£o Bayesiana...\")\n",
    "    model, best_params = fit_lstm_with_optuna(\n",
    "        Xtr, ytr, Xva, yva, n_trials=n_trials, device=device\n",
    "    )\n",
    "    \n",
    "    # 4. CalibraÃ§Ã£o e threshold\n",
    "    print(f\"\\n4ï¸âƒ£ CalibraÃ§Ã£o de probabilidades...\")\n",
    "    calibrator, threshold = calibrate_and_choose_threshold(\n",
    "        model, Xva, yva, device\n",
    "    )\n",
    "    \n",
    "    # 5. AvaliaÃ§Ã£o no teste\n",
    "    print(f\"\\n5ï¸âƒ£ AvaliaÃ§Ã£o no conjunto de teste...\")\n",
    "    eval_results = evaluate_on_test(\n",
    "        model, Xte, yte, calibrator, threshold, test_index, device\n",
    "    )\n",
    "    \n",
    "    # 6. Export para produÃ§Ã£o\n",
    "    print(f\"\\n6ï¸âƒ£ Exportando modelo...\")\n",
    "    model_path = f\"artifacts/models/lstm_{horizon}.pt\"\n",
    "    os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "    \n",
    "    export_path = export_torchscript(\n",
    "        model, Xtr.shape[-1], seq_len, \n",
    "        path=model_path, device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "    # 7. Preparar resultados no formato esperado\n",
    "    results = {\n",
    "        horizon: {\n",
    "            \"best_params\": best_params,\n",
    "            \"threshold\": threshold,\n",
    "            \"test_metrics\": {\n",
    "                \"accuracy\": eval_results[\"accuracy\"],\n",
    "                \"precision\": eval_results[\"confusion_matrix\"][1,1] / \n",
    "                            (eval_results[\"confusion_matrix\"][1,1] + \n",
    "                             eval_results[\"confusion_matrix\"][0,1] + 1e-10),\n",
    "                \"recall\": eval_results[\"confusion_matrix\"][1,1] / \n",
    "                         (eval_results[\"confusion_matrix\"][1,1] + \n",
    "                          eval_results[\"confusion_matrix\"][1,0] + 1e-10),\n",
    "                \"f1\": eval_results[\"f1\"],\n",
    "                \"pr_auc\": eval_results[\"ap\"],\n",
    "                \"mcc\": eval_results[\"mcc\"],\n",
    "                \"brier\": eval_results[\"brier\"]\n",
    "            },\n",
    "            \"test_indices\": test_index.tolist(),\n",
    "            \"predictions\": {\n",
    "                \"proba\": eval_results[\"proba\"],\n",
    "                \"binary\": eval_results[\"pred\"]\n",
    "            },\n",
    "            \"signals\": eval_results[\"signals\"],\n",
    "            \"confusion_matrix\": eval_results[\"confusion_matrix\"],\n",
    "            \"artifact\": export_path,\n",
    "            \"scaler\": scaler,\n",
    "            \"model_type\": \"lstm\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # MLflow logging se disponÃ­vel\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        \n",
    "        mlflow.log_params({\n",
    "            f\"lstm_seq_len_{horizon}\": seq_len,\n",
    "            f\"lstm_device_{horizon}\": device,\n",
    "            **{f\"lstm_{k}_{horizon}\": v for k, v in best_params.items()}\n",
    "        })\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            f\"lstm_pr_auc_test_{horizon}\": eval_results[\"ap\"],\n",
    "            f\"lstm_f1_test_{horizon}\": eval_results[\"f1\"],\n",
    "            f\"lstm_mcc_test_{horizon}\": eval_results[\"mcc\"],\n",
    "            f\"lstm_brier_test_{horizon}\": eval_results[\"brier\"]\n",
    "        })\n",
    "        \n",
    "        mlflow.log_artifact(export_path)\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline LSTM completo para horizonte {horizon}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Pipeline LSTM definido e pronto para uso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b176c",
   "metadata": {},
   "source": [
    "## 9. Sistema de Backtest Multi-Horizonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e65596fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Usando implementaÃ§Ã£o local do BacktestEngine (imports do projeto nÃ£o disponÃ­veis)\n",
      "âœ… FunÃ§Ã£o run_multi_horizon_backtest definida\n"
     ]
    }
   ],
   "source": [
    "# Definir BacktestConfig e BacktestEngine caso nÃ£o estejam disponÃ­veis via import local\n",
    "# NOTA: Esta Ã© uma implementaÃ§Ã£o de fallback quando os imports do projeto nÃ£o estÃ£o disponÃ­veis\n",
    "# A implementaÃ§Ã£o principal estÃ¡ em src/backtest/engine.py\n",
    "# Ambas as implementaÃ§Ãµes mantÃªm a mesma interface e lÃ³gica de custos\n",
    "if not LOCAL_IMPORTS_AVAILABLE:\n",
    "    print(\"âš ï¸ Usando implementaÃ§Ã£o local do BacktestEngine (imports do projeto nÃ£o disponÃ­veis)\")\n",
    "    \n",
    "    @dataclass\n",
    "    class BacktestConfig:\n",
    "        \"\"\"ConfiguraÃ§Ã£o para backtest - versÃ£o local de fallback\"\"\"\n",
    "        initial_capital: float = 100000\n",
    "        fee_bps: float = 5\n",
    "        slippage_bps: float = 10\n",
    "        funding_apr_est: float = 0.00\n",
    "        borrow_apr_est: float = 0.00\n",
    "        execution_rule: str = 'next_bar_open'\n",
    "        max_leverage: float = 1.0\n",
    "        position_mode: str = 'long_short'\n",
    "        \n",
    "    class BacktestEngine:\n",
    "        \"\"\"Engine simplificado de backtest\"\"\"\n",
    "        def __init__(self, config: BacktestConfig):\n",
    "            self.config = config\n",
    "            \n",
    "        def run_backtest(self, df: pd.DataFrame, signals: pd.Series):\n",
    "            \"\"\"Executa backtest com PnL real\"\"\"\n",
    "            # Garantir alinhamento de Ã­ndices\n",
    "            perf = pd.DataFrame(index=signals.index)\n",
    "            perf['signals'] = signals\n",
    "            perf['close'] = df.loc[signals.index, 'close']\n",
    "            perf['returns'] = perf['close'].pct_change()\n",
    "            \n",
    "            # EstratÃ©gia: sinal em t, execuÃ§Ã£o em t+1\n",
    "            perf['strategy_returns'] = perf['returns'] * perf['signals'].shift(1)\n",
    "            \n",
    "            # Aplicar custos\n",
    "            position_changes = signals.diff().abs()\n",
    "            costs = position_changes * (self.config.fee_bps + self.config.slippage_bps) / 10000\n",
    "            perf['net_returns'] = perf['strategy_returns'] - costs\n",
    "            \n",
    "            # Calcular trades com PnL real\n",
    "            trades = pd.DataFrame()\n",
    "            trade_signals = signals.diff()\n",
    "            entries = trade_signals != 0\n",
    "            \n",
    "            if entries.any():\n",
    "                entry_points = signals.index[entries]\n",
    "                trade_list = []\n",
    "                \n",
    "                for i, entry_time in enumerate(entry_points[:-1]):\n",
    "                    exit_time = entry_points[i+1]\n",
    "                    entry_idx = signals.index.get_loc(entry_time)\n",
    "                    exit_idx = signals.index.get_loc(exit_time)\n",
    "                    \n",
    "                    # PnL real baseado nos retornos\n",
    "                    trade_returns = perf['net_returns'].iloc[entry_idx+1:exit_idx+1]\n",
    "                    trade_pnl = (1 + trade_returns).prod() - 1\n",
    "                    \n",
    "                    trade_list.append({\n",
    "                        'entry_time': entry_time,\n",
    "                        'exit_time': exit_time,\n",
    "                        'pnl': trade_pnl * self.config.initial_capital\n",
    "                    })\n",
    "                \n",
    "                trades = pd.DataFrame(trade_list)\n",
    "            \n",
    "            return perf, trades\n",
    "\n",
    "def run_multi_horizon_backtest(df: pd.DataFrame,\n",
    "                              results: Dict,\n",
    "                              initial_capital: float = 100000,\n",
    "                              fee_bps: float = 5,\n",
    "                              slippage_bps: float = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Executa backtest para mÃºltiplos horizontes e compara performance\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com OHLC\n",
    "        results: Resultados do pipeline multi-horizonte\n",
    "        initial_capital: Capital inicial\n",
    "        fee_bps: Taxa em basis points\n",
    "        slippage_bps: Slippage em basis points\n",
    "        \n",
    "    Returns:\n",
    "        Dict com resultados de backtest por horizonte\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ“Š BACKTEST MULTI-HORIZONTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Usar import local ou classe definida acima\n",
    "    if LOCAL_IMPORTS_AVAILABLE:\n",
    "        from src.backtest.engine import BacktestEngine, BacktestConfig\n",
    "    \n",
    "    backtest_results = {}\n",
    "    \n",
    "    for horizon, horizon_results in results.items():\n",
    "        print(f\"\\nâ±ï¸ Backtesting horizonte: {horizon}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Configurar backtest\n",
    "        config = BacktestConfig(\n",
    "            initial_capital=initial_capital,\n",
    "            fee_bps=fee_bps,\n",
    "            slippage_bps=slippage_bps,\n",
    "            funding_apr_est=0.00,  # Simplificado para demo\n",
    "            execution_rule='next_bar_open'\n",
    "        )\n",
    "        \n",
    "        # Gerar sinais a partir das prediÃ§Ãµes com Ã­ndices corretos\n",
    "        predictions = horizon_results['predictions']['binary']\n",
    "        labels = horizon_results['labels']\n",
    "        \n",
    "        # Usar o Ã­ndice do conjunto de teste apÃ³s as mÃ¡scaras\n",
    "        # Isso garante alinhamento correto com os dados\n",
    "        test_indices = horizon_results.get('test_indices', None)\n",
    "        if test_indices is None:\n",
    "            # Fallback se nÃ£o tivermos os Ã­ndices salvos\n",
    "            test_start_idx = len(df) - len(predictions)\n",
    "            test_indices = df.index[test_start_idx:test_start_idx + len(predictions)]\n",
    "        \n",
    "        signals = pd.Series(predictions * 2 - 1, index=test_indices)  # Converter 0/1 para -1/1\n",
    "        \n",
    "        # Executar backtest\n",
    "        bt_engine = BacktestEngine(config)\n",
    "        perf, trades = bt_engine.run_backtest(df.loc[signals.index], signals)\n",
    "        \n",
    "        # MÃ©tricas de trading - usar net_returns que inclui custos\n",
    "        returns = perf['net_returns'].dropna()\n",
    "        cumulative_return = (1 + returns).cumprod().iloc[-1] - 1 if len(returns) > 0 else 0\n",
    "        \n",
    "        # Sharpe Ratio\n",
    "        if returns.std() > 0:\n",
    "            sharpe = returns.mean() / returns.std() * np.sqrt(365 * 24 * 4)  # Anualizado para 15min\n",
    "        else:\n",
    "            sharpe = 0\n",
    "            \n",
    "        # Maximum Drawdown\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Calmar Ratio\n",
    "        calmar = cumulative_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
    "        \n",
    "        # Win rate\n",
    "        winning_trades = trades[trades['pnl'] > 0] if len(trades) > 0 else pd.DataFrame()\n",
    "        win_rate = len(winning_trades) / len(trades) if len(trades) > 0 else 0\n",
    "        \n",
    "        # Profit factor\n",
    "        gross_profit = trades[trades['pnl'] > 0]['pnl'].sum() if len(trades) > 0 else 0\n",
    "        gross_loss = abs(trades[trades['pnl'] < 0]['pnl'].sum()) if len(trades) > 0 else 0\n",
    "        profit_factor = gross_profit / gross_loss if gross_loss > 0 else 0\n",
    "        \n",
    "        # Turnover\n",
    "        position_changes = signals.diff().abs()\n",
    "        turnover = position_changes.sum() / len(signals)\n",
    "        \n",
    "        # Expected Value per trade\n",
    "        ev_per_trade = trades['pnl'].mean() if len(trades) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ MÃ©tricas de Trading para {horizon}:\")\n",
    "        print(f\"  Retorno Total:    {cumulative_return:+.2%}\")\n",
    "        print(f\"  Sharpe Ratio:     {sharpe:.3f}\")\n",
    "        print(f\"  Max Drawdown:     {max_drawdown:.2%}\")\n",
    "        print(f\"  Calmar Ratio:     {calmar:.3f}\")\n",
    "        print(f\"  Win Rate:         {win_rate:.2%}\")\n",
    "        print(f\"  Profit Factor:    {profit_factor:.2f}\")\n",
    "        print(f\"  Turnover:         {turnover:.3f}\")\n",
    "        print(f\"  EV per Trade:     ${ev_per_trade:.2f}\")\n",
    "        print(f\"  Num Trades:       {len(trades)}\")\n",
    "        \n",
    "        # Comparar com Buy & Hold\n",
    "        buy_hold_return = (df.loc[signals.index, 'close'].iloc[-1] / \n",
    "                          df.loc[signals.index, 'close'].iloc[0] - 1)\n",
    "        outperformance = cumulative_return - buy_hold_return\n",
    "        \n",
    "        print(f\"\\n  Buy & Hold:       {buy_hold_return:+.2%}\")\n",
    "        print(f\"  Outperformance:   {outperformance:+.2%}\")\n",
    "        \n",
    "        # Salvar resultados\n",
    "        backtest_results[horizon] = {\n",
    "            'performance': perf,\n",
    "            'trades': trades,\n",
    "            'metrics': {\n",
    "                'cumulative_return': cumulative_return,\n",
    "                'sharpe_ratio': sharpe,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'calmar_ratio': calmar,\n",
    "                'win_rate': win_rate,\n",
    "                'profit_factor': profit_factor,\n",
    "                'turnover': turnover,\n",
    "                'ev_per_trade': ev_per_trade,\n",
    "                'num_trades': len(trades),\n",
    "                'buy_hold_return': buy_hold_return,\n",
    "                'outperformance': outperformance\n",
    "            },\n",
    "            'signals': signals,\n",
    "            'returns': returns\n",
    "        }\n",
    "    \n",
    "    # AnÃ¡lise comparativa\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ† COMPARAÃ‡ÃƒO ENTRE HORIZONTES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_metrics = pd.DataFrame({\n",
    "        horizon: backtest_results[horizon]['metrics']\n",
    "        for horizon in backtest_results.keys()\n",
    "    }).T\n",
    "    \n",
    "    print(\"\\nğŸ“Š Tabela Comparativa de Backtest:\")\n",
    "    print(comparison_metrics.round(3))\n",
    "    \n",
    "    # Identificar melhor horizonte por diferentes mÃ©tricas\n",
    "    best_return = comparison_metrics['cumulative_return'].idxmax()\n",
    "    best_sharpe = comparison_metrics['sharpe_ratio'].idxmax()\n",
    "    best_calmar = comparison_metrics['calmar_ratio'].idxmax()\n",
    "    \n",
    "    print(f\"\\nğŸ¥‡ Melhores Horizontes:\")\n",
    "    print(f\"  Melhor Retorno: {best_return} ({comparison_metrics.loc[best_return, 'cumulative_return']:+.2%})\")\n",
    "    print(f\"  Melhor Sharpe:  {best_sharpe} ({comparison_metrics.loc[best_sharpe, 'sharpe_ratio']:.3f})\")\n",
    "    print(f\"  Melhor Calmar:  {best_calmar} ({comparison_metrics.loc[best_calmar, 'calmar_ratio']:.3f})\")\n",
    "    \n",
    "    # AnÃ¡lise de correlaÃ§Ã£o de retornos\n",
    "    print(f\"\\nğŸ”— CorrelaÃ§Ã£o entre retornos dos horizontes:\")\n",
    "    returns_df = pd.DataFrame({\n",
    "        horizon: backtest_results[horizon]['returns']\n",
    "        for horizon in backtest_results.keys()\n",
    "    })\n",
    "    \n",
    "    # Alinhar Ã­ndices\n",
    "    returns_df = returns_df.dropna()\n",
    "    if len(returns_df) > 0:\n",
    "        corr_matrix = returns_df.corr()\n",
    "        print(corr_matrix.round(3))\n",
    "    \n",
    "    # Salvar resultados\n",
    "    os.makedirs(config.artifacts_path + \"/backtest\", exist_ok=True)\n",
    "    comparison_metrics.to_csv(f\"{config.artifacts_path}/backtest/horizon_backtest_comparison.csv\")\n",
    "    \n",
    "    # Plotar equity curves (opcional - salvando dados para visualizaÃ§Ã£o posterior)\n",
    "    equity_curves = {}\n",
    "    for horizon in backtest_results.keys():\n",
    "        returns = backtest_results[horizon]['returns']\n",
    "        equity = (1 + returns).cumprod()\n",
    "        equity_curves[horizon] = equity\n",
    "    \n",
    "    equity_df = pd.DataFrame(equity_curves)\n",
    "    equity_df.to_csv(f\"{config.artifacts_path}/backtest/equity_curves.csv\")\n",
    "    \n",
    "    print(f\"\\nâœ… Resultados salvos em {config.artifacts_path}/backtest/\")\n",
    "    \n",
    "    return backtest_results\n",
    "\n",
    "print(\"âœ… FunÃ§Ã£o run_multi_horizon_backtest definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46742787",
   "metadata": {},
   "source": [
    "## 9. EstratÃ©gia Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be5e8ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FunÃ§Ã£o create_ensemble_signals definida\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble_signals(results: Dict, \n",
    "                           weights: Dict = None,\n",
    "                           voting: str = 'soft') -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cria sinais ensemble combinando mÃºltiplos horizontes\n",
    "    \n",
    "    Args:\n",
    "        results: Resultados do pipeline multi-horizonte\n",
    "        weights: Pesos para cada horizonte (None = igual peso)\n",
    "        voting: 'soft' (mÃ©dia ponderada) ou 'hard' (votaÃ§Ã£o majoritÃ¡ria)\n",
    "        \n",
    "    Returns:\n",
    "        Series com sinais combinados\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {h: 1.0/len(results) for h in results.keys()}\n",
    "    \n",
    "    # Coletar probabilidades calibradas\n",
    "    probabilities = {}\n",
    "    for horizon, horizon_results in results.items():\n",
    "        probs = horizon_results['predictions']['calibrated']\n",
    "        probabilities[horizon] = probs\n",
    "    \n",
    "    # Criar DataFrame alinhado\n",
    "    prob_df = pd.DataFrame(probabilities)\n",
    "    \n",
    "    if voting == 'soft':\n",
    "        # MÃ©dia ponderada das probabilidades\n",
    "        weighted_probs = sum(prob_df[h] * weights[h] for h in prob_df.columns)\n",
    "        # Aplicar threshold mÃ©dio dos horizontes\n",
    "        avg_threshold = np.mean([results[h]['threshold'] for h in results.keys()])\n",
    "        signals = (weighted_probs >= avg_threshold).astype(int) * 2 - 1\n",
    "    else:  # voting == 'hard'\n",
    "        # VotaÃ§Ã£o majoritÃ¡ria\n",
    "        binary_preds = pd.DataFrame({\n",
    "            h: (prob_df[h] >= results[h]['threshold']).astype(int)\n",
    "            for h in prob_df.columns\n",
    "        })\n",
    "        signals = (binary_preds.mean(axis=1) >= 0.5).astype(int) * 2 - 1\n",
    "    \n",
    "    return signals\n",
    "\n",
    "print(\"âœ… FunÃ§Ã£o create_ensemble_signals definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271d42e",
   "metadata": {},
   "source": [
    "## 10. FunÃ§Ãµes de DemonstraÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4e57ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FunÃ§Ãµes de demonstraÃ§Ã£o definidas\n"
     ]
    }
   ],
   "source": [
    "def create_sample_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cria features bÃ¡sicas para demonstraÃ§Ã£o\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Returns\n",
    "    for period in [1, 5, 10, 20, 50]:\n",
    "        features[f'return_{period}'] = df['close'].pct_change(period)\n",
    "    \n",
    "    # Moving averages\n",
    "    for period in [10, 20, 50, 100]:\n",
    "        features[f'ma_{period}'] = df['close'].rolling(period).mean() / df['close'] - 1\n",
    "    \n",
    "    # Volume\n",
    "    features['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    features['volume_ma_20'] = df['volume'].rolling(20).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility_20'] = df['close'].pct_change().rolling(20).std()\n",
    "    features['high_low_ratio'] = df['high'] / df['low'] - 1\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    ma_20 = df['close'].rolling(20).mean()\n",
    "    std_20 = df['close'].rolling(20).std()\n",
    "    features['bb_upper'] = (ma_20 + 2 * std_20) / df['close'] - 1\n",
    "    features['bb_lower'] = (ma_20 - 2 * std_20) / df['close'] - 1\n",
    "    features['bb_width'] = features['bb_upper'] - features['bb_lower']\n",
    "    \n",
    "    # Price position\n",
    "    features['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_sample_data(n_samples: int = 10000, freq: str = '15min') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gera dados OHLCV sintÃ©ticos para teste\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range('2023-01-01', periods=n_samples, freq=freq)\n",
    "    \n",
    "    # Simular preÃ§o com tendÃªncia e volatilidade\n",
    "    returns = np.random.randn(n_samples) * 0.01  # 1% vol\n",
    "    price = 100 * np.exp(returns.cumsum())\n",
    "    \n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['close'] = price\n",
    "    \n",
    "    # Gerar OHLV a partir do close\n",
    "    df['open'] = df['close'] * (1 + np.random.randn(n_samples) * 0.001)\n",
    "    df['high'] = df[['open', 'close']].max(axis=1) * (1 + np.abs(np.random.randn(n_samples)) * 0.002)\n",
    "    df['low'] = df[['open', 'close']].min(axis=1) * (1 - np.abs(np.random.randn(n_samples)) * 0.002)\n",
    "    df['volume'] = np.random.exponential(1000, n_samples) * (1 + np.abs(returns) * 10)\n",
    "    \n",
    "    # Garantir consistÃªncia OHLC\n",
    "    df['high'] = df[['open', 'high', 'close']].max(axis=1)\n",
    "    df['low'] = df[['open', 'low', 'close']].min(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def demo_multi_horizon_pipeline():\n",
    "    \"\"\"\n",
    "    DemonstraÃ§Ã£o completa do pipeline multi-horizonte\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ DEMONSTRAÃ‡ÃƒO DO PIPELINE MULTI-HORIZONTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Gerar dados sintÃ©ticos\n",
    "    print(\"\\nğŸ“Š Gerando dados sintÃ©ticos...\")\n",
    "    df = generate_sample_data(n_samples=10000)\n",
    "    print(f\"  Dados gerados: {len(df)} barras de 15min\")\n",
    "    print(f\"  PerÃ­odo: {df.index[0]} a {df.index[-1]}\")\n",
    "    \n",
    "    # 2. Criar features\n",
    "    print(\"\\nğŸ”§ Criando features...\")\n",
    "    features = create_sample_features(df)\n",
    "    \n",
    "    # Adicionar features de calendÃ¡rio\n",
    "    crypto_features = Crypto24x7Features()\n",
    "    features = pd.concat([\n",
    "        features,\n",
    "        crypto_features.create_calendar_features(df),\n",
    "        crypto_features.create_session_features(df)\n",
    "    ], axis=1)\n",
    "    \n",
    "    print(f\"  Features criadas: {len(features.columns)}\")\n",
    "    print(f\"  Features: {', '.join(features.columns[:10])}...\")\n",
    "    \n",
    "    # 3. Executar pipeline multi-horizonte\n",
    "    print(\"\\nğŸ¯ Executando pipeline multi-horizonte...\")\n",
    "    results = run_multi_horizon_pipeline(\n",
    "        df=df,\n",
    "        features=features,\n",
    "        horizons=['15m', '30m', '60m', '120m'],\n",
    "        n_trials=10  # Reduzido para demo\n",
    "    )\n",
    "    \n",
    "    # 4. Executar backtest\n",
    "    print(\"\\nğŸ“Š Executando backtest multi-horizonte...\")\n",
    "    backtest_results = run_multi_horizon_backtest(df, results)\n",
    "    \n",
    "    # 5. Criar sinais ensemble\n",
    "    print(\"\\nğŸ¯ Criando estratÃ©gia ensemble...\")\n",
    "    ensemble_signals = create_ensemble_signals(results, voting='soft')\n",
    "    print(f\"  Sinais ensemble criados: {len(ensemble_signals)}\")\n",
    "    \n",
    "    print(\"\\nâœ… DemonstraÃ§Ã£o concluÃ­da!\")\n",
    "    \n",
    "    return results, backtest_results\n",
    "\n",
    "print(\"âœ… FunÃ§Ãµes de demonstraÃ§Ã£o definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9474a",
   "metadata": {},
   "source": [
    "## 11. Como Usar o Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f8a1ed1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                        GUIA DE USO DO PIPELINE                              â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "1. CONFIGURAÃ‡ÃƒO INICIAL:\n",
      "   ```python\n",
      "   # Configurar ambiente determinÃ­stico\n",
      "   setup_deterministic_environment(seed=42)\n",
      "\n",
      "   # Configurar projeto\n",
      "   config = ProjectConfig()\n",
      "   config.create_directories()\n",
      "   ```\n",
      "\n",
      "2. CARREGAR SEUS DADOS:\n",
      "   ```python\n",
      "   # OpÃ§Ã£o 1: Dados locais\n",
      "   df = pd.read_csv('seu_arquivo.csv', index_col='timestamp', parse_dates=True)\n",
      "\n",
      "   # OpÃ§Ã£o 2: API (se disponÃ­vel)\n",
      "   from src.data.binance_loader import CryptoDataLoader\n",
      "   loader = CryptoDataLoader()\n",
      "   df = loader.fetch_ohlcv('BTCUSDT', '15m', limit=10000)\n",
      "   ```\n",
      "\n",
      "3. CRIAR FEATURES:\n",
      "   ```python\n",
      "   # Features bÃ¡sicas\n",
      "   features = create_sample_features(df)\n",
      "\n",
      "   # Adicionar features crypto 24/7\n",
      "   crypto_features = Crypto24x7Features()\n",
      "   # Definir perÃ­odo de funding baseado no contrato (480 min para maioria, 60 min para alguns)\n",
      "   funding_period = 480  # Ajustar conforme sÃ­mbolo/exchange\n",
      "   features = pd.concat([\n",
      "       features,\n",
      "       crypto_features.create_calendar_features(df),\n",
      "       crypto_features.create_session_features(df),\n",
      "       crypto_features.create_funding_features(df, funding_period_minutes=funding_period)\n",
      "   ], axis=1)\n",
      "   ```\n",
      "\n",
      "4. TREINAR MODELOS MULTI-HORIZONTE:\n",
      "   ```python\n",
      "   results = run_multi_horizon_pipeline(\n",
      "       df=df,\n",
      "       features=features,\n",
      "       horizons=['15m', '30m', '60m', '120m'],\n",
      "       test_size=0.2,\n",
      "       val_size=0.2,\n",
      "       n_trials=50  # Aumentar para produÃ§Ã£o\n",
      "   )\n",
      "   ```\n",
      "\n",
      "5. EXECUTAR BACKTEST:\n",
      "   ```python\n",
      "   backtest_results = run_multi_horizon_backtest(\n",
      "       df=df,\n",
      "       results=results,\n",
      "       initial_capital=100000,\n",
      "       fee_bps=5,\n",
      "       slippage_bps=10\n",
      "   )\n",
      "   ```\n",
      "\n",
      "6. CRIAR ESTRATÃ‰GIA ENSEMBLE:\n",
      "   ```python\n",
      "   # Combinar sinais de mÃºltiplos horizontes\n",
      "   ensemble_signals = create_ensemble_signals(\n",
      "       results,\n",
      "       weights={'15m': 0.2, '30m': 0.3, '60m': 0.3, '120m': 0.2},\n",
      "       voting='soft'\n",
      "   )\n",
      "   ```\n",
      "\n",
      "7. DEMO RÃPIDA:\n",
      "   ```python\n",
      "   # Executar demonstraÃ§Ã£o completa com dados sintÃ©ticos\n",
      "   results, backtest_results = demo_multi_horizon_pipeline()\n",
      "   ```\n",
      "\n",
      "NOTAS IMPORTANTES:\n",
      "- Sempre use dados de 15 minutos como base\n",
      "- Horizontes sÃ£o mÃºltiplos de 15min (15m, 30m, 60m, 120m)\n",
      "- PR-AUC Ã© a mÃ©trica principal (nÃ£o ROC-AUC)\n",
      "- CalibraÃ§Ã£o de probabilidades Ã© obrigatÃ³ria\n",
      "- Backtest usa execuÃ§Ã£o t+1 (sinal em t, execuÃ§Ã£o em t+1)\n",
      "- Custos incluem fees e slippage\n",
      "\n",
      "Para mais informaÃ§Ãµes, consulte a documentaÃ§Ã£o em docs/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                        GUIA DE USO DO PIPELINE                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. CONFIGURAÃ‡ÃƒO INICIAL:\n",
    "   ```python\n",
    "   # Configurar ambiente determinÃ­stico\n",
    "   setup_deterministic_environment(seed=42)\n",
    "   \n",
    "   # Configurar projeto\n",
    "   config = ProjectConfig()\n",
    "   config.create_directories()\n",
    "   ```\n",
    "\n",
    "2. CARREGAR SEUS DADOS:\n",
    "   ```python\n",
    "   # OpÃ§Ã£o 1: Dados locais\n",
    "   df = pd.read_csv('seu_arquivo.csv', index_col='timestamp', parse_dates=True)\n",
    "   \n",
    "   # OpÃ§Ã£o 2: API (se disponÃ­vel)\n",
    "   from src.data.binance_loader import CryptoDataLoader\n",
    "   loader = CryptoDataLoader()\n",
    "   df = loader.fetch_ohlcv('BTCUSDT', '15m', limit=10000)\n",
    "   ```\n",
    "\n",
    "3. CRIAR FEATURES:\n",
    "   ```python\n",
    "   # Features bÃ¡sicas\n",
    "   features = create_sample_features(df)\n",
    "   \n",
    "   # Adicionar features crypto 24/7\n",
    "   crypto_features = Crypto24x7Features()\n",
    "   # Definir perÃ­odo de funding baseado no contrato (480 min para maioria, 60 min para alguns)\n",
    "   funding_period = 480  # Ajustar conforme sÃ­mbolo/exchange\n",
    "   features = pd.concat([\n",
    "       features,\n",
    "       crypto_features.create_calendar_features(df),\n",
    "       crypto_features.create_session_features(df),\n",
    "       crypto_features.create_funding_features(df, funding_period_minutes=funding_period)\n",
    "   ], axis=1)\n",
    "   ```\n",
    "\n",
    "4. TREINAR MODELOS MULTI-HORIZONTE:\n",
    "   ```python\n",
    "   results = run_multi_horizon_pipeline(\n",
    "       df=df,\n",
    "       features=features,\n",
    "       horizons=['15m', '30m', '60m', '120m'],\n",
    "       test_size=0.2,\n",
    "       val_size=0.2,\n",
    "       n_trials=50  # Aumentar para produÃ§Ã£o\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. EXECUTAR BACKTEST:\n",
    "   ```python\n",
    "   backtest_results = run_multi_horizon_backtest(\n",
    "       df=df,\n",
    "       results=results,\n",
    "       initial_capital=100000,\n",
    "       fee_bps=5,\n",
    "       slippage_bps=10\n",
    "   )\n",
    "   ```\n",
    "\n",
    "6. CRIAR ESTRATÃ‰GIA ENSEMBLE:\n",
    "   ```python\n",
    "   # Combinar sinais de mÃºltiplos horizontes\n",
    "   ensemble_signals = create_ensemble_signals(\n",
    "       results,\n",
    "       weights={'15m': 0.2, '30m': 0.3, '60m': 0.3, '120m': 0.2},\n",
    "       voting='soft'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "7. DEMO RÃPIDA:\n",
    "   ```python\n",
    "   # Executar demonstraÃ§Ã£o completa com dados sintÃ©ticos\n",
    "   results, backtest_results = demo_multi_horizon_pipeline()\n",
    "   ```\n",
    "\n",
    "NOTAS IMPORTANTES:\n",
    "- Sempre use dados de 15 minutos como base\n",
    "- Horizontes sÃ£o mÃºltiplos de 15min (15m, 30m, 60m, 120m)\n",
    "- PR-AUC Ã© a mÃ©trica principal (nÃ£o ROC-AUC)\n",
    "- CalibraÃ§Ã£o de probabilidades Ã© obrigatÃ³ria\n",
    "- Backtest usa execuÃ§Ã£o t+1 (sinal em t, execuÃ§Ã£o em t+1)\n",
    "- Custos incluem fees e slippage\n",
    "\n",
    "Para mais informaÃ§Ãµes, consulte a documentaÃ§Ã£o em docs/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709541f",
   "metadata": {},
   "source": [
    "## 12. Executar DemonstraÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d74b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ DEMONSTRAÃ‡ÃƒO DO PIPELINE MULTI-HORIZONTE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Gerando dados sintÃ©ticos...\n",
      "  Dados gerados: 10000 barras de 15min\n",
      "  PerÃ­odo: 2023-01-01 00:00:00 a 2023-04-15 03:45:00\n",
      "\n",
      "ğŸ”§ Criando features...\n",
      "  Features criadas: 41\n",
      "  Features: return_1, return_5, return_10, return_20, return_50, ma_10, ma_20, ma_50, ma_100, volume_ratio...\n",
      "\n",
      "ğŸ¯ Executando pipeline multi-horizonte...\n",
      "================================================================================\n",
      "ğŸš€ INICIANDO PIPELINE MULTI-HORIZONTE\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Optuna nÃ£o estÃ¡ disponÃ­vel",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Para executar a demonstraÃ§Ã£o, descomente a linha abaixo:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results, backtest_results = \u001b[43mdemo_multi_horizon_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mdemo_multi_horizon_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# 3. Executar pipeline multi-horizonte\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ¯ Executando pipeline multi-horizonte...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m results = \u001b[43mrun_multi_horizon_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m15m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m30m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m60m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m120m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduzido para demo\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# 4. Executar backtest\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“Š Executando backtest multi-horizonte...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_multi_horizon_pipeline\u001b[39m\u001b[34m(df, features, horizons, test_size, val_size, n_trials, k_range)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mXGBoost nÃ£o estÃ¡ disponÃ­vel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OPTUNA_AVAILABLE:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOptuna nÃ£o estÃ¡ disponÃ­vel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MLFLOW_AVAILABLE:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâš ï¸ MLflow nÃ£o disponÃ­vel - resultados nÃ£o serÃ£o tracked\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Optuna nÃ£o estÃ¡ disponÃ­vel"
     ]
    }
   ],
   "source": [
    "# Para executar a demonstraÃ§Ã£o, descomente a linha abaixo:\n",
    "# results, backtest_results = demo_multi_horizon_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0f653",
   "metadata": {},
   "source": [
    "## 13. SuÃ­te Completa de Testes - ValidaÃ§Ã£o de Requisitos PRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTestSuite:\n",
    "    \"\"\"\n",
    "    SuÃ­te completa de testes para validar todos os requisitos dos PRDs\n",
    "    Inclui testes de integridade, performance e requisitos econÃ´micos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProjectConfig = None):\n",
    "        self.config = config or ProjectConfig()\n",
    "        self.test_results = {}\n",
    "        \n",
    "    def test_temporal_leakage(self, df: pd.DataFrame, features: pd.DataFrame, \n",
    "                              labels: pd.Series) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de vazamento temporal - CRÃTICO\n",
    "        Requisito PRD: Sem vazamento temporal comprovado\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando vazamento temporal...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        # Verificar se features usam informaÃ§Ã£o futura\n",
    "        for col in features.columns:\n",
    "            if 'future' in col.lower() or 'next' in col.lower():\n",
    "                passed = False\n",
    "                metrics[f'feature_{col}'] = 'SUSPEITA: nome sugere informaÃ§Ã£o futura'\n",
    "        \n",
    "        # Verificar correlaÃ§Ã£o com retornos futuros nÃ£o shiftados\n",
    "        future_returns = df['close'].pct_change().shift(-1)  # Retorno futuro\n",
    "        \n",
    "        for col in features.columns:\n",
    "            if features[col].notna().sum() > 100:  # SÃ³ testar se tiver dados suficientes\n",
    "                corr = features[col].corr(future_returns)\n",
    "                if abs(corr) > 0.95:  # CorrelaÃ§Ã£o muito alta Ã© suspeita\n",
    "                    passed = False\n",
    "                    metrics[f'correlation_{col}'] = f'ALTA: {corr:.3f}'\n",
    "        \n",
    "        # Verificar alinhamento temporal de labels\n",
    "        label_returns = labels.shift(-1)  # Labels devem estar no futuro\n",
    "        label_feature_corr = labels.corr(features.mean(axis=1))\n",
    "        \n",
    "        if abs(label_feature_corr) > 0.8:\n",
    "            passed = False\n",
    "            metrics['label_alignment'] = f'PROBLEMA: correlaÃ§Ã£o {label_feature_corr:.3f}'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_data_quality(self, df: pd.DataFrame, features: pd.DataFrame) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de qualidade de dados\n",
    "        Requisito PRD: Dados limpos e consistentes\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando qualidade de dados...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        # Verificar NaN\n",
    "        nan_ratio = features.isna().sum().sum() / (len(features) * len(features.columns))\n",
    "        metrics['nan_ratio'] = nan_ratio\n",
    "        if nan_ratio > 0.1:  # Mais de 10% NaN Ã© problemÃ¡tico\n",
    "            passed = False\n",
    "        \n",
    "        # Verificar consistÃªncia OHLC\n",
    "        ohlc_errors = 0\n",
    "        ohlc_errors += (df['high'] < df['low']).sum()\n",
    "        ohlc_errors += (df['high'] < df['open']).sum()\n",
    "        ohlc_errors += (df['high'] < df['close']).sum()\n",
    "        ohlc_errors += (df['low'] > df['open']).sum()\n",
    "        ohlc_errors += (df['low'] > df['close']).sum()\n",
    "        \n",
    "        metrics['ohlc_errors'] = ohlc_errors\n",
    "        if ohlc_errors > 0:\n",
    "            passed = False\n",
    "        \n",
    "        # Verificar outliers extremos (> 10 desvios padrÃ£o)\n",
    "        outliers = 0\n",
    "        for col in features.select_dtypes(include=[np.number]).columns:\n",
    "            z_scores = np.abs(stats.zscore(features[col].dropna()))\n",
    "            outliers += (z_scores > 10).sum()\n",
    "        \n",
    "        metrics['extreme_outliers'] = outliers\n",
    "        if outliers > len(features) * 0.01:  # Mais de 1% outliers extremos\n",
    "            passed = False\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_model_performance(self, results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de performance do modelo\n",
    "        Requisito PRD: PR-AUC acima do baseline, F1 adequado\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando performance dos modelos...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        baseline_pr_auc = 0.5  # Baseline aleatÃ³rio\n",
    "        min_acceptable_f1 = 0.3  # MÃ­nimo aceitÃ¡vel para crypto\n",
    "        \n",
    "        for horizon, result in results.items():\n",
    "            horizon_metrics = result['metrics']\n",
    "            \n",
    "            # PR-AUC deve ser melhor que baseline\n",
    "            pr_auc = horizon_metrics['pr_auc']\n",
    "            metrics[f'{horizon}_pr_auc'] = pr_auc\n",
    "            if pr_auc <= baseline_pr_auc:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_pr_auc_status'] = 'FAIL: abaixo do baseline'\n",
    "            \n",
    "            # F1 score mÃ­nimo\n",
    "            f1 = horizon_metrics['f1']\n",
    "            metrics[f'{horizon}_f1'] = f1\n",
    "            if f1 < min_acceptable_f1:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_f1_status'] = 'FAIL: F1 muito baixo'\n",
    "            \n",
    "            # MCC (Matthews Correlation Coefficient)\n",
    "            mcc = horizon_metrics['mcc']\n",
    "            metrics[f'{horizon}_mcc'] = mcc\n",
    "            if mcc < 0:  # MCC negativo indica pior que aleatÃ³rio\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_mcc_status'] = 'FAIL: MCC negativo'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_calibration(self, results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de calibraÃ§Ã£o de probabilidades\n",
    "        Requisito PRD: CalibraÃ§Ã£o dentro de Â±2 p.p.\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando calibraÃ§Ã£o de probabilidades...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        for horizon, result in results.items():\n",
    "            predictions = result['predictions']['calibrated']\n",
    "            labels = result['labels']\n",
    "            \n",
    "            # Calcular ECE (Expected Calibration Error)\n",
    "            n_bins = 10\n",
    "            bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "            bin_lowers = bin_boundaries[:-1]\n",
    "            bin_uppers = bin_boundaries[1:]\n",
    "            \n",
    "            ece = 0\n",
    "            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "                in_bin = (predictions > bin_lower) & (predictions <= bin_upper)\n",
    "                prop_in_bin = in_bin.mean()\n",
    "                \n",
    "                if prop_in_bin > 0:\n",
    "                    accuracy_in_bin = labels[in_bin].mean()\n",
    "                    avg_confidence_in_bin = predictions[in_bin].mean()\n",
    "                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "            \n",
    "            metrics[f'{horizon}_ece'] = ece\n",
    "            \n",
    "            # ECE deve ser < 0.02 (2%)\n",
    "            if ece > 0.02:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_ece_status'] = f'FAIL: ECE {ece:.3f} > 0.02'\n",
    "            \n",
    "            # Brier Score (menor Ã© melhor)\n",
    "            brier = brier_score_loss(labels, predictions)\n",
    "            metrics[f'{horizon}_brier'] = brier\n",
    "            \n",
    "            if brier > 0.25:  # Brier > 0.25 indica mÃ¡ calibraÃ§Ã£o\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_brier_status'] = f'FAIL: Brier {brier:.3f} > 0.25'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_economic_metrics(self, backtest_results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de mÃ©tricas econÃ´micas\n",
    "        Requisito PRD: Sharpe > 1.0, DSR > 0.8\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando mÃ©tricas econÃ´micas...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        min_sharpe = 1.0  # Requisito PRD\n",
    "        min_dsr = 0.8     # Requisito PRD\n",
    "        max_acceptable_drawdown = 0.25  # Max 25% drawdown\n",
    "        \n",
    "        for horizon, result in backtest_results.items():\n",
    "            horizon_metrics = result['metrics']\n",
    "            \n",
    "            # Sharpe Ratio\n",
    "            sharpe = horizon_metrics['sharpe_ratio']\n",
    "            metrics[f'{horizon}_sharpe'] = sharpe\n",
    "            if sharpe < min_sharpe:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_sharpe_status'] = f'FAIL: Sharpe {sharpe:.2f} < {min_sharpe}'\n",
    "            \n",
    "            # Maximum Drawdown\n",
    "            mdd = abs(horizon_metrics['max_drawdown'])\n",
    "            metrics[f'{horizon}_max_drawdown'] = mdd\n",
    "            if mdd > max_acceptable_drawdown:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_mdd_status'] = f'FAIL: MDD {mdd:.2%} > {max_acceptable_drawdown:.0%}'\n",
    "            \n",
    "            # Calmar Ratio\n",
    "            calmar = horizon_metrics['calmar_ratio']\n",
    "            metrics[f'{horizon}_calmar'] = calmar\n",
    "            \n",
    "            # Win Rate\n",
    "            win_rate = horizon_metrics['win_rate']\n",
    "            metrics[f'{horizon}_win_rate'] = win_rate\n",
    "            if win_rate < 0.45:  # Win rate muito baixo\n",
    "                metrics[f'{horizon}_win_rate_status'] = f'WARNING: Win rate {win_rate:.1%} baixo'\n",
    "            \n",
    "            # Calcular DSR (Deflated Sharpe Ratio) simplificado\n",
    "            # DSR = Sharpe * sqrt(T) / sqrt(1 + skew^2/4 + (kurt-3)^2/24)\n",
    "            returns = result.get('returns', pd.Series([0]))\n",
    "            if len(returns) > 30:\n",
    "                skew = returns.skew()\n",
    "                kurt = returns.kurt()\n",
    "                T = len(returns) / (365 * 24 * 4)  # Anos de dados\n",
    "                dsr = sharpe * np.sqrt(T) / np.sqrt(1 + skew**2/4 + (kurt-3)**2/24)\n",
    "                metrics[f'{horizon}_dsr'] = dsr\n",
    "                \n",
    "                if dsr < min_dsr:\n",
    "                    passed = False\n",
    "                    metrics[f'{horizon}_dsr_status'] = f'FAIL: DSR {dsr:.2f} < {min_dsr}'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_feature_importance_stability(self, results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de estabilidade das feature importances\n",
    "        Requisito PRD: Feature importances consistentes\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando estabilidade de feature importance...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        # Coletar top features de cada horizonte\n",
    "        top_features_by_horizon = {}\n",
    "        for horizon, result in results.items():\n",
    "            top_10 = result['feature_importance'].head(10)['feature'].tolist()\n",
    "            top_features_by_horizon[horizon] = set(top_10)\n",
    "        \n",
    "        # Verificar overlap entre horizontes\n",
    "        horizons = list(results.keys())\n",
    "        for i in range(len(horizons)-1):\n",
    "            h1, h2 = horizons[i], horizons[i+1]\n",
    "            overlap = len(top_features_by_horizon[h1] & top_features_by_horizon[h2])\n",
    "            overlap_ratio = overlap / 10\n",
    "            \n",
    "            metrics[f'overlap_{h1}_{h2}'] = overlap_ratio\n",
    "            \n",
    "            if overlap_ratio < 0.3:  # Menos de 30% de overlap Ã© suspeito\n",
    "                passed = False\n",
    "                metrics[f'overlap_{h1}_{h2}_status'] = f'FAIL: apenas {overlap_ratio:.1%} overlap'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_execution_realism(self, backtest_results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de realismo da execuÃ§Ã£o\n",
    "        Requisito: ExecuÃ§Ã£o t+1, custos aplicados\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” Testando realismo da execuÃ§Ã£o...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        for horizon, result in backtest_results.items():\n",
    "            # Verificar turnover\n",
    "            turnover = result['metrics'].get('turnover', 0)\n",
    "            metrics[f'{horizon}_turnover'] = turnover\n",
    "            \n",
    "            if turnover > 10:  # Turnover muito alto Ã© irrealista\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_turnover_status'] = f'FAIL: turnover {turnover:.1f} muito alto'\n",
    "            \n",
    "            # Verificar se custos foram aplicados\n",
    "            if 'outperformance' in result['metrics']:\n",
    "                outperf = result['metrics']['outperformance']\n",
    "                if outperf > 0.5:  # Outperformance > 50% Ã© suspeito\n",
    "                    metrics[f'{horizon}_outperf_warning'] = f'WARNING: outperformance {outperf:.1%} muito alto'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'âœ… PASS' if passed else 'âŒ FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def run_all_tests(self, df: pd.DataFrame, features: pd.DataFrame, \n",
    "                      results: Dict, backtest_results: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Executa todos os testes e gera relatÃ³rio completo\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ§ª EXECUTANDO SUÃTE COMPLETA DE TESTES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Preparar labels para teste (usar do primeiro horizonte)\n",
    "        first_horizon = list(results.keys())[0]\n",
    "        labeler = results[first_horizon]['labeler']\n",
    "        labels = labeler.create_labels(df)\n",
    "        \n",
    "        # 1. Teste de vazamento temporal\n",
    "        passed, metrics = self.test_temporal_leakage(df, features, labels)\n",
    "        all_results['temporal_leakage'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 2. Teste de qualidade de dados\n",
    "        passed, metrics = self.test_data_quality(df, features)\n",
    "        all_results['data_quality'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 3. Teste de performance do modelo\n",
    "        passed, metrics = self.test_model_performance(results)\n",
    "        all_results['model_performance'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 4. Teste de calibraÃ§Ã£o\n",
    "        passed, metrics = self.test_calibration(results)\n",
    "        all_results['calibration'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 5. Teste de mÃ©tricas econÃ´micas\n",
    "        passed, metrics = self.test_economic_metrics(backtest_results)\n",
    "        all_results['economic_metrics'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 6. Teste de estabilidade de features\n",
    "        passed, metrics = self.test_feature_importance_stability(results)\n",
    "        all_results['feature_stability'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 7. Teste de realismo de execuÃ§Ã£o\n",
    "        passed, metrics = self.test_execution_realism(backtest_results)\n",
    "        all_results['execution_realism'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # Resumo final\n",
    "        self.print_test_summary(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def print_test_summary(self, results: Dict):\n",
    "        \"\"\"\n",
    "        Imprime resumo dos testes\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“Š RESUMO DOS TESTES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        total_tests = len(results)\n",
    "        passed_tests = sum(1 for r in results.values() if r['passed'])\n",
    "        \n",
    "        print(f\"\\nTotal de testes: {total_tests}\")\n",
    "        print(f\"Testes aprovados: {passed_tests}\")\n",
    "        print(f\"Taxa de aprovaÃ§Ã£o: {passed_tests/total_tests:.1%}\")\n",
    "        \n",
    "        print(\"\\nğŸ“‹ Detalhes por teste:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for test_name, result in results.items():\n",
    "            status = \"âœ… PASS\" if result['passed'] else \"âŒ FAIL\"\n",
    "            print(f\"{test_name:25s}: {status}\")\n",
    "            \n",
    "            # Mostrar mÃ©tricas crÃ­ticas se falhou\n",
    "            if not result['passed']:\n",
    "                for key, value in result['metrics'].items():\n",
    "                    if 'FAIL' in str(value) or 'status' in key:\n",
    "                        print(f\"  â””â”€ {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # VerificaÃ§Ã£o dos requisitos PRD\n",
    "        print(\"\\nğŸ¯ REQUISITOS PRD:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Requisitos crÃ­ticos\n",
    "        requirements = {\n",
    "            'Sem vazamento temporal': results['temporal_leakage']['passed'],\n",
    "            'PR-AUC acima do baseline': 'model_performance' in results and results['model_performance']['passed'],\n",
    "            'Sharpe > 1.0': False,  # Verificar nas mÃ©tricas\n",
    "            'DSR > 0.8': False,  # Verificar nas mÃ©tricas\n",
    "            'CalibraÃ§Ã£o < 2%': results.get('calibration', {}).get('passed', False),\n",
    "            'Features estÃ¡veis': results.get('feature_stability', {}).get('passed', False)\n",
    "        }\n",
    "        \n",
    "        # Verificar Sharpe e DSR\n",
    "        if 'economic_metrics' in results:\n",
    "            metrics = results['economic_metrics']['metrics']\n",
    "            # Verificar se algum horizonte passou no Sharpe\n",
    "            sharpe_passed = any(\n",
    "                metrics.get(f'{h}_sharpe', 0) >= 1.0 \n",
    "                for h in ['15m', '30m', '60m', '120m']\n",
    "            )\n",
    "            requirements['Sharpe > 1.0'] = sharpe_passed\n",
    "            \n",
    "            # Verificar DSR\n",
    "            dsr_passed = any(\n",
    "                metrics.get(f'{h}_dsr', 0) >= 0.8\n",
    "                for h in ['15m', '30m', '60m', '120m']\n",
    "                if f'{h}_dsr' in metrics\n",
    "            )\n",
    "            requirements['DSR > 0.8'] = dsr_passed\n",
    "        \n",
    "        for req, passed in requirements.items():\n",
    "            status = \"âœ…\" if passed else \"âŒ\"\n",
    "            print(f\"{status} {req}\")\n",
    "        \n",
    "        # ConclusÃ£o\n",
    "        all_requirements_met = all(requirements.values())\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        if all_requirements_met:\n",
    "            print(\"ğŸ‰ TODOS OS REQUISITOS PRD FORAM ATENDIDOS!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ALGUNS REQUISITOS PRD NÃƒO FORAM ATENDIDOS\")\n",
    "            print(\"   Revise os testes falhados e ajuste o modelo\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "# FunÃ§Ã£o para executar os testes\n",
    "def run_model_tests(df: pd.DataFrame = None, features: pd.DataFrame = None,\n",
    "                    results: Dict = None, backtest_results: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Executa a suÃ­te completa de testes\n",
    "    \n",
    "    Se nÃ£o fornecer dados, executa com dados de demo\n",
    "    \"\"\"\n",
    "    if df is None or features is None or results is None:\n",
    "        print(\"Gerando dados de demonstraÃ§Ã£o para testes...\")\n",
    "        df = generate_sample_data(10000)\n",
    "        features = create_sample_features(df)\n",
    "        \n",
    "        # Adicionar features crypto\n",
    "        crypto_features = Crypto24x7Features()\n",
    "        features = pd.concat([\n",
    "            features,\n",
    "            crypto_features.create_calendar_features(df),\n",
    "            crypto_features.create_session_features(df)\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Executar pipeline\n",
    "        print(\"Executando pipeline para gerar resultados...\")\n",
    "        results = run_multi_horizon_pipeline(\n",
    "            df, features, \n",
    "            horizons=['15m', '30m'],  # Menos horizontes para teste rÃ¡pido\n",
    "            n_trials=5  # Poucos trials para teste\n",
    "        )\n",
    "        \n",
    "        # Executar backtest\n",
    "        print(\"Executando backtest...\")\n",
    "        backtest_results = run_multi_horizon_backtest(df, results)\n",
    "    \n",
    "    # Executar testes\n",
    "    test_suite = ModelTestSuite()\n",
    "    test_results = test_suite.run_all_tests(df, features, results, backtest_results)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "\n",
    "# Para executar os testes:\n",
    "# test_results = run_model_tests()\n",
    "\n",
    "print(\"âœ… SuÃ­te de testes definida\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}