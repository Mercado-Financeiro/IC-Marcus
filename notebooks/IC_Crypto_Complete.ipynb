{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f2f3bb",
   "metadata": {},
   "source": [
    "# IC Crypto Complete - Pipeline ML para Trading de Criptomoedas\n",
    "\n",
    "Pipeline completo de Machine Learning para trading de criptomoedas com:\n",
    "- Labeling adaptativo baseado em volatilidade\n",
    "- M√∫ltiplos horizontes de predi√ß√£o (15m, 30m, 60m, 120m)\n",
    "- Features espec√≠ficas para mercado 24/7\n",
    "- Backtest realista com custos e execu√ß√£o t+1\n",
    "- Otimiza√ß√£o Bayesiana com Optuna\n",
    "- Calibra√ß√£o de probabilidades\n",
    "- MLflow tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528cac0",
   "metadata": {},
   "source": [
    "## 1. Setup Completo - Imports e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33823ff1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Optuna n√£o dispon√≠vel\n",
      "‚ö†Ô∏è TA-Lib n√£o dispon√≠vel\n",
      "‚ö†Ô∏è Imports locais n√£o dispon√≠veis - usando implementa√ß√µes do notebook\n",
      "‚úÖ Imports conclu√≠dos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# ========================== IMPORTS ORGANIZADOS ==========================\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import hashlib\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "# Configurar warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report, roc_curve,\n",
    "    precision_recall_curve, brier_score_loss, balanced_accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost n√£o dispon√≠vel\")\n",
    "\n",
    "# Optuna\n",
    "OPTUNA_AVAILABLE = False\n",
    "OPTUNA_XGBOOST_INTEGRATION = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.pruners import MedianPruner, SuccessiveHalvingPruner, HyperbandPruner\n",
    "    from optuna.samplers import TPESampler\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"‚úÖ Optuna core dispon√≠vel\")\n",
    "    \n",
    "    # Tentar carregar integra√ß√£o XGBoost separadamente\n",
    "    try:\n",
    "        from optuna.integration import XGBoostPruningCallback\n",
    "        OPTUNA_XGBOOST_INTEGRATION = True\n",
    "        print(\"‚úÖ Optuna XGBoost integration dispon√≠vel\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Optuna XGBoost integration n√£o dispon√≠vel: {e}\")\n",
    "        print(\"   Pipeline continuar√° com otimiza√ß√£o b√°sica\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Optuna n√£o dispon√≠vel: {e}\")\n",
    "    print(\"   Pipeline continuar√° com hiperpar√¢metros padr√£o\")\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    TORCH_AVAILABLE = True\n",
    "    \n",
    "    # Configurar determinismo do PyTorch\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è PyTorch n√£o dispon√≠vel\")\n",
    "\n",
    "# MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.xgboost\n",
    "    import mlflow.pytorch\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è MLflow n√£o dispon√≠vel\")\n",
    "\n",
    "# SHAP\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è SHAP n√£o dispon√≠vel\")\n",
    "\n",
    "# Data APIs\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è yfinance n√£o dispon√≠vel\")\n",
    "\n",
    "try:\n",
    "    import ccxt\n",
    "    CCXT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CCXT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è CCXT n√£o dispon√≠vel\")\n",
    "\n",
    "# Technical Analysis\n",
    "try:\n",
    "    import ta\n",
    "    TA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è TA-Lib n√£o dispon√≠vel\")\n",
    "\n",
    "# Data Validation\n",
    "try:\n",
    "    import pandera as pa\n",
    "    from pandera import Column, DataFrameSchema, Check\n",
    "    PANDERA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PANDERA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Pandera n√£o dispon√≠vel\")\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly n√£o dispon√≠vel\")\n",
    "\n",
    "# Imports locais do projeto (quando dispon√≠veis)\n",
    "try:\n",
    "    from src.data.binance_loader import CryptoDataLoader\n",
    "    from src.data.splits import PurgedKFold as ImportedPurgedKFold\n",
    "    from src.features.engineering import FeatureEngineer as BaseFeatureEngineer\n",
    "    from src.models.xgb_optuna import XGBoostOptuna as ImportedXGBoostOptuna\n",
    "    from src.backtest.engine import BacktestEngine as ImportedBacktestEngine, BacktestConfig\n",
    "    LOCAL_IMPORTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LOCAL_IMPORTS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Imports locais n√£o dispon√≠veis - usando implementa√ß√µes do notebook\")\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "print(\"‚úÖ Imports conclu√≠dos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4826e",
   "metadata": {},
   "source": [
    "## 2. Fallbacks para Bibliotecas Opcionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791abc0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FallbackPruningCallback:\n",
    "    \"\"\"Fallback para XGBoostPruningCallback quando optuna-integration n√£o dispon√≠vel\"\"\"\n",
    "    def __init__(self, trial, metric_name):\n",
    "        self.trial = trial\n",
    "        self.metric_name = metric_name\n",
    "        \n",
    "    def __call__(self, env):\n",
    "        \"\"\"Callback compat√≠vel com XGBoost\"\"\"\n",
    "        # Extrair m√©trica de valida√ß√£o\n",
    "        if hasattr(env, 'evaluation_result_list') and env.evaluation_result_list:\n",
    "            # Formato: [('validation_0', 'aucpr', value)]\n",
    "            for eval_name, metric, value in env.evaluation_result_list:\n",
    "                if metric in self.metric_name or self.metric_name in metric:\n",
    "                    self.trial.report(value, env.iteration)\n",
    "                    if self.trial.should_prune():\n",
    "                        raise optuna.TrialPruned()\n",
    "        return False\n",
    "\n",
    "def create_xgb_pruning_callback(trial, metric_name):\n",
    "    \"\"\"Factory function para criar callback apropriado\"\"\"\n",
    "    if OPTUNA_XGBOOST_INTEGRATION:\n",
    "        from optuna.integration import XGBoostPruningCallback\n",
    "        return XGBoostPruningCallback(trial, metric_name)\n",
    "    elif OPTUNA_AVAILABLE:\n",
    "        return FallbackPruningCallback(trial, metric_name)\n",
    "    else:\n",
    "        # Retorna callback dummy que n√£o faz nada\n",
    "        return lambda env: False\n",
    "\n",
    "def safe_optuna_study(direction='maximize', sampler=None, pruner=None):\n",
    "    \"\"\"Criar study do Optuna com fallbacks\"\"\"\n",
    "    if OPTUNA_AVAILABLE:\n",
    "        return optuna.create_study(\n",
    "            direction=direction,\n",
    "            sampler=sampler or optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=pruner or optuna.pruners.MedianPruner()\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: usar grid search simples\n",
    "        print(\"‚ö†Ô∏è Usando grid search b√°sico no lugar do Optuna\")\n",
    "        return None\n",
    "\n",
    "def safe_mlflow_log(func_name, *args, **kwargs):\n",
    "    \"\"\"Helper para fazer log no MLflow de forma segura\"\"\"\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        func = getattr(mlflow, func_name)\n",
    "        return func(*args, **kwargs)\n",
    "    return None\n",
    "\n",
    "def safe_mlflow_start_run(**kwargs):\n",
    "    \"\"\"Helper para iniciar run do MLflow de forma segura\"\"\"\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        return mlflow.start_run(**kwargs)\n",
    "    return None\n",
    "\n",
    "def safe_mlflow_end_run():\n",
    "    \"\"\"Helper para finalizar run do MLflow de forma segura\"\"\"\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        return mlflow.end_run()\n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Fallbacks configurados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480505f",
   "metadata": {},
   "source": [
    "## 3. Configura√ß√£o Determin√≠stica do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f8f09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ambiente configurado para determinismo com seed=42\n",
      "   PYTHONHASHSEED=42\n"
     ]
    }
   ],
   "source": [
    "def setup_deterministic_environment(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Configura ambiente para reprodutibilidade total\n",
    "    \"\"\"\n",
    "    # Python built-in\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Scikit-learn (se aplic√°vel)\n",
    "    os.environ['SKLEARN_SEED'] = str(seed)\n",
    "    \n",
    "    # PyTorch (se dispon√≠vel)\n",
    "    if TORCH_AVAILABLE:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            # Para opera√ß√µes determin√≠sticas em GPU\n",
    "            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    \n",
    "    # TensorFlow (se dispon√≠vel)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    print(f\"‚úÖ Ambiente configurado para determinismo com seed={seed}\")\n",
    "    print(f\"   PYTHONHASHSEED={os.environ.get('PYTHONHASHSEED', 'not set')}\")\n",
    "    if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "        print(f\"   CUDA determin√≠stico: {torch.backends.cudnn.deterministic}\")\n",
    "        print(f\"   CUBLAS_WORKSPACE_CONFIG={os.environ.get('CUBLAS_WORKSPACE_CONFIG', 'not set')}\")\n",
    "    \n",
    "    return seed\n",
    "\n",
    "# Aplicar configura√ß√£o determin√≠stica\n",
    "GLOBAL_SEED = setup_deterministic_environment(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82713",
   "metadata": {},
   "source": [
    "## 3. Configura√ß√µes Globais do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666b0857",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Diret√≥rios criados\n",
      "‚úÖ MLflow configurado: artifacts/mlruns\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class ProjectConfig:\n",
    "    \"\"\"Configura√ß√µes globais do projeto\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    data_path: str = \"data\"\n",
    "    artifacts_path: str = \"artifacts\"\n",
    "    models_path: str = \"artifacts/models\"\n",
    "    reports_path: str = \"artifacts/reports\"\n",
    "    mlflow_tracking_uri: str = \"artifacts/mlruns\"\n",
    "    \n",
    "    # Data\n",
    "    symbol: str = \"BTCUSDT\"\n",
    "    timeframe: str = \"15m\"\n",
    "    start_date: str = \"2023-01-01\"\n",
    "    end_date: str = \"2024-01-01\"\n",
    "    \n",
    "    # Funding periods por s√≠mbolo (em minutos)\n",
    "    # Fonte: https://www.binance.com/en/support/faq/detail/360033525031\n",
    "    # Padr√£o: 480 min (8h) para maioria dos contratos perp√©tuos\n",
    "    # Alguns contratos espec√≠ficos usam 60 min (1h) ou 240 min (4h)\n",
    "    FUNDING_MIN_BY_SYMBOL: Dict[str, int] = field(default_factory=lambda: {\n",
    "        \"BTCUSDT\": 480,    # 8 horas\n",
    "        \"ETHUSDT\": 480,    # 8 horas  \n",
    "        \"BNBUSDT\": 480,    # 8 horas\n",
    "        \"SOLUSDT\": 480,    # 8 horas\n",
    "        \"XRPUSDT\": 480,    # 8 horas\n",
    "        \"ADAUSDT\": 480,    # 8 horas\n",
    "        # Adicionar outros s√≠mbolos conforme necess√°rio\n",
    "        # Alguns contratos ex√≥ticos podem ter 60 ou 240 minutos\n",
    "    })\n",
    "    \n",
    "    # Horizontes de predi√ß√£o (em barras de 15min)\n",
    "    horizons: Dict[str, int] = None\n",
    "    \n",
    "    # Features\n",
    "    feature_windows: List[int] = None\n",
    "    volatility_estimators: List[str] = None\n",
    "    \n",
    "    # Model\n",
    "    test_size: float = 0.2\n",
    "    val_size: float = 0.2\n",
    "    cv_splits: int = 5\n",
    "    embargo_bars: int = 10\n",
    "    \n",
    "    # Trading\n",
    "    initial_capital: float = 100000\n",
    "    fee_bps: float = 5  # basis points\n",
    "    slippage_bps: float = 10\n",
    "    max_leverage: float = 1.0\n",
    "    funding_period_minutes: int = 480  # per√≠odo de funding em minutos (8 horas por padr√£o)\n",
    "    \n",
    "    # Optimization\n",
    "    n_trials_optuna: int = 50\n",
    "    optuna_timeout: int = 3600  # seconds\n",
    "    \n",
    "    # MLflow\n",
    "    experiment_name: str = \"crypto_ml_trading\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Inicializar valores padr√£o para campos mut√°veis\"\"\"\n",
    "        if self.horizons is None:\n",
    "            # Calcular horizonte de funding dinamicamente\n",
    "            funding_horizon_bars = self.funding_period_minutes // 15  # converter para barras de 15min\n",
    "            self.horizons = {\n",
    "                '15m': 1,   # 15 minutos\n",
    "                '30m': 2,   # 30 minutos  \n",
    "                '60m': 4,   # 1 hora\n",
    "                '120m': 8,  # 2 horas\n",
    "                '240m': 16, # 4 horas\n",
    "                f'{self.funding_period_minutes}m': funding_horizon_bars  # funding cycle din√¢mico\n",
    "            }\n",
    "        \n",
    "        if self.feature_windows is None:\n",
    "            self.feature_windows = [5, 10, 20, 50, 100, 200]\n",
    "        \n",
    "        if self.volatility_estimators is None:\n",
    "            self.volatility_estimators = ['atr', 'garman_klass', 'yang_zhang', 'parkinson']\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Criar estrutura de diret√≥rios\"\"\"\n",
    "        for path in [self.data_path, self.artifacts_path, self.models_path, self.reports_path]:\n",
    "            Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        print(\"‚úÖ Diret√≥rios criados\")\n",
    "\n",
    "# Instanciar configura√ß√£o global\n",
    "config = ProjectConfig()\n",
    "config.create_directories()\n",
    "\n",
    "# Configurar MLflow\n",
    "if MLFLOW_AVAILABLE:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(config.mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(config.experiment_name)\n",
    "    print(f\"‚úÖ MLflow configurado: {config.mlflow_tracking_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ff321",
   "metadata": {},
   "source": [
    "## 4. Classes de Estimadores de Volatilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daebc14c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe VolatilityEstimators definida\n"
     ]
    }
   ],
   "source": [
    "class VolatilityEstimators:\n",
    "    \"\"\"\n",
    "    Implementa√ß√£o de diversos estimadores de volatilidade para mercados 24/7\n",
    "    Refer√™ncia: Sinclair (2008) - Volatility Trading\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def atr(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"Average True Range - robusto para gaps\"\"\"\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close']\n",
    "        \n",
    "        tr1 = high - low\n",
    "        tr2 = abs(high - close.shift())\n",
    "        tr3 = abs(low - close.shift())\n",
    "        \n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=window).mean()\n",
    "        \n",
    "        # Normalizar como propor√ß√£o do pre√ßo (retorno impl√≠cito)\n",
    "        return atr / close\n",
    "    \n",
    "    @staticmethod\n",
    "    def garman_klass(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Garman-Klass estimator (1980)\n",
    "        Usa OHLC, ~8x mais eficiente que close-to-close\n",
    "        \"\"\"\n",
    "        log_hl = np.log(df['high'] / df['low'])\n",
    "        log_co = np.log(df['close'] / df['open'])\n",
    "        \n",
    "        gk = np.sqrt(\n",
    "            0.5 * log_hl**2 - \n",
    "            (2 * np.log(2) - 1) * log_co**2\n",
    "        )\n",
    "        \n",
    "        # Normalizar para escala compar√°vel ao ATR (retorno fracion√°rio)\n",
    "        gk_mean = gk.rolling(window=window).mean()\n",
    "        return gk_mean.clip(lower=1e-8)  # Evitar divis√£o por zero\n",
    "    \n",
    "    @staticmethod  \n",
    "    def yang_zhang(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Yang-Zhang estimator (2000)\n",
    "        Melhor estimador para drift e gaps\n",
    "        \"\"\"\n",
    "        log_ho = np.log(df['high'] / df['open'])\n",
    "        log_lo = np.log(df['low'] / df['open'])\n",
    "        log_co = np.log(df['close'] / df['open'])\n",
    "        \n",
    "        log_oc = np.log(df['open'] / df['close'].shift())\n",
    "        log_oc_mean = log_oc.rolling(window=window).mean()\n",
    "        \n",
    "        log_cc = np.log(df['close'] / df['close'].shift())\n",
    "        log_cc_mean = log_cc.rolling(window=window).mean()\n",
    "        \n",
    "        # Volatilidade overnight\n",
    "        vol_overnight = (log_oc - log_oc_mean)**2\n",
    "        vol_overnight = vol_overnight.rolling(window=window).mean()\n",
    "        \n",
    "        # Volatilidade close-to-close\n",
    "        vol_cc = (log_cc - log_cc_mean)**2\n",
    "        vol_cc = vol_cc.rolling(window=window).mean()\n",
    "        \n",
    "        # Volatilidade Rogers-Satchell\n",
    "        rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
    "        vol_rs = rs.rolling(window=window).mean()\n",
    "        \n",
    "        # Combinar com pesos √≥timos\n",
    "        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "        yz = np.sqrt(vol_overnight + k * vol_cc + (1 - k) * vol_rs)\n",
    "        \n",
    "        # Normalizar para escala compar√°vel (retorno fracion√°rio)\n",
    "        # Yang-Zhang j√° est√° em escala de log-retorno, clipar para evitar zero\n",
    "        return yz.clip(lower=1e-8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parkinson(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Parkinson estimator (1980)\n",
    "        Usa high-low, ~5x mais eficiente que close-to-close\n",
    "        \"\"\"\n",
    "        log_hl = np.log(df['high'] / df['low'])\n",
    "        park = log_hl / (2 * np.sqrt(np.log(2)))\n",
    "        \n",
    "        # Normalizar para escala compar√°vel (retorno fracion√°rio)\n",
    "        park_mean = park.rolling(window=window).mean()\n",
    "        return park_mean.clip(lower=1e-8)  # Evitar divis√£o por zero\n",
    "    \n",
    "    @staticmethod\n",
    "    def realized_volatility(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"Volatilidade realizada cl√°ssica\"\"\"\n",
    "        returns = np.log(df['close'] / df['close'].shift())\n",
    "        return returns.rolling(window=window).std()\n",
    "\n",
    "print(\"‚úÖ Classe VolatilityEstimators definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14e03f",
   "metadata": {},
   "source": [
    "## 5. Sistema de Labeling Adaptativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4091405",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe AdaptiveLabeler definida\n"
     ]
    }
   ],
   "source": [
    "class AdaptiveLabeler:\n",
    "    \"\"\"\n",
    "    Sistema de rotulagem adaptativo baseado em volatilidade\n",
    "    Sistema mais robusto e interpret√°vel para mercados 24/7\n",
    "    Suporta m√∫ltiplos horizontes alinhados com timeframe de 15m\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 horizon_bars: int = 4,  # 1h em dados de 15min\n",
    "                 k: float = 1.0,  # Multiplicador do threshold\n",
    "                 vol_estimator: str = 'atr',  # Estimador de volatilidade\n",
    "                 neutral_zone: bool = True):  # Usar zona neutra\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            horizon_bars: Janela futura para calcular retorno\n",
    "            k: Multiplicador do threshold (hiperpar√¢metro a otimizar)\n",
    "            vol_estimator: 'atr', 'garman_klass', 'yang_zhang', 'parkinson'\n",
    "            neutral_zone: Se True, cria zona morta entre thresholds\n",
    "        \"\"\"\n",
    "        self.horizon_bars = horizon_bars\n",
    "        self.k = k\n",
    "        self.vol_estimator = vol_estimator\n",
    "        self.neutral_zone = neutral_zone\n",
    "        self.volatility_estimators = VolatilityEstimators()\n",
    "        \n",
    "        # Mapeamento de horizontes em minutos para bars de 15m\n",
    "        # Calcular horizonte de funding dinamicamente\n",
    "        funding_period_minutes = getattr(self, 'funding_period_minutes', 480)\n",
    "        funding_horizon_bars = funding_period_minutes // 15  # converter para barras de 15min\n",
    "        \n",
    "        self.horizon_map = {\n",
    "            '15m': 1,   # 15 minutos = 1 bar\n",
    "            '30m': 2,   # 30 minutos = 2 bars\n",
    "            '60m': 4,   # 60 minutos = 4 bars\n",
    "            '120m': 8,  # 120 minutos = 8 bars\n",
    "            '240m': 16, # 240 minutos = 16 bars\n",
    "            f'{funding_period_minutes}m': funding_horizon_bars  # funding cycle din√¢mico\n",
    "        }\n",
    "    \n",
    "    def calculate_volatility(self, df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "        \"\"\"Calcula volatilidade usando estimador selecionado\"\"\"\n",
    "        estimator_map = {\n",
    "            'atr': self.volatility_estimators.atr,\n",
    "            'garman_klass': self.volatility_estimators.garman_klass,\n",
    "            'yang_zhang': self.volatility_estimators.yang_zhang,\n",
    "            'parkinson': self.volatility_estimators.parkinson,\n",
    "            'realized': self.volatility_estimators.realized_volatility\n",
    "        }\n",
    "        \n",
    "        if self.vol_estimator not in estimator_map:\n",
    "            raise ValueError(f\"Estimador {self.vol_estimator} n√£o suportado\")\n",
    "        \n",
    "        return estimator_map[self.vol_estimator](df, window)\n",
    "    \n",
    "    def calculate_adaptive_threshold(self, df: pd.DataFrame, \n",
    "                                    window: int = 20) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calcula threshold adaptativo baseado em volatilidade\n",
    "        \n",
    "        Returns:\n",
    "            Series com threshold adaptativo para cada barra\n",
    "        \"\"\"\n",
    "        volatility = self.calculate_volatility(df, window)\n",
    "        \n",
    "        # Ajustar threshold baseado na volatilidade e horizonte\n",
    "        # Horizonte maior = threshold maior\n",
    "        horizon_adjustment = np.sqrt(self.horizon_bars)\n",
    "        \n",
    "        threshold = self.k * volatility * horizon_adjustment\n",
    "        \n",
    "        # Aplicar limite m√≠nimo e m√°ximo\n",
    "        threshold = threshold.clip(lower=0.001, upper=0.10)\n",
    "        \n",
    "        return threshold\n",
    "    \n",
    "    def create_labels(self, df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Cria labels baseados em threshold adaptativo\n",
    "        \n",
    "        Returns:\n",
    "            Series com labels: 1 (long), 0 (neutral), -1 (short)\n",
    "        \"\"\"\n",
    "        # Calcular retorno futuro\n",
    "        future_return = (\n",
    "            df['close'].shift(-self.horizon_bars) / df['close'] - 1\n",
    "        )\n",
    "        \n",
    "        # Calcular threshold adaptativo\n",
    "        threshold = self.calculate_adaptive_threshold(df, window)\n",
    "        \n",
    "        # Criar labels\n",
    "        labels = pd.Series(index=df.index, dtype=float)\n",
    "        \n",
    "        if self.neutral_zone:\n",
    "            # Com zona neutra: -1, 0, 1\n",
    "            labels[future_return > threshold] = 1  # Long\n",
    "            labels[future_return < -threshold] = -1  # Short  \n",
    "            labels[(future_return >= -threshold) & (future_return <= threshold)] = 0  # Neutral\n",
    "        else:\n",
    "            # Sem zona neutra: -1, 1\n",
    "            labels[future_return > 0] = 1  # Long\n",
    "            labels[future_return <= 0] = -1  # Short\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def get_label_distribution(self, labels: pd.Series) -> Dict:\n",
    "        \"\"\"Retorna distribui√ß√£o dos labels\"\"\"\n",
    "        counts = labels.value_counts()\n",
    "        proportions = labels.value_counts(normalize=True)\n",
    "        \n",
    "        return {\n",
    "            'counts': counts.to_dict(),\n",
    "            'proportions': proportions.to_dict(),\n",
    "            'total': len(labels.dropna()),\n",
    "            'balance_ratio': counts.min() / counts.max() if len(counts) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def optimize_k_for_horizon(self, df: pd.DataFrame, X: pd.DataFrame,\n",
    "                               horizon: str, cv_splits: int = 3,\n",
    "                               metric: str = 'f1',\n",
    "                               k_range: Tuple[float, float] = (0.5, 2.0)) -> float:\n",
    "        \"\"\"\n",
    "        Otimiza o multiplicador k para um horizonte espec√≠fico\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com OHLC\n",
    "            X: Features\n",
    "            horizon: Horizonte alvo ('15m', '30m', etc)\n",
    "            cv_splits: N√∫mero de splits para valida√ß√£o\n",
    "            metric: M√©trica para otimiza√ß√£o ('f1', 'pr_auc')\n",
    "            k_range: Range de valores de k para testar\n",
    "            \n",
    "        Returns:\n",
    "            float: k √≥timo para o horizonte\n",
    "        \"\"\"\n",
    "        # Imports j√° feitos no in√≠cio do arquivo, n√£o precisam ser repetidos\n",
    "        \n",
    "        # Configurar horizonte\n",
    "        self.horizon_bars = self.horizon_map[horizon]\n",
    "        \n",
    "        best_k = self.k\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        # Testar diferentes valores de k\n",
    "        k_values = np.linspace(k_range[0], k_range[1], 20)\n",
    "        \n",
    "        for k in k_values:\n",
    "            self.k = k\n",
    "            \n",
    "            # Criar labels com k atual\n",
    "            labels = self.create_labels(df)\n",
    "            \n",
    "            # Remover NaN\n",
    "            mask = ~(labels.isna() | X.isna().any(axis=1))\n",
    "            X_clean = X[mask]\n",
    "            y_clean = labels[mask]\n",
    "            \n",
    "            # Converter para bin√°rio se necess√°rio\n",
    "            if metric in ['f1', 'pr_auc']:\n",
    "                y_clean = (y_clean > 0).astype(int)\n",
    "            \n",
    "            # Valida√ß√£o cruzada temporal\n",
    "            tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_clean):\n",
    "                X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
    "                y_train, y_val = y_clean.iloc[train_idx], y_clean.iloc[val_idx]\n",
    "                \n",
    "                # Modelo simples para avalia√ß√£o r√°pida\n",
    "                model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                if metric == 'f1':\n",
    "                    y_pred = model.predict(X_val)\n",
    "                    score = f1_score(y_val, y_pred, average='weighted')\n",
    "                elif metric == 'pr_auc':\n",
    "                    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "                    score = average_precision_score(y_val, y_proba)\n",
    "                else:\n",
    "                    raise ValueError(f\"M√©trica {metric} n√£o suportada\")\n",
    "                \n",
    "                scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_k = k\n",
    "            \n",
    "            print(f\"k={k:.2f}: {metric}={avg_score:.4f}\")\n",
    "        \n",
    "        print(f\"‚úÖ k √≥timo para {horizon}: {best_k:.3f}\")\n",
    "        \n",
    "        return best_k\n",
    "    \n",
    "    def optimize_k_multi_horizon(self, df: pd.DataFrame, X: pd.DataFrame,\n",
    "                                 horizons: List[str] = None,\n",
    "                                 cv_splits: int = 3,\n",
    "                                 metric: str = 'pr_auc') -> Dict:\n",
    "        \"\"\"\n",
    "        Otimiza k para m√∫ltiplos horizontes simultaneamente\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com OHLC\n",
    "            X: Features\n",
    "            horizons: Lista de horizontes para otimizar\n",
    "            cv_splits: N√∫mero de splits para CV\n",
    "            metric: M√©trica para otimiza√ß√£o ('f1', 'pr_auc')\n",
    "            \n",
    "        Returns:\n",
    "            Dict com k √≥timo para cada horizonte\n",
    "        \"\"\"\n",
    "        if horizons is None:\n",
    "            horizons = ['15m', '30m', '60m', '120m']\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for horizon in horizons:\n",
    "            print(f\"\\nOtimizando k para horizonte {horizon}...\")\n",
    "            optimal_k = self.optimize_k_for_horizon(\n",
    "                df, X, horizon, cv_splits, metric\n",
    "            )\n",
    "            results[horizon] = optimal_k\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize_k(self, df: pd.DataFrame, X: pd.DataFrame, \n",
    "                   cv_splits: int = 5, metric: str = 'f1') -> float:\n",
    "        \"\"\"\n",
    "        Otimiza o multiplicador k usando valida√ß√£o cruzada temporal\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com OHLC\n",
    "            X: Features para treino\n",
    "            cv_splits: N√∫mero de splits temporais\n",
    "            metric: 'f1' ou 'balanced_accuracy'\n",
    "            \n",
    "        Returns:\n",
    "            float: k √≥timo\n",
    "        \"\"\"\n",
    "        # Imports j√° feitos no in√≠cio do arquivo, n√£o precisam ser repetidos\n",
    "        \n",
    "        best_k = self.k\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        # Range de k para testar\n",
    "        k_values = np.linspace(0.5, 2.0, 20)\n",
    "        \n",
    "        for k in k_values:\n",
    "            self.k = k\n",
    "            \n",
    "            # Criar labels com k atual\n",
    "            labels = self.create_labels(df)\n",
    "            \n",
    "            # Remover NaN\n",
    "            mask = ~(labels.isna() | X.isna().any(axis=1))\n",
    "            X_clean = X[mask]\n",
    "            y_clean = labels[mask]\n",
    "            \n",
    "            # Converter para bin√°rio (up/down)\n",
    "            y_binary = (y_clean > 0).astype(int)\n",
    "            \n",
    "            # Time Series CV\n",
    "            tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_clean):\n",
    "                X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
    "                y_train, y_val = y_binary.iloc[train_idx], y_binary.iloc[val_idx]\n",
    "                \n",
    "                # Modelo simples para teste r√°pido\n",
    "                clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_val)\n",
    "                \n",
    "                if metric == 'f1':\n",
    "                    score = f1_score(y_val, y_pred, average='weighted')\n",
    "                else:\n",
    "                    score = balanced_accuracy_score(y_val, y_pred)\n",
    "                \n",
    "                scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_k = k\n",
    "            \n",
    "            print(f\"k={k:.2f}: {metric}={avg_score:.4f}\")\n",
    "        \n",
    "        self.k = best_k\n",
    "        return best_k\n",
    "\n",
    "print(\"‚úÖ Classe AdaptiveLabeler definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37658733",
   "metadata": {},
   "source": [
    "## 6. Features para Mercado Cripto 24/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a940e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe Crypto24x7Features definida\n"
     ]
    }
   ],
   "source": [
    "# Resolver din√¢mico de funding period\n",
    "def resolve_funding_minutes(symbol: str, timestamp: pd.Timestamp = None) -> int:\n",
    "    \"\"\"\n",
    "    Resolve o per√≠odo de funding dinamicamente baseado no s√≠mbolo e data.\n",
    "    \n",
    "    Regras (fonte: Binance Support):\n",
    "    - Padr√£o: 480 min (8 horas) para maioria dos contratos perp√©tuos\n",
    "    - Condicional: 60 min (1 hora) quando funding rate atinge cap/floor\n",
    "    - Especial: Alguns contratos podem ter 240 min (4 horas)\n",
    "    \n",
    "    Args:\n",
    "        symbol: S√≠mbolo do contrato (ex: BTCUSDT)\n",
    "        timestamp: Data/hora para verificar regras espec√≠ficas do per√≠odo\n",
    "        \n",
    "    Returns:\n",
    "        Per√≠odo de funding em minutos\n",
    "    \"\"\"\n",
    "    # Dicion√°rio base de funding por s√≠mbolo\n",
    "    # Fonte: https://www.binance.com/en/support/faq/detail/360033525031\n",
    "    FUNDING_PERIODS = {\n",
    "        # Majors - 8 horas padr√£o\n",
    "        \"BTCUSDT\": 480,\n",
    "        \"ETHUSDT\": 480,\n",
    "        \"BNBUSDT\": 480,\n",
    "        \"XRPUSDT\": 480,\n",
    "        \"ADAUSDT\": 480,\n",
    "        \"SOLUSDT\": 480,\n",
    "        \"DOTUSDT\": 480,\n",
    "        \"DOGEUSDT\": 480,\n",
    "        \n",
    "        # Contratos com per√≠odo especial (exemplos)\n",
    "        # Adicionar conforme documenta√ß√£o da exchange\n",
    "    }\n",
    "    \n",
    "    # Obter per√≠odo base do s√≠mbolo\n",
    "    base_period = FUNDING_PERIODS.get(symbol, 480)  # Default 8h\n",
    "    \n",
    "    # TODO: Implementar l√≥gica condicional\n",
    "    # Se funding rate atingir cap/floor, pode mudar para 1h temporariamente\n",
    "    # Isso requer acesso ao funding rate atual da exchange\n",
    "    \n",
    "    # Log da decis√£o\n",
    "    if timestamp:\n",
    "        print(f\"üìä Funding period para {symbol} em {timestamp}: {base_period} min ({base_period/60:.1f}h)\")\n",
    "    \n",
    "    return base_period\n",
    "\n",
    "class Crypto24x7Features:\n",
    "    \"\"\"\n",
    "    Features espec√≠ficas para mercado cripto 24/7\n",
    "    Inclui calend√°rio, sess√µes regionais e funding\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cria features de calend√°rio 24/7\n",
    "        \n",
    "        Crypto n√£o tem fechamento, mas tem padr√µes:\n",
    "        - Hor√°rios de maior volume (overlaps de mercados)\n",
    "        - Dias da semana\n",
    "        - Fim de m√™s (rebalanceamento de portfolios)\n",
    "        \"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Extrair componentes temporais\n",
    "        features['hour'] = df.index.hour\n",
    "        features['day_of_week'] = df.index.dayofweek\n",
    "        features['day_of_month'] = df.index.day\n",
    "        features['week_of_year'] = df.index.isocalendar().week\n",
    "        features['month'] = df.index.month\n",
    "        features['quarter'] = df.index.quarter\n",
    "        \n",
    "        # Features c√≠clicas (encoding circular)\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)\n",
    "        features['dow_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)\n",
    "        features['dow_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)\n",
    "        features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)\n",
    "        features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)\n",
    "        \n",
    "        # Per√≠odos especiais\n",
    "        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)\n",
    "        features['is_month_end'] = (df.index.day >= 28).astype(int)\n",
    "        features['is_quarter_end'] = ((features['month'] % 3 == 0) & \n",
    "                                      (features['is_month_end'] == 1)).astype(int)\n",
    "        \n",
    "        # Hor√°rio combinado (0-167 para hora da semana)\n",
    "        features['hour_of_week'] = features['day_of_week'] * 24 + features['hour']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_session_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identifica sess√µes de trading regionais\n",
    "        \n",
    "        Principais sess√µes (UTC):\n",
    "        - Asia: 00:00 - 09:00\n",
    "        - Europe: 07:00 - 16:00  \n",
    "        - Americas: 13:00 - 22:00\n",
    "        \"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        hour = df.index.hour\n",
    "        \n",
    "        # Sess√µes principais\n",
    "        features['session_asia'] = ((hour >= 0) & (hour < 9)).astype(int)\n",
    "        features['session_europe'] = ((hour >= 7) & (hour < 16)).astype(int)\n",
    "        features['session_americas'] = ((hour >= 13) & (hour < 22)).astype(int)\n",
    "        \n",
    "        # Overlaps (maior volume/volatilidade)\n",
    "        features['overlap_asia_europe'] = ((hour >= 7) & (hour < 9)).astype(int)\n",
    "        features['overlap_europe_americas'] = ((hour >= 13) & (hour < 16)).astype(int)\n",
    "        \n",
    "        # Contagem de sess√µes ativas\n",
    "        features['active_sessions'] = (\n",
    "            features['session_asia'] + \n",
    "            features['session_europe'] + \n",
    "            features['session_americas']\n",
    "        )\n",
    "        \n",
    "        # Per√≠odo de baixa atividade\n",
    "        features['low_activity'] = (features['active_sessions'] == 0).astype(int)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_funding_features(df: pd.DataFrame, \n",
    "                               features: pd.DataFrame = None,\n",
    "                               funding_period_minutes: int = 480) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Features relacionadas ao funding rate (perpetual futures)\n",
    "        \n",
    "        Default √© 480 minutos (8 horas) - padr√£o da maioria dos contratos perpetuais\n",
    "        Alguns contratos espec√≠ficos usam 60 minutos (1 hora) - ajustar por s√≠mbolo\n",
    "        \"\"\"\n",
    "        if features is None:\n",
    "            features = pd.DataFrame(index=df.index)\n",
    "        else:\n",
    "            features = features.copy()\n",
    "        \n",
    "        # Converter per√≠odo de funding para barras (15min cada)\n",
    "        funding_period_bars = funding_period_minutes // 15  # minutos / 15 = barras\n",
    "        \n",
    "        # Identificar proximidade ao funding\n",
    "        hour = df.index.hour\n",
    "        minute = df.index.minute\n",
    "        \n",
    "        # Minutos at√© pr√≥ximo funding\n",
    "        minutes_in_day = hour * 60 + minute\n",
    "        \n",
    "        # Gerar funding times dinamicamente baseado no per√≠odo\n",
    "        funding_times = list(range(0, 1440, funding_period_minutes))\n",
    "        \n",
    "        # Calcular minutos at√© pr√≥ximo funding\n",
    "        features['minutes_to_funding'] = [\n",
    "            min(((ft - m) % 1440) for ft in funding_times) \n",
    "            for m in minutes_in_day\n",
    "        ]\n",
    "        \n",
    "        features['bars_to_funding'] = features['minutes_to_funding'] / 15\n",
    "        \n",
    "        # Proximidade ao funding (decai exponencialmente)\n",
    "        features['funding_proximity'] = np.exp(-features['bars_to_funding'] / 10)\n",
    "        \n",
    "        # √â hora de funding?\n",
    "        features['is_funding_time'] = (features['minutes_to_funding'] == 0).astype(int)\n",
    "        \n",
    "        # Janela pr√©-funding (propor√ß√£o do per√≠odo - 12.5% antes do funding)\n",
    "        pre_funding_minutes = min(60, funding_period_minutes // 8)  # M√°ximo 1 hora, ou 1/8 do per√≠odo\n",
    "        features['pre_funding_window'] = (features['minutes_to_funding'] <= pre_funding_minutes).astype(int)\n",
    "        \n",
    "        # Ciclo de funding (qual per√≠odo estamos)\n",
    "        features['funding_cycle'] = (minutes_in_day // funding_period_minutes).astype(int)\n",
    "        \n",
    "        # Features c√≠clicas para funding\n",
    "        features['funding_cycle_sin'] = np.sin(2 * np.pi * features['bars_to_funding'] / funding_period_bars)\n",
    "        features['funding_cycle_cos'] = np.cos(2 * np.pi * features['bars_to_funding'] / funding_period_bars)\n",
    "        \n",
    "        return features\n",
    "\n",
    "print(\"‚úÖ Classe Crypto24x7Features definida\")# %% [markdown]\n",
    "# ## 7. Pipeline Multi-Horizonte de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d802a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o run_multi_horizon_pipeline definida\n"
     ]
    }
   ],
   "source": [
    "def run_multi_horizon_pipeline(df: pd.DataFrame, \n",
    "                              features: pd.DataFrame,\n",
    "                              horizons: List[str] = ['15m', '30m', '60m', '120m'],\n",
    "                              test_size: float = 0.2,\n",
    "                              val_size: float = 0.2,\n",
    "                              n_trials: int = 20,  # Reduzido para otimiza√ß√£o de mem√≥ria\n",
    "                              k_range: Tuple[float, float] = (0.5, 2.0)) -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline completo para treinar e avaliar modelos em m√∫ltiplos horizontes\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com OHLC\n",
    "        features: Features preparadas\n",
    "        horizons: Lista de horizontes para avaliar\n",
    "        test_size: Propor√ß√£o para teste\n",
    "        val_size: Propor√ß√£o para valida√ß√£o  \n",
    "        n_trials: N√∫mero de trials Optuna\n",
    "        k_range: Range para otimiza√ß√£o do k\n",
    "        \n",
    "    Returns:\n",
    "        Dict com resultados para cada horizonte\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ INICIANDO PIPELINE MULTI-HORIZONTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Verificar disponibilidade de bibliotecas\n",
    "    if not XGB_AVAILABLE:\n",
    "        raise ImportError(\"XGBoost n√£o est√° dispon√≠vel\")\n",
    "    \n",
    "    # Informar sobre status das bibliotecas opcionais\n",
    "    if not OPTUNA_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è Optuna n√£o dispon√≠vel - usando hiperpar√¢metros padr√£o\")\n",
    "    elif not OPTUNA_XGBOOST_INTEGRATION:\n",
    "        print(\"‚ö†Ô∏è Optuna XGBoost integration n√£o dispon√≠vel - usando otimiza√ß√£o b√°sica\")\n",
    "    \n",
    "    if not MLFLOW_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è MLflow n√£o dispon√≠vel - resultados n√£o ser√£o tracked\")\n",
    "    \n",
    "    if not SHAP_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è SHAP n√£o dispon√≠vel - interpretabilidade limitada\")\n",
    "    \n",
    "    # Estrutura para armazenar resultados\n",
    "    results = {}\n",
    "    \n",
    "    # Configurar MLflow\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        experiment_name = f\"multi_horizon_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Split temporal dos dados\n",
    "    n_samples = len(df)\n",
    "    test_start = int(n_samples * (1 - test_size))\n",
    "    val_start = int(n_samples * (1 - test_size - val_size))\n",
    "    \n",
    "    train_idx = slice(0, val_start)\n",
    "    val_idx = slice(val_start, test_start)\n",
    "    test_idx = slice(test_start, n_samples)\n",
    "    \n",
    "    print(f\"\\nüìä Split dos dados:\")\n",
    "    print(f\"  Train: {val_start} samples ({val_start/n_samples:.1%})\")\n",
    "    print(f\"  Val:   {test_start - val_start} samples ({val_size:.1%})\")\n",
    "    print(f\"  Test:  {n_samples - test_start} samples ({test_size:.1%})\")\n",
    "    \n",
    "    # Adicionar features de funding cycle parametrizado por s√≠mbolo\n",
    "    crypto_features = Crypto24x7Features()\n",
    "    \n",
    "    # Resolver per√≠odo de funding dinamicamente\n",
    "    symbol = config.symbol if 'config' in locals() else \"BTCUSDT\"\n",
    "    last_timestamp = df.index[-1] if not df.empty else None\n",
    "    funding_period_minutes = resolve_funding_minutes(symbol, last_timestamp)\n",
    "    \n",
    "    # Log adicional se MLflow dispon√≠vel  \n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        mlflow.log_param(\"funding_period_minutes\", funding_period_minutes)\n",
    "        mlflow.log_param(\"funding_symbol\", symbol)\n",
    "    \n",
    "    features_with_funding = crypto_features.create_funding_features(\n",
    "        df, features, funding_period_minutes=funding_period_minutes\n",
    "    )\n",
    "    \n",
    "    # Processar cada horizonte\n",
    "    for horizon in horizons:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚è±Ô∏è Processando horizonte: {horizon}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        run_context = None\n",
    "        if MLFLOW_AVAILABLE:\n",
    "            import mlflow\n",
    "            try:\n",
    "                run_context = mlflow.start_run(run_name=f\"horizon_{horizon}\", nested=True)\n",
    "            except Exception:\n",
    "                # Se der erro com nested, tentar sem ou finalizar run anterior\n",
    "                try:\n",
    "                    mlflow.end_run()\n",
    "                    run_context = mlflow.start_run(run_name=f\"horizon_{horizon}\")\n",
    "                except Exception:\n",
    "                    print(\"‚ö†Ô∏è Erro ao inicializar MLflow run - continuando sem tracking\")\n",
    "                    run_context = None\n",
    "        \n",
    "        try:\n",
    "            # Log do horizonte\n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_param(\"horizon\", horizon)\n",
    "                mlflow.log_param(\"n_trials\", n_trials)\n",
    "                mlflow.log_param(\"k_range\", k_range)\n",
    "            \n",
    "            # 1. Criar labels para este horizonte\n",
    "            labeler = AdaptiveLabeler(vol_estimator='yang_zhang')\n",
    "            horizon_bars = labeler.horizon_map[horizon]\n",
    "            \n",
    "            # Otimizar k para este horizonte\n",
    "            print(f\"\\nüîç Otimizando k para horizonte {horizon}...\")\n",
    "            optimal_k = labeler.optimize_k_for_horizon(\n",
    "                df[train_idx], \n",
    "                features_with_funding[train_idx],\n",
    "                horizon=horizon,\n",
    "                cv_splits=3,\n",
    "                metric='pr_auc'\n",
    "            )\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_metric(f\"optimal_k_{horizon}\", optimal_k)\n",
    "            \n",
    "            # Criar labels com k otimizado\n",
    "            labeler.k = optimal_k\n",
    "            labeler.horizon_bars = horizon_bars\n",
    "            labels = labeler.create_labels(df)\n",
    "            \n",
    "            # 2. Preparar dados\n",
    "            X_train = features_with_funding[train_idx]\n",
    "            y_train = labels[train_idx]\n",
    "            X_val = features_with_funding[val_idx]\n",
    "            y_val = labels[val_idx]\n",
    "            X_test = features_with_funding[test_idx]\n",
    "            y_test = labels[test_idx]\n",
    "            \n",
    "            # Remover NaN\n",
    "            mask_train = ~(X_train.isna().any(axis=1) | y_train.isna())\n",
    "            mask_val = ~(X_val.isna().any(axis=1) | y_val.isna())\n",
    "            mask_test = ~(X_test.isna().any(axis=1) | y_test.isna())\n",
    "            \n",
    "            X_train = X_train[mask_train]\n",
    "            y_train = y_train[mask_train]\n",
    "            X_val = X_val[mask_val]\n",
    "            y_val = y_val[mask_val]\n",
    "            X_test = X_test[mask_test]\n",
    "            y_test = y_test[mask_test]\n",
    "            \n",
    "            # Converter labels para bin√°rio (1: up, 0: down/neutral)\n",
    "            y_train_binary = (y_train > 0).astype(int)\n",
    "            y_val_binary = (y_val > 0).astype(int)\n",
    "            y_test_binary = (y_test > 0).astype(int)\n",
    "            \n",
    "            # Log distribui√ß√£o das classes\n",
    "            train_pos_pct = y_train_binary.mean()\n",
    "            val_pos_pct = y_val_binary.mean()\n",
    "            test_pos_pct = y_test_binary.mean()\n",
    "            \n",
    "            print(f\"\\nüìà Distribui√ß√£o das classes:\")\n",
    "            print(f\"  Train: {train_pos_pct:.2%} positivos\")\n",
    "            print(f\"  Val:   {val_pos_pct:.2%} positivos\")\n",
    "            print(f\"  Test:  {test_pos_pct:.2%} positivos\")\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                mlflow.log_metric(\"train_positive_pct\", train_pos_pct)\n",
    "                mlflow.log_metric(\"val_positive_pct\", val_pos_pct)\n",
    "            \n",
    "            # 3. XGBoost n√£o precisa de normaliza√ß√£o (trees s√£o invariantes √† escala)\n",
    "            # Manter dados originais para melhor interpretabilidade\n",
    "            X_train_scaled = X_train\n",
    "            X_val_scaled = X_val\n",
    "            X_test_scaled = X_test\n",
    "            scaler = None  # XGBoost n√£o precisa\n",
    "            \n",
    "            # 4. Otimiza√ß√£o com Optuna\n",
    "            print(f\"\\nüéØ Otimizando XGBoost com Optuna...\")\n",
    "            \n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                    'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'scale_pos_weight': ((1 - train_pos_pct) / train_pos_pct) if train_pos_pct > 0 else 1.0,\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'eval_metric': 'aucpr',\n",
    "                    'tree_method': 'hist',\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                \n",
    "                # Treinar com early stopping e pruning\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                \n",
    "                # Usar callback apropriado baseado na disponibilidade\n",
    "                callbacks = []\n",
    "                if OPTUNA_AVAILABLE:\n",
    "                    callbacks.append(create_xgb_pruning_callback(trial, \"validation_0-aucpr\"))\n",
    "                \n",
    "                model.fit(\n",
    "                    X_train_scaled, y_train_binary,\n",
    "                    eval_set=[(X_val_scaled, y_val_binary)],\n",
    "                    verbose=False,\n",
    "                    early_stopping_rounds=200,\n",
    "                    callbacks=callbacks if callbacks else None\n",
    "                )\n",
    "                \n",
    "                # Salvar melhor itera√ß√£o no trial\n",
    "                if hasattr(model, 'best_iteration'):\n",
    "                    trial.set_user_attr('best_iteration', model.best_iteration)\n",
    "                \n",
    "                # Avaliar com PR-AUC\n",
    "                y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "                pr_auc = average_precision_score(y_val_binary, y_pred_proba)\n",
    "                \n",
    "                return pr_auc\n",
    "            \n",
    "            # Executar otimiza√ß√£o\n",
    "            if OPTUNA_AVAILABLE:\n",
    "                study = safe_optuna_study(\n",
    "                    direction='maximize',\n",
    "                    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                    pruner=optuna.pruners.MedianPruner()\n",
    "                )\n",
    "                \n",
    "                study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "                \n",
    "                # Melhores par√¢metros\n",
    "                best_params = study.best_params\n",
    "                best_score = study.best_value\n",
    "            else:\n",
    "                # Fallback: usar hiperpar√¢metros padr√£o otimizados\n",
    "                print(\"‚ö†Ô∏è Usando hiperpar√¢metros padr√£o (Optuna n√£o dispon√≠vel)\")\n",
    "                best_params = {\n",
    "                    'max_depth': 6,\n",
    "                    'learning_rate': 0.05,\n",
    "                    'n_estimators': 200,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'gamma': 0.1,\n",
    "                    'reg_alpha': 0.1,\n",
    "                    'reg_lambda': 1.0,\n",
    "                    'min_child_weight': 1,\n",
    "                    'scale_pos_weight': ((1 - train_pos_pct) / train_pos_pct) if train_pos_pct > 0 else 1.0,\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'eval_metric': 'aucpr',\n",
    "                    'tree_method': 'hist',\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                \n",
    "                # Avaliar hiperpar√¢metros padr√£o\n",
    "                temp_model = xgb.XGBClassifier(**best_params)\n",
    "                temp_model.fit(X_train_scaled, y_train_binary, eval_set=[(X_val_scaled, y_val_binary)], verbose=False, early_stopping_rounds=200)\n",
    "                y_pred_proba = temp_model.predict_proba(X_val_scaled)[:, 1]\n",
    "                best_score = average_precision_score(y_val_binary, y_pred_proba)\n",
    "            \n",
    "            # Recuperar melhor itera√ß√£o se dispon√≠vel (s√≥ quando Optuna usado)\n",
    "            if OPTUNA_AVAILABLE and 'study' in locals():\n",
    "                best_iteration = study.best_trial.user_attrs.get('best_iteration')\n",
    "                if best_iteration is not None:\n",
    "                    best_params['n_estimators'] = int(best_iteration)\n",
    "                    print(f\"üìä Usando melhor itera√ß√£o do early stopping: {best_iteration}\")\n",
    "            \n",
    "            best_params.update({\n",
    "                'scale_pos_weight': ((1 - train_pos_pct) / train_pos_pct) if train_pos_pct > 0 else 1.0,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'aucpr',\n",
    "                'tree_method': 'hist',\n",
    "                'random_state': 42\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n‚úÖ Melhor PR-AUC em valida√ß√£o: {best_score:.4f}\")\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                mlflow.log_metric(f\"best_pr_auc_val_{horizon}\", best_score)\n",
    "                mlflow.log_params({f\"xgb_{k}_{horizon}\": v for k, v in best_params.items()})\n",
    "            \n",
    "            # 5. Treinar modelo final\n",
    "            print(f\"\\nüèãÔ∏è Treinando modelo final...\")\n",
    "            final_model = xgb.XGBClassifier(**best_params)\n",
    "            final_model.fit(\n",
    "                X_train_scaled, y_train_binary,\n",
    "                eval_set=[(X_val_scaled, y_val_binary)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200  # Manter early stopping no modelo final\n",
    "            )\n",
    "            \n",
    "            # 6. Calibra√ß√£o de probabilidades\n",
    "            print(f\"\\nüìê Calibrando probabilidades...\")\n",
    "            calibrator = CalibratedClassifierCV(\n",
    "                final_model, \n",
    "                method='isotonic',\n",
    "                cv='prefit'\n",
    "            )\n",
    "            calibrator.fit(X_val, y_val_binary)  # Usar dados originais\n",
    "            \n",
    "            # 7. Otimizar threshold no VALIDATION (n√£o no teste!)\n",
    "            print(f\"\\nüîç Otimizando threshold no conjunto de valida√ß√£o...\")\n",
    "            \n",
    "            # Predi√ß√µes calibradas no validation para escolher threshold\n",
    "            y_val_pred_cal = calibrator.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Otimizar threshold baseado em F1 no VALIDATION\n",
    "            precision, recall, thresholds = precision_recall_curve(\n",
    "                y_val_binary, y_val_pred_cal\n",
    "            )\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            best_threshold_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds[best_threshold_idx] if best_threshold_idx < len(thresholds) else 0.5\n",
    "            \n",
    "            print(f\"  Threshold √≥timo (do validation): {best_threshold:.4f}\")\n",
    "            \n",
    "            # 8. Avalia√ß√£o em teste com threshold fixo\n",
    "            print(f\"\\nüìä Avaliando em conjunto de teste com threshold fixo...\")\n",
    "            \n",
    "            # Predi√ß√µes n√£o calibradas\n",
    "            y_test_pred_raw = final_model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Predi√ß√µes calibradas\n",
    "            y_test_pred_cal = calibrator.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Aplicar threshold\n",
    "            y_test_pred_binary = (y_test_pred_cal >= best_threshold).astype(int)\n",
    "            \n",
    "            # M√©tricas finais\n",
    "            test_pr_auc = average_precision_score(y_test_binary, y_test_pred_cal)\n",
    "            test_f1 = f1_score(y_test_binary, y_test_pred_binary)\n",
    "            test_mcc = matthews_corrcoef(y_test_binary, y_test_pred_binary)\n",
    "            \n",
    "            # Matriz de confus√£o (com labels expl√≠citos para evitar erros)\n",
    "            cm = confusion_matrix(y_test_binary, y_test_pred_binary, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "            # M√©tricas adicionais\n",
    "            precision_score_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall_score_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            print(f\"\\nüìà M√©tricas em teste para {horizon}:\")\n",
    "            print(f\"  PR-AUC:      {test_pr_auc:.4f}\")\n",
    "            print(f\"  F1 Score:    {test_f1:.4f}\")\n",
    "            print(f\"  MCC:         {test_mcc:.4f}\")\n",
    "            print(f\"  Precision:   {precision_score_val:.4f}\")\n",
    "            print(f\"  Recall:      {recall_score_val:.4f}\")\n",
    "            print(f\"  Specificity: {specificity:.4f}\")\n",
    "            print(f\"  Threshold:   {best_threshold:.4f}\")\n",
    "            \n",
    "            # Log m√©tricas no MLflow\n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_metrics({\n",
    "                    f\"test_pr_auc_{horizon}\": test_pr_auc,\n",
    "                    f\"test_f1_{horizon}\": test_f1,\n",
    "                    f\"test_mcc_{horizon}\": test_mcc,\n",
    "                    f\"test_precision_{horizon}\": precision_score_val,\n",
    "                    f\"test_recall_{horizon}\": recall_score_val,\n",
    "                    f\"test_specificity_{horizon}\": specificity,\n",
    "                    f\"best_threshold_{horizon}\": best_threshold\n",
    "                })\n",
    "            \n",
    "            # 8. An√°lise de import√¢ncia de features\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': final_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nüîù Top 10 features mais importantes:\")\n",
    "            for idx, row in feature_importance.head(10).iterrows():\n",
    "                print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "            \n",
    "            # Salvar resultados (incluindo √≠ndices para alinhamento no backtest)\n",
    "            results[horizon] = {\n",
    "                'model': final_model,\n",
    "                'calibrator': calibrator,\n",
    "                'scaler': scaler,\n",
    "                'labeler': labeler,\n",
    "                'threshold': best_threshold,\n",
    "                'metrics': {\n",
    "                    'pr_auc': test_pr_auc,\n",
    "                    'f1': test_f1,\n",
    "                    'mcc': test_mcc,\n",
    "                    'precision': precision_score_val,\n",
    "                    'recall': recall_score_val,\n",
    "                    'specificity': specificity\n",
    "                },\n",
    "                'confusion_matrix': cm,\n",
    "                'feature_importance': feature_importance,\n",
    "                'predictions': {\n",
    "                    'raw': y_test_pred_raw,\n",
    "                    'calibrated': y_test_pred_cal,\n",
    "                    'binary': y_test_pred_binary\n",
    "                },\n",
    "                'labels': y_test_binary,\n",
    "                'optimal_k': optimal_k,\n",
    "                'test_indices': X_test.index  # Salvar √≠ndices para backtest\n",
    "            }\n",
    "            \n",
    "            # Limpeza de mem√≥ria ap√≥s cada horizonte\n",
    "            del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "            del X_train_scaled, X_val_scaled, X_test_scaled\n",
    "            del y_train_binary, y_val_binary, y_test_binary\n",
    "            del y_test_pred_raw, y_test_pred_cal, y_test_pred_binary\n",
    "            if 'temp_model' in locals():\n",
    "                del temp_model\n",
    "            gc.collect()\n",
    "            print(f\"üßπ Mem√≥ria limpa ap√≥s horizonte {horizon}\")\n",
    "            \n",
    "            # Salvar modelo\n",
    "            import joblib\n",
    "            model_path = f\"{config.models_path}/xgb_{horizon}_{experiment_name if MLFLOW_AVAILABLE else 'local'}.pkl\"\n",
    "            os.makedirs(config.models_path, exist_ok=True)\n",
    "            joblib.dump({\n",
    "                'model': final_model,\n",
    "                'calibrator': calibrator,\n",
    "                'scaler': scaler,\n",
    "                'threshold': best_threshold\n",
    "            }, model_path)\n",
    "            \n",
    "            if MLFLOW_AVAILABLE:\n",
    "                import mlflow\n",
    "                mlflow.log_artifact(model_path)\n",
    "        \n",
    "        finally:\n",
    "            if MLFLOW_AVAILABLE and run_context:\n",
    "                import mlflow\n",
    "                mlflow.end_run()\n",
    "    \n",
    "    # 9. An√°lise comparativa entre horizontes\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìä AN√ÅLISE COMPARATIVA ENTRE HORIZONTES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        horizon: {\n",
    "            'PR-AUC': results[horizon]['metrics']['pr_auc'],\n",
    "            'F1': results[horizon]['metrics']['f1'],\n",
    "            'MCC': results[horizon]['metrics']['mcc'],\n",
    "            'Precision': results[horizon]['metrics']['precision'],\n",
    "            'Recall': results[horizon]['metrics']['recall'],\n",
    "            'Optimal_k': results[horizon]['optimal_k']\n",
    "        }\n",
    "        for horizon in horizons\n",
    "    }).T\n",
    "    \n",
    "    print(\"\\nüìà Tabela Comparativa:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Identificar melhor horizonte\n",
    "    best_horizon_pr_auc = comparison_df['PR-AUC'].idxmax()\n",
    "    best_horizon_f1 = comparison_df['F1'].idxmax()\n",
    "    \n",
    "    print(f\"\\nüèÜ Melhores horizontes:\")\n",
    "    print(f\"  Melhor PR-AUC: {best_horizon_pr_auc} ({comparison_df.loc[best_horizon_pr_auc, 'PR-AUC']:.4f})\")\n",
    "    print(f\"  Melhor F1:     {best_horizon_f1} ({comparison_df.loc[best_horizon_f1, 'F1']:.4f})\")\n",
    "    \n",
    "    # 10. An√°lise de correla√ß√£o entre predi√ß√µes\n",
    "    print(f\"\\nüîó Correla√ß√£o entre predi√ß√µes dos horizontes:\")\n",
    "    pred_matrix = pd.DataFrame({\n",
    "        horizon: results[horizon]['predictions']['calibrated']\n",
    "        for horizon in horizons\n",
    "    })\n",
    "    \n",
    "    corr_matrix = pred_matrix.corr()\n",
    "    print(corr_matrix.round(3))\n",
    "    \n",
    "    # Salvar compara√ß√£o\n",
    "    comparison_df.to_csv(f\"{config.reports_path}/horizon_comparison_{experiment_name if MLFLOW_AVAILABLE else 'local'}.csv\")\n",
    "    \n",
    "    # Limpeza final de mem√≥ria\n",
    "    del features_with_funding, pred_matrix, corr_matrix, comparison_df\n",
    "    gc.collect()\n",
    "    print(f\"\\nüßπ Pipeline finalizado - mem√≥ria limpa\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o run_multi_horizon_pipeline definida\")# %% [markdown]\n",
    "# ## 8. Pipeline LSTM para S√©ries Temporais\n",
    "# \n",
    "# Implementa√ß√£o de LSTM com:\n",
    "# - Otimiza√ß√£o Bayesiana via Optuna\n",
    "# - PR-AUC como m√©trica alvo\n",
    "# - Calibra√ß√£o isot√¥nica\n",
    "# - Threshold otimizado no validation\n",
    "# - Export para produ√ß√£o via TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "023d2d4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fun√ß√µes de prepara√ß√£o de dados sequenciais\n",
    "def make_sequences(X_df: pd.DataFrame, y_series: pd.Series, seq_len: int):\n",
    "    \"\"\"\n",
    "    Cria sequ√™ncias para LSTM a partir de dados tabulares\n",
    "    \n",
    "    Args:\n",
    "        X_df: Features\n",
    "        y_series: Labels\n",
    "        seq_len: Comprimento da sequ√™ncia\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: Array de sequ√™ncias [n_samples, seq_len, n_features]\n",
    "        y_seq: Array de labels\n",
    "        idx_seq: √çndices pandas correspondentes\n",
    "    \"\"\"\n",
    "    X = X_df.values.astype(np.float32)\n",
    "    y = y_series.values.astype(np.int64)\n",
    "    idx = X_df.index\n",
    "\n",
    "    X_seq, y_seq, idx_seq = [], [], []\n",
    "    for t in range(seq_len, len(X)):\n",
    "        X_seq.append(X[t-seq_len:t])\n",
    "        y_seq.append(y[t])\n",
    "        idx_seq.append(idx[t])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq), pd.Index(idx_seq)\n",
    "\n",
    "def train_val_test_split_time(X_df: pd.DataFrame, y: pd.Series, n_splits: int = 5):\n",
    "    \"\"\"\n",
    "    Split temporal para treino/valida√ß√£o/teste\n",
    "    \n",
    "    Args:\n",
    "        X_df: Features\n",
    "        y: Labels\n",
    "        n_splits: N√∫mero de splits para TimeSeriesSplit\n",
    "        \n",
    "    Returns:\n",
    "        tr_idx: √çndices de treino\n",
    "        va_idx: √çndices de valida√ß√£o\n",
    "        te_idx: √çndices de teste\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = list(tscv.split(X_df))\n",
    "    \n",
    "    # Pen√∫ltimo split para valida√ß√£o\n",
    "    (tr_idx, va_idx) = splits[-2]\n",
    "    # √öltimo split para teste\n",
    "    (tr2_idx, te_idx) = splits[-1]\n",
    "    \n",
    "    # Treino = do in√≠cio at√© fim do pen√∫ltimo split\n",
    "    tr_idx_full = np.arange(0, va_idx[-1] + 1)\n",
    "    \n",
    "    return tr_idx_full, va_idx, te_idx\n",
    "\n",
    "def build_lstm_tensors(X_df: pd.DataFrame, y: pd.Series, seq_len: int, \n",
    "                      tr_idx, va_idx, te_idx):\n",
    "    \"\"\"\n",
    "    Prepara tensores para LSTM com escalonamento\n",
    "    \n",
    "    Args:\n",
    "        X_df: Features\n",
    "        y: Labels\n",
    "        seq_len: Comprimento da sequ√™ncia\n",
    "        tr_idx, va_idx, te_idx: √çndices dos splits\n",
    "        \n",
    "    Returns:\n",
    "        Tupla com tensores de treino/val/test, √≠ndices e scaler\n",
    "    \"\"\"\n",
    "    # Escalonar features com base no treino\n",
    "    scaler = StandardScaler()\n",
    "    X_train_df = X_df.iloc[tr_idx]\n",
    "    scaler.fit(X_train_df.values)\n",
    "    \n",
    "    # Aplicar escalonamento\n",
    "    Xs = pd.DataFrame(\n",
    "        scaler.transform(X_df.values), \n",
    "        index=X_df.index, \n",
    "        columns=X_df.columns\n",
    "    )\n",
    "    \n",
    "    # Gerar sequ√™ncias\n",
    "    X_seq, y_seq, idx_seq = make_sequences(Xs, y, seq_len)\n",
    "    \n",
    "    # Mapear √≠ndices originais para √≠ndices da sequ√™ncia\n",
    "    idx_map = pd.Series(range(len(idx_seq)), index=idx_seq)\n",
    "    \n",
    "    # Obter √≠ndices das sequ√™ncias para cada split\n",
    "    tr = idx_map[idx_seq.intersection(X_df.index[tr_idx])].dropna().astype(int).values\n",
    "    va = idx_map[idx_seq.intersection(X_df.index[va_idx])].dropna().astype(int).values\n",
    "    te = idx_map[idx_seq.intersection(X_df.index[te_idx])].dropna().astype(int).values\n",
    "    \n",
    "    # Separar tensores\n",
    "    Xtr, ytr = X_seq[tr], y_seq[tr]\n",
    "    Xva, yva = X_seq[va], y_seq[va]\n",
    "    Xte, yte = X_seq[te], y_seq[te]\n",
    "    \n",
    "    return (Xtr, ytr, Xva, yva, Xte, yte, idx_seq[te], scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7be7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo LSTM para classifica√ß√£o bin√°ria\n",
    "if TORCH_AVAILABLE:\n",
    "    class LSTMClassifier(nn.Module):\n",
    "        \"\"\"\n",
    "        LSTM para classifica√ß√£o bin√°ria de s√©ries temporais\n",
    "        \"\"\"\n",
    "        def __init__(self, in_dim: int, hidden: int, layers: int, dropout: float):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(\n",
    "                in_dim, hidden, \n",
    "                num_layers=layers, \n",
    "                batch_first=True, \n",
    "                dropout=dropout if layers > 1 else 0\n",
    "            )\n",
    "            self.head = nn.Linear(hidden, 1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x: [batch, seq, feat]\n",
    "            out, _ = self.lstm(x)\n",
    "            # Usar apenas √∫ltimo timestep\n",
    "            logit = self.head(out[:, -1, :])\n",
    "            return logit.squeeze(1)  # Retorna logits (sem sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3314238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes de treinamento e avalia√ß√£o\n",
    "if TORCH_AVAILABLE:\n",
    "    def train_one_epoch(model, optimizer, loss_fn, loader, device):\n",
    "        \"\"\"\n",
    "        Treina modelo por uma √©poca\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            optimizer: Otimizador\n",
    "            loss_fn: Fun√ß√£o de perda\n",
    "            loader: DataLoader\n",
    "            device: Device (cuda/cpu)\n",
    "            \n",
    "        Returns:\n",
    "            Loss m√©dio da √©poca\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping para estabilidade\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            \n",
    "        return total_loss / len(loader.dataset)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_ap(model, loader, device):\n",
    "        \"\"\"\n",
    "        Avalia modelo com Average Precision (PR-AUC)\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            loader: DataLoader\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            ap: Average Precision score\n",
    "            probs: Probabilidades preditas\n",
    "            labels: Labels verdadeiros\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        probs_list, labels_list = [], []\n",
    "        \n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs_list.append(probs)\n",
    "            labels_list.append(yb.numpy())\n",
    "        \n",
    "        labels = np.concatenate(labels_list)\n",
    "        probs = np.concatenate(probs_list)\n",
    "        \n",
    "        ap = average_precision_score(labels, probs)\n",
    "        return ap, probs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d65e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo do Optuna para LSTM\n",
    "if TORCH_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    def objective_lstm(trial, X_train, y_train, X_val, y_val, seq_len, device):\n",
    "        \"\"\"\n",
    "        Fun√ß√£o objetivo para otimiza√ß√£o com Optuna\n",
    "        \n",
    "        Args:\n",
    "            trial: Trial do Optuna\n",
    "            X_train, y_train: Dados de treino\n",
    "            X_val, y_val: Dados de valida√ß√£o\n",
    "            seq_len: Comprimento da sequ√™ncia\n",
    "            device: Device (cuda/cpu)\n",
    "            \n",
    "        Returns:\n",
    "            Best Average Precision obtido\n",
    "        \"\"\"\n",
    "        # Hiperpar√¢metros para otimizar\n",
    "        hidden = trial.suggest_categorical(\"hidden\", [64, 128, 256])\n",
    "        layers = trial.suggest_int(\"layers\", 1, 3)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "        \n",
    "        # Criar modelo\n",
    "        model = LSTMClassifier(\n",
    "            X_train.shape[-1], hidden, layers, dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        # BCEWithLogitsLoss com peso para desbalanceamento\n",
    "        pos_ratio = y_train.mean()\n",
    "        pos_ratio = float(pos_ratio if pos_ratio > 0 else 1e-6)\n",
    "        pos_weight = torch.tensor((1 - pos_ratio) / pos_ratio, device=device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Otimizador\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=wd\n",
    "        )\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32)\n",
    "        )\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(X_val, dtype=torch.float32),\n",
    "            torch.tensor(y_val, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=512, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Treinar com early stopping via pruning\n",
    "        best_ap = 0.0\n",
    "        patience_counter = 0\n",
    "        patience = 10\n",
    "        \n",
    "        for epoch in range(60):\n",
    "            # Treinar\n",
    "            train_loss = train_one_epoch(\n",
    "                model, optimizer, loss_fn, train_loader, device\n",
    "            )\n",
    "            \n",
    "            # Avaliar\n",
    "            ap, _, _ = eval_ap(model, val_loader, device)\n",
    "            \n",
    "            # Reportar ao Optuna\n",
    "            trial.report(ap, epoch)\n",
    "            \n",
    "            # Pruning\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "            # Track best\n",
    "            if ap > best_ap:\n",
    "                best_ap = ap\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "                \n",
    "        return best_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97efe3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de treinamento com Optuna\n",
    "if TORCH_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    def fit_lstm_with_optuna(Xtr, ytr, Xva, yva, n_trials=50, \n",
    "                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Treina LSTM com otimiza√ß√£o Bayesiana\n",
    "        \n",
    "        Args:\n",
    "            Xtr, ytr: Dados de treino\n",
    "            Xva, yva: Dados de valida√ß√£o\n",
    "            n_trials: N√∫mero de trials do Optuna\n",
    "            device: Device para treino\n",
    "            \n",
    "        Returns:\n",
    "            model: Modelo treinado\n",
    "            best_params: Melhores hiperpar√¢metros\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç Otimiza√ß√£o Bayesiana com Optuna ({n_trials} trials)\")\n",
    "        \n",
    "        # Criar estudo\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner()\n",
    "        )\n",
    "        \n",
    "        # Otimizar\n",
    "        study.optimize(\n",
    "            lambda t: objective_lstm(t, Xtr, ytr, Xva, yva, Xtr.shape[1], device),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"‚úÖ Melhor AP em valida√ß√£o: {study.best_value:.4f}\")\n",
    "        print(f\"üìä Melhores par√¢metros: {best_params}\")\n",
    "        \n",
    "        # Re-treinar com melhores par√¢metros no conjunto completo (treino + val)\n",
    "        print(f\"\\nüèãÔ∏è Treinando modelo final...\")\n",
    "        \n",
    "        model = LSTMClassifier(\n",
    "            Xtr.shape[-1], \n",
    "            best_params['hidden'],\n",
    "            best_params['layers'],\n",
    "            best_params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Combinar treino e valida√ß√£o\n",
    "        X_combined = np.concatenate([Xtr, Xva])\n",
    "        y_combined = np.concatenate([ytr, yva])\n",
    "        \n",
    "        # Loss com peso para manter balanceamento de classes\n",
    "        pos_ratio = y_combined.mean()\n",
    "        pos_ratio = float(pos_ratio if pos_ratio > 0 else 1e-6)\n",
    "        pos_weight = torch.tensor((1 - pos_ratio) / pos_ratio, device=device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=best_params['lr'],\n",
    "            weight_decay=best_params['weight_decay']\n",
    "        )\n",
    "        \n",
    "        combined_dataset = TensorDataset(\n",
    "            torch.tensor(X_combined, dtype=torch.float32),\n",
    "            torch.tensor(y_combined, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        combined_loader = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # Treinar por mais √©pocas (1.5x)\n",
    "        for epoch in range(90):\n",
    "            train_loss = train_one_epoch(\n",
    "                model, optimizer, loss_fn, combined_loader, device\n",
    "            )\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  √âpoca {epoch}: Loss = {train_loss:.4f}\")\n",
    "        \n",
    "        return model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57c7f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibra√ß√£o e sele√ß√£o de threshold\n",
    "if TORCH_AVAILABLE:\n",
    "    @torch.no_grad()\n",
    "    def calibrate_and_choose_threshold(model, Xva, yva, device):\n",
    "        \"\"\"\n",
    "        Calibra probabilidades e escolhe threshold √≥timo\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM treinado\n",
    "            Xva, yva: Dados de valida√ß√£o\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            calibrator: Calibrador isot√¥nico\n",
    "            threshold: Threshold √≥timo\n",
    "        \"\"\"\n",
    "        from sklearn.isotonic import IsotonicRegression\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Obter probabilidades\n",
    "        Xva_tensor = torch.tensor(Xva, dtype=torch.float32).to(device)\n",
    "        logits = model(Xva_tensor)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        # Calibra√ß√£o isot√¥nica\n",
    "        calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        probs_cal = calibrator.fit_transform(probs, yva)\n",
    "        \n",
    "        # Escolher threshold que maximiza F1\n",
    "        precision, recall, thresholds = precision_recall_curve(yva, probs_cal)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
    "        \n",
    "        # Ignorar √∫ltimo elemento (threshold = 1.0)\n",
    "        best_idx = np.nanargmax(f1_scores[:-1])\n",
    "        best_threshold = float(thresholds[best_idx]) if len(thresholds) > 0 else 0.5\n",
    "        best_f1 = f1_scores[best_idx]\n",
    "        \n",
    "        print(f\"\\nüìê Calibra√ß√£o completa\")\n",
    "        print(f\"  Threshold √≥timo: {best_threshold:.3f}\")\n",
    "        print(f\"  F1 em valida√ß√£o: {best_f1:.3f}\")\n",
    "        \n",
    "        # Calcular Brier score\n",
    "        from sklearn.metrics import brier_score_loss\n",
    "        brier_before = brier_score_loss(yva, probs)\n",
    "        brier_after = brier_score_loss(yva, probs_cal)\n",
    "        print(f\"  Brier Score: {brier_before:.4f} ‚Üí {brier_after:.4f}\")\n",
    "        \n",
    "        return calibrator, best_threshold\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_proba(model, X, device):\n",
    "        \"\"\"\n",
    "        Prediz probabilidades para conjunto de dados\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            X: Features\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            Probabilidades preditas\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        logits = model(X_tensor)\n",
    "        return torch.sigmoid(logits).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d80a7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia√ß√£o no teste e gera√ß√£o de sinais\n",
    "if TORCH_AVAILABLE:\n",
    "    def evaluate_on_test(model, Xte, yte, calibrator, threshold, test_index, device):\n",
    "        \"\"\"\n",
    "        Avalia modelo no conjunto de teste\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM treinado\n",
    "            Xte, yte: Dados de teste\n",
    "            calibrator: Calibrador isot√¥nico\n",
    "            threshold: Threshold escolhido\n",
    "            test_index: √çndices do teste\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            Dicion√°rio com m√©tricas e predi√ß√µes\n",
    "        \"\"\"\n",
    "        # Predi√ß√µes\n",
    "        probs = predict_proba(model, Xte, device)\n",
    "        probs_cal = calibrator.transform(probs)\n",
    "        preds = (probs_cal >= threshold).astype(int)\n",
    "        \n",
    "        # M√©tricas\n",
    "        ap = average_precision_score(yte, probs_cal)\n",
    "        f1 = f1_score(yte, preds)\n",
    "        acc = accuracy_score(yte, preds)\n",
    "        \n",
    "        # MCC e Brier\n",
    "        mcc = matthews_corrcoef(yte, preds)\n",
    "        brier = brier_score_loss(yte, probs_cal)\n",
    "        \n",
    "        # Confusion matrix - for√ßar shape 2x2 para evitar erro quando s√≥ uma classe √© predita\n",
    "        cm = confusion_matrix(yte, preds, labels=[0, 1])\n",
    "        \n",
    "        # Desempacotar matriz de forma segura\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Fallback se algo der errado\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Sinais para backtest (-1 para short, +1 para long)\n",
    "        signals = pd.Series((preds * 2 - 1), index=test_index)\n",
    "        \n",
    "        print(f\"\\nüìä Resultados no Teste:\")\n",
    "        print(f\"  AP (PR-AUC): {ap:.4f}\")\n",
    "        print(f\"  F1 Score:    {f1:.4f}\")\n",
    "        print(f\"  Accuracy:    {acc:.4f}\")\n",
    "        print(f\"  MCC:         {mcc:.4f}\")\n",
    "        print(f\"  Brier Score: {brier:.4f}\")\n",
    "        print(f\"\\n  Confusion Matrix:\")\n",
    "        print(f\"    TN: {tn:4d}  FP: {fp:4d}\")\n",
    "        print(f\"    FN: {fn:4d}  TP: {tp:4d}\")\n",
    "        \n",
    "        return {\n",
    "            \"ap\": ap,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy\": acc,\n",
    "            \"mcc\": mcc,\n",
    "            \"brier\": brier,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"proba\": probs_cal,\n",
    "            \"pred\": preds,\n",
    "            \"signals\": signals\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "303d9b0f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Export para produ√ß√£o\n",
    "if TORCH_AVAILABLE:\n",
    "    def export_torchscript(model, in_dim, seq_len, path=\"lstm_model.pt\", device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Exporta modelo para TorchScript\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo LSTM\n",
    "            in_dim: Dimens√£o de entrada\n",
    "            seq_len: Comprimento da sequ√™ncia\n",
    "            path: Caminho para salvar\n",
    "            device: Device\n",
    "            \n",
    "        Returns:\n",
    "            Caminho do arquivo salvo\n",
    "        \"\"\"\n",
    "        model_cpu = model.to(device).eval()\n",
    "        \n",
    "        # Input dummy para tracing\n",
    "        dummy_input = torch.randn(1, seq_len, in_dim).to(device)\n",
    "        \n",
    "        # Trace e salvar\n",
    "        traced_model = torch.jit.trace(model_cpu, dummy_input)\n",
    "        traced_model.save(path)\n",
    "        \n",
    "        print(f\"‚úÖ Modelo exportado para: {path}\")\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b3323f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline LSTM definido e pronto para uso\n"
     ]
    }
   ],
   "source": [
    "# Pipeline completo LSTM\n",
    "def run_lstm_pipeline(X_df: pd.DataFrame, \n",
    "                     y_series: pd.Series,\n",
    "                     seq_len: int = 64,\n",
    "                     n_trials: int = 20,  # Reduzido para otimiza√ß√£o de mem√≥ria\n",
    "                     device: str = None,\n",
    "                     horizon: str = \"lstm\") -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline completo para treinar LSTM com Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_df: DataFrame com features\n",
    "        y_series: Series com labels bin√°rias\n",
    "        seq_len: Comprimento das sequ√™ncias (default: 64 = 16 horas em 15min)\n",
    "        n_trials: N√∫mero de trials do Optuna\n",
    "        device: Device para treino (None = auto-detectar)\n",
    "        horizon: Nome do horizonte para logging\n",
    "        \n",
    "    Returns:\n",
    "        Dicion√°rio com resultados compat√≠vel com run_multi_horizon_backtest\n",
    "    \"\"\"\n",
    "    if not TORCH_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è PyTorch n√£o dispon√≠vel - pulando LSTM\")\n",
    "        return {}\n",
    "    \n",
    "    if not OPTUNA_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è Optuna n√£o dispon√≠vel - pulando otimiza√ß√£o LSTM\")\n",
    "        return {}\n",
    "    \n",
    "    # Auto-detectar device\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ PIPELINE LSTM - Horizonte: {horizon}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"üìä Dataset: {len(X_df)} amostras, {X_df.shape[1]} features\")\n",
    "    print(f\"‚öôÔ∏è  Device: {device}\")\n",
    "    print(f\"üìè Sequ√™ncia: {seq_len} timesteps\")\n",
    "    \n",
    "    # 1. Split temporal\n",
    "    print(f\"\\n1Ô∏è‚É£ Preparando splits temporais...\")\n",
    "    tr_idx, va_idx, te_idx = train_val_test_split_time(X_df, y_series)\n",
    "    print(f\"  Treino:    {len(tr_idx)} amostras\")\n",
    "    print(f\"  Valida√ß√£o: {len(va_idx)} amostras\")\n",
    "    print(f\"  Teste:     {len(te_idx)} amostras\")\n",
    "    \n",
    "    # 2. Preparar tensores\n",
    "    print(f\"\\n2Ô∏è‚É£ Gerando sequ√™ncias e escalonando features...\")\n",
    "    (Xtr, ytr, Xva, yva, Xte, yte, test_index, scaler) = build_lstm_tensors(\n",
    "        X_df, y_series, seq_len, tr_idx, va_idx, te_idx\n",
    "    )\n",
    "    print(f\"  Sequ√™ncias treino: {Xtr.shape}\")\n",
    "    print(f\"  Sequ√™ncias valid:  {Xva.shape}\")\n",
    "    print(f\"  Sequ√™ncias teste:  {Xte.shape}\")\n",
    "    \n",
    "    # 3. Otimiza√ß√£o com Optuna\n",
    "    print(f\"\\n3Ô∏è‚É£ Otimiza√ß√£o Bayesiana...\")\n",
    "    model, best_params = fit_lstm_with_optuna(\n",
    "        Xtr, ytr, Xva, yva, n_trials=n_trials, device=device\n",
    "    )\n",
    "    \n",
    "    # 4. Calibra√ß√£o e threshold\n",
    "    print(f\"\\n4Ô∏è‚É£ Calibra√ß√£o de probabilidades...\")\n",
    "    calibrator, threshold = calibrate_and_choose_threshold(\n",
    "        model, Xva, yva, device\n",
    "    )\n",
    "    \n",
    "    # 5. Avalia√ß√£o no teste\n",
    "    print(f\"\\n5Ô∏è‚É£ Avalia√ß√£o no conjunto de teste...\")\n",
    "    eval_results = evaluate_on_test(\n",
    "        model, Xte, yte, calibrator, threshold, test_index, device\n",
    "    )\n",
    "    \n",
    "    # 6. Export para produ√ß√£o\n",
    "    print(f\"\\n6Ô∏è‚É£ Exportando modelo...\")\n",
    "    model_path = f\"artifacts/models/lstm_{horizon}.pt\"\n",
    "    os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "    \n",
    "    export_path = export_torchscript(\n",
    "        model, Xtr.shape[-1], seq_len, \n",
    "        path=model_path, device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "    # 7. Preparar resultados no formato esperado\n",
    "    results = {\n",
    "        horizon: {\n",
    "            \"best_params\": best_params,\n",
    "            \"threshold\": threshold,\n",
    "            \"test_metrics\": {\n",
    "                \"accuracy\": eval_results[\"accuracy\"],\n",
    "                \"precision\": eval_results[\"confusion_matrix\"][1,1] / \n",
    "                            (eval_results[\"confusion_matrix\"][1,1] + \n",
    "                             eval_results[\"confusion_matrix\"][0,1] + 1e-10),\n",
    "                \"recall\": eval_results[\"confusion_matrix\"][1,1] / \n",
    "                         (eval_results[\"confusion_matrix\"][1,1] + \n",
    "                          eval_results[\"confusion_matrix\"][1,0] + 1e-10),\n",
    "                \"f1\": eval_results[\"f1\"],\n",
    "                \"pr_auc\": eval_results[\"ap\"],\n",
    "                \"mcc\": eval_results[\"mcc\"],\n",
    "                \"brier\": eval_results[\"brier\"]\n",
    "            },\n",
    "            \"test_indices\": test_index.tolist(),\n",
    "            \"predictions\": {\n",
    "                \"proba\": eval_results[\"proba\"],\n",
    "                \"binary\": eval_results[\"pred\"]\n",
    "            },\n",
    "            \"signals\": eval_results[\"signals\"],\n",
    "            \"confusion_matrix\": eval_results[\"confusion_matrix\"],\n",
    "            \"artifact\": export_path,\n",
    "            \"scaler\": scaler,\n",
    "            \"model_type\": \"lstm\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # MLflow logging se dispon√≠vel\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        import mlflow\n",
    "        \n",
    "        mlflow.log_params({\n",
    "            f\"lstm_seq_len_{horizon}\": seq_len,\n",
    "            f\"lstm_device_{horizon}\": device,\n",
    "            **{f\"lstm_{k}_{horizon}\": v for k, v in best_params.items()}\n",
    "        })\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            f\"lstm_pr_auc_test_{horizon}\": eval_results[\"ap\"],\n",
    "            f\"lstm_f1_test_{horizon}\": eval_results[\"f1\"],\n",
    "            f\"lstm_mcc_test_{horizon}\": eval_results[\"mcc\"],\n",
    "            f\"lstm_brier_test_{horizon}\": eval_results[\"brier\"]\n",
    "        })\n",
    "        \n",
    "        mlflow.log_artifact(export_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline LSTM completo para horizonte {horizon}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Pipeline LSTM definido e pronto para uso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b176c",
   "metadata": {},
   "source": [
    "## 9. Sistema de Backtest Multi-Horizonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e65596fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Usando implementa√ß√£o local do BacktestEngine (imports do projeto n√£o dispon√≠veis)\n",
      "‚úÖ Fun√ß√£o run_multi_horizon_backtest definida\n"
     ]
    }
   ],
   "source": [
    "# Definir BacktestConfig e BacktestEngine caso n√£o estejam dispon√≠veis via import local\n",
    "# NOTA: Esta √© uma implementa√ß√£o de fallback quando os imports do projeto n√£o est√£o dispon√≠veis\n",
    "# A implementa√ß√£o principal est√° em src/backtest/engine.py\n",
    "# Ambas as implementa√ß√µes mant√™m a mesma interface e l√≥gica de custos\n",
    "if not LOCAL_IMPORTS_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è Usando implementa√ß√£o local do BacktestEngine (imports do projeto n√£o dispon√≠veis)\")\n",
    "    \n",
    "    @dataclass\n",
    "    class BacktestConfig:\n",
    "        \"\"\"Configura√ß√£o para backtest - vers√£o local de fallback\"\"\"\n",
    "        initial_capital: float = 100000\n",
    "        fee_bps: float = 5\n",
    "        slippage_bps: float = 10\n",
    "        funding_apr_est: float = 0.00\n",
    "        borrow_apr_est: float = 0.00\n",
    "        execution_rule: str = 'next_bar_open'\n",
    "        max_leverage: float = 1.0\n",
    "        position_mode: str = 'long_short'\n",
    "        \n",
    "    class BacktestEngine:\n",
    "        \"\"\"Engine simplificado de backtest\"\"\"\n",
    "        def __init__(self, config: BacktestConfig):\n",
    "            self.config = config\n",
    "            \n",
    "        def run_backtest(self, df: pd.DataFrame, signals: pd.Series):\n",
    "            \"\"\"Executa backtest com PnL real\"\"\"\n",
    "            # Garantir alinhamento de √≠ndices\n",
    "            perf = pd.DataFrame(index=signals.index)\n",
    "            perf['signals'] = signals\n",
    "            perf['close'] = df.loc[signals.index, 'close']\n",
    "            perf['returns'] = perf['close'].pct_change()\n",
    "            \n",
    "            # Estrat√©gia: sinal em t, execu√ß√£o em t+1\n",
    "            perf['strategy_returns'] = perf['returns'] * perf['signals'].shift(1)\n",
    "            \n",
    "            # Aplicar custos\n",
    "            position_changes = signals.diff().abs()\n",
    "            costs = position_changes * (self.config.fee_bps + self.config.slippage_bps) / 10000\n",
    "            perf['net_returns'] = perf['strategy_returns'] - costs\n",
    "            \n",
    "            # Calcular trades com PnL real\n",
    "            trades = pd.DataFrame()\n",
    "            trade_signals = signals.diff()\n",
    "            entries = trade_signals != 0\n",
    "            \n",
    "            if entries.any():\n",
    "                entry_points = signals.index[entries]\n",
    "                trade_list = []\n",
    "                \n",
    "                for i, entry_time in enumerate(entry_points[:-1]):\n",
    "                    exit_time = entry_points[i+1]\n",
    "                    entry_idx = signals.index.get_loc(entry_time)\n",
    "                    exit_idx = signals.index.get_loc(exit_time)\n",
    "                    \n",
    "                    # PnL real baseado nos retornos\n",
    "                    trade_returns = perf['net_returns'].iloc[entry_idx+1:exit_idx+1]\n",
    "                    trade_pnl = (1 + trade_returns).prod() - 1\n",
    "                    \n",
    "                    trade_list.append({\n",
    "                        'entry_time': entry_time,\n",
    "                        'exit_time': exit_time,\n",
    "                        'pnl': trade_pnl * self.config.initial_capital\n",
    "                    })\n",
    "                \n",
    "                trades = pd.DataFrame(trade_list)\n",
    "            \n",
    "            return perf, trades\n",
    "\n",
    "def run_multi_horizon_backtest(df: pd.DataFrame,\n",
    "                              results: Dict,\n",
    "                              initial_capital: float = 100000,\n",
    "                              fee_bps: float = 5,\n",
    "                              slippage_bps: float = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Executa backtest para m√∫ltiplos horizontes e compara performance\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com OHLC\n",
    "        results: Resultados do pipeline multi-horizonte\n",
    "        initial_capital: Capital inicial\n",
    "        fee_bps: Taxa em basis points\n",
    "        slippage_bps: Slippage em basis points\n",
    "        \n",
    "    Returns:\n",
    "        Dict com resultados de backtest por horizonte\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä BACKTEST MULTI-HORIZONTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Usar import local ou classe definida acima\n",
    "    if LOCAL_IMPORTS_AVAILABLE:\n",
    "        from src.backtest.engine import BacktestEngine, BacktestConfig\n",
    "    \n",
    "    backtest_results = {}\n",
    "    \n",
    "    for horizon, horizon_results in results.items():\n",
    "        print(f\"\\n‚è±Ô∏è Backtesting horizonte: {horizon}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Configurar backtest\n",
    "        config = BacktestConfig(\n",
    "            initial_capital=initial_capital,\n",
    "            fee_bps=fee_bps,\n",
    "            slippage_bps=slippage_bps,\n",
    "            funding_apr_est=0.00,  # Simplificado para demo\n",
    "            execution_rule='next_bar_open'\n",
    "        )\n",
    "        \n",
    "        # Gerar sinais a partir das predi√ß√µes com √≠ndices corretos\n",
    "        predictions = horizon_results['predictions']['binary']\n",
    "        labels = horizon_results['labels']\n",
    "        \n",
    "        # Usar o √≠ndice do conjunto de teste ap√≥s as m√°scaras\n",
    "        # Isso garante alinhamento correto com os dados\n",
    "        test_indices = horizon_results.get('test_indices', None)\n",
    "        if test_indices is None:\n",
    "            # Fallback se n√£o tivermos os √≠ndices salvos\n",
    "            test_start_idx = len(df) - len(predictions)\n",
    "            test_indices = df.index[test_start_idx:test_start_idx + len(predictions)]\n",
    "        \n",
    "        signals = pd.Series(predictions * 2 - 1, index=test_indices)  # Converter 0/1 para -1/1\n",
    "        \n",
    "        # Executar backtest\n",
    "        bt_engine = BacktestEngine(config)\n",
    "        perf, trades = bt_engine.run_backtest(df.loc[signals.index], signals)\n",
    "        \n",
    "        # M√©tricas de trading - usar net_returns que inclui custos\n",
    "        returns = perf['net_returns'].dropna()\n",
    "        cumulative_return = (1 + returns).cumprod().iloc[-1] - 1 if len(returns) > 0 else 0\n",
    "        \n",
    "        # Sharpe Ratio\n",
    "        if returns.std() > 0:\n",
    "            sharpe = returns.mean() / returns.std() * np.sqrt(365 * 24 * 4)  # Anualizado para 15min\n",
    "        else:\n",
    "            sharpe = 0\n",
    "            \n",
    "        # Maximum Drawdown\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Calmar Ratio\n",
    "        calmar = cumulative_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
    "        \n",
    "        # Win rate\n",
    "        winning_trades = trades[trades['pnl'] > 0] if len(trades) > 0 else pd.DataFrame()\n",
    "        win_rate = len(winning_trades) / len(trades) if len(trades) > 0 else 0\n",
    "        \n",
    "        # Profit factor\n",
    "        gross_profit = trades[trades['pnl'] > 0]['pnl'].sum() if len(trades) > 0 else 0\n",
    "        gross_loss = abs(trades[trades['pnl'] < 0]['pnl'].sum()) if len(trades) > 0 else 0\n",
    "        profit_factor = gross_profit / gross_loss if gross_loss > 0 else 0\n",
    "        \n",
    "        # Turnover\n",
    "        position_changes = signals.diff().abs()\n",
    "        turnover = position_changes.sum() / len(signals)\n",
    "        \n",
    "        # Expected Value per trade\n",
    "        ev_per_trade = trades['pnl'].mean() if len(trades) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìà M√©tricas de Trading para {horizon}:\")\n",
    "        print(f\"  Retorno Total:    {cumulative_return:+.2%}\")\n",
    "        print(f\"  Sharpe Ratio:     {sharpe:.3f}\")\n",
    "        print(f\"  Max Drawdown:     {max_drawdown:.2%}\")\n",
    "        print(f\"  Calmar Ratio:     {calmar:.3f}\")\n",
    "        print(f\"  Win Rate:         {win_rate:.2%}\")\n",
    "        print(f\"  Profit Factor:    {profit_factor:.2f}\")\n",
    "        print(f\"  Turnover:         {turnover:.3f}\")\n",
    "        print(f\"  EV per Trade:     ${ev_per_trade:.2f}\")\n",
    "        print(f\"  Num Trades:       {len(trades)}\")\n",
    "        \n",
    "        # Comparar com Buy & Hold\n",
    "        buy_hold_return = (df.loc[signals.index, 'close'].iloc[-1] / \n",
    "                          df.loc[signals.index, 'close'].iloc[0] - 1)\n",
    "        outperformance = cumulative_return - buy_hold_return\n",
    "        \n",
    "        print(f\"\\n  Buy & Hold:       {buy_hold_return:+.2%}\")\n",
    "        print(f\"  Outperformance:   {outperformance:+.2%}\")\n",
    "        \n",
    "        # Salvar resultados\n",
    "        backtest_results[horizon] = {\n",
    "            'performance': perf,\n",
    "            'trades': trades,\n",
    "            'metrics': {\n",
    "                'cumulative_return': cumulative_return,\n",
    "                'sharpe_ratio': sharpe,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'calmar_ratio': calmar,\n",
    "                'win_rate': win_rate,\n",
    "                'profit_factor': profit_factor,\n",
    "                'turnover': turnover,\n",
    "                'ev_per_trade': ev_per_trade,\n",
    "                'num_trades': len(trades),\n",
    "                'buy_hold_return': buy_hold_return,\n",
    "                'outperformance': outperformance\n",
    "            },\n",
    "            'signals': signals,\n",
    "            'returns': returns\n",
    "        }\n",
    "    \n",
    "    # An√°lise comparativa\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üèÜ COMPARA√á√ÉO ENTRE HORIZONTES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_metrics = pd.DataFrame({\n",
    "        horizon: backtest_results[horizon]['metrics']\n",
    "        for horizon in backtest_results.keys()\n",
    "    }).T\n",
    "    \n",
    "    print(\"\\nüìä Tabela Comparativa de Backtest:\")\n",
    "    print(comparison_metrics.round(3))\n",
    "    \n",
    "    # Identificar melhor horizonte por diferentes m√©tricas\n",
    "    best_return = comparison_metrics['cumulative_return'].idxmax()\n",
    "    best_sharpe = comparison_metrics['sharpe_ratio'].idxmax()\n",
    "    best_calmar = comparison_metrics['calmar_ratio'].idxmax()\n",
    "    \n",
    "    print(f\"\\nü•á Melhores Horizontes:\")\n",
    "    print(f\"  Melhor Retorno: {best_return} ({comparison_metrics.loc[best_return, 'cumulative_return']:+.2%})\")\n",
    "    print(f\"  Melhor Sharpe:  {best_sharpe} ({comparison_metrics.loc[best_sharpe, 'sharpe_ratio']:.3f})\")\n",
    "    print(f\"  Melhor Calmar:  {best_calmar} ({comparison_metrics.loc[best_calmar, 'calmar_ratio']:.3f})\")\n",
    "    \n",
    "    # An√°lise de correla√ß√£o de retornos\n",
    "    print(f\"\\nüîó Correla√ß√£o entre retornos dos horizontes:\")\n",
    "    returns_df = pd.DataFrame({\n",
    "        horizon: backtest_results[horizon]['returns']\n",
    "        for horizon in backtest_results.keys()\n",
    "    })\n",
    "    \n",
    "    # Alinhar √≠ndices\n",
    "    returns_df = returns_df.dropna()\n",
    "    if len(returns_df) > 0:\n",
    "        corr_matrix = returns_df.corr()\n",
    "        print(corr_matrix.round(3))\n",
    "    \n",
    "    # Salvar resultados\n",
    "    os.makedirs(config.artifacts_path + \"/backtest\", exist_ok=True)\n",
    "    comparison_metrics.to_csv(f\"{config.artifacts_path}/backtest/horizon_backtest_comparison.csv\")\n",
    "    \n",
    "    # Plotar equity curves (opcional - salvando dados para visualiza√ß√£o posterior)\n",
    "    equity_curves = {}\n",
    "    for horizon in backtest_results.keys():\n",
    "        returns = backtest_results[horizon]['returns']\n",
    "        equity = (1 + returns).cumprod()\n",
    "        equity_curves[horizon] = equity\n",
    "    \n",
    "    equity_df = pd.DataFrame(equity_curves)\n",
    "    equity_df.to_csv(f\"{config.artifacts_path}/backtest/equity_curves.csv\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Resultados salvos em {config.artifacts_path}/backtest/\")\n",
    "    \n",
    "    return backtest_results\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o run_multi_horizon_backtest definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46742787",
   "metadata": {},
   "source": [
    "## 9. Estrat√©gia Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be5e8ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o create_ensemble_signals definida\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble_signals(results: Dict, \n",
    "                           weights: Dict = None,\n",
    "                           voting: str = 'soft') -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cria sinais ensemble combinando m√∫ltiplos horizontes\n",
    "    \n",
    "    Args:\n",
    "        results: Resultados do pipeline multi-horizonte\n",
    "        weights: Pesos para cada horizonte (None = igual peso)\n",
    "        voting: 'soft' (m√©dia ponderada) ou 'hard' (vota√ß√£o majorit√°ria)\n",
    "        \n",
    "    Returns:\n",
    "        Series com sinais combinados\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {h: 1.0/len(results) for h in results.keys()}\n",
    "    \n",
    "    # Coletar probabilidades calibradas\n",
    "    probabilities = {}\n",
    "    for horizon, horizon_results in results.items():\n",
    "        probs = horizon_results['predictions']['calibrated']\n",
    "        probabilities[horizon] = probs\n",
    "    \n",
    "    # Criar DataFrame alinhado\n",
    "    prob_df = pd.DataFrame(probabilities)\n",
    "    \n",
    "    if voting == 'soft':\n",
    "        # M√©dia ponderada das probabilidades\n",
    "        weighted_probs = sum(prob_df[h] * weights[h] for h in prob_df.columns)\n",
    "        # Aplicar threshold m√©dio dos horizontes\n",
    "        avg_threshold = np.mean([results[h]['threshold'] for h in results.keys()])\n",
    "        signals = (weighted_probs >= avg_threshold).astype(int) * 2 - 1\n",
    "    else:  # voting == 'hard'\n",
    "        # Vota√ß√£o majorit√°ria\n",
    "        binary_preds = pd.DataFrame({\n",
    "            h: (prob_df[h] >= results[h]['threshold']).astype(int)\n",
    "            for h in prob_df.columns\n",
    "        })\n",
    "        signals = (binary_preds.mean(axis=1) >= 0.5).astype(int) * 2 - 1\n",
    "    \n",
    "    return signals\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o create_ensemble_signals definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271d42e",
   "metadata": {},
   "source": [
    "## 10. Fun√ß√µes de Demonstra√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4e57ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√µes de demonstra√ß√£o definidas\n"
     ]
    }
   ],
   "source": [
    "def create_sample_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cria features b√°sicas para demonstra√ß√£o\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Returns\n",
    "    for period in [1, 5, 10, 20, 50]:\n",
    "        features[f'return_{period}'] = df['close'].pct_change(period)\n",
    "    \n",
    "    # Moving averages\n",
    "    for period in [10, 20, 50, 100]:\n",
    "        features[f'ma_{period}'] = df['close'].rolling(period).mean() / df['close'] - 1\n",
    "    \n",
    "    # Volume\n",
    "    features['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    features['volume_ma_20'] = df['volume'].rolling(20).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility_20'] = df['close'].pct_change().rolling(20).std()\n",
    "    features['high_low_ratio'] = df['high'] / df['low'] - 1\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    ma_20 = df['close'].rolling(20).mean()\n",
    "    std_20 = df['close'].rolling(20).std()\n",
    "    features['bb_upper'] = (ma_20 + 2 * std_20) / df['close'] - 1\n",
    "    features['bb_lower'] = (ma_20 - 2 * std_20) / df['close'] - 1\n",
    "    features['bb_width'] = features['bb_upper'] - features['bb_lower']\n",
    "    \n",
    "    # Price position\n",
    "    features['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_sample_data(n_samples: int = 10000, freq: str = '15min') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gera dados OHLCV sint√©ticos para teste\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range('2023-01-01', periods=n_samples, freq=freq)\n",
    "    \n",
    "    # Simular pre√ßo com tend√™ncia e volatilidade\n",
    "    returns = np.random.randn(n_samples) * 0.01  # 1% vol\n",
    "    price = 100 * np.exp(returns.cumsum())\n",
    "    \n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['close'] = price\n",
    "    \n",
    "    # Gerar OHLV a partir do close\n",
    "    df['open'] = df['close'] * (1 + np.random.randn(n_samples) * 0.001)\n",
    "    df['high'] = df[['open', 'close']].max(axis=1) * (1 + np.abs(np.random.randn(n_samples)) * 0.002)\n",
    "    df['low'] = df[['open', 'close']].min(axis=1) * (1 - np.abs(np.random.randn(n_samples)) * 0.002)\n",
    "    df['volume'] = np.random.exponential(1000, n_samples) * (1 + np.abs(returns) * 10)\n",
    "    \n",
    "    # Garantir consist√™ncia OHLC\n",
    "    df['high'] = df[['open', 'high', 'close']].max(axis=1)\n",
    "    df['low'] = df[['open', 'low', 'close']].min(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def demo_multi_horizon_pipeline():\n",
    "    \"\"\"\n",
    "    Demonstra√ß√£o completa do pipeline multi-horizonte\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ DEMONSTRA√á√ÉO DO PIPELINE MULTI-HORIZONTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Gerar dados sint√©ticos\n",
    "    print(\"\\nüìä Gerando dados sint√©ticos...\")\n",
    "    df = generate_sample_data(n_samples=10000)\n",
    "    print(f\"  Dados gerados: {len(df)} barras de 15min\")\n",
    "    print(f\"  Per√≠odo: {df.index[0]} a {df.index[-1]}\")\n",
    "    \n",
    "    # 2. Criar features\n",
    "    print(\"\\nüîß Criando features...\")\n",
    "    features = create_sample_features(df)\n",
    "    \n",
    "    # Adicionar features de calend√°rio\n",
    "    crypto_features = Crypto24x7Features()\n",
    "    features = pd.concat([\n",
    "        features,\n",
    "        crypto_features.create_calendar_features(df),\n",
    "        crypto_features.create_session_features(df)\n",
    "    ], axis=1)\n",
    "    \n",
    "    print(f\"  Features criadas: {len(features.columns)}\")\n",
    "    print(f\"  Features: {', '.join(features.columns[:10])}...\")\n",
    "    \n",
    "    # 3. Executar pipeline multi-horizonte\n",
    "    print(\"\\nüéØ Executando pipeline multi-horizonte...\")\n",
    "    results = run_multi_horizon_pipeline(\n",
    "        df=df,\n",
    "        features=features,\n",
    "        horizons=['15m', '30m', '60m', '120m'],\n",
    "        n_trials=10  # Reduzido para demo\n",
    "    )\n",
    "    \n",
    "    # 4. Executar backtest\n",
    "    print(\"\\nüìä Executando backtest multi-horizonte...\")\n",
    "    backtest_results = run_multi_horizon_backtest(df, results)\n",
    "    \n",
    "    # 5. Criar sinais ensemble\n",
    "    print(\"\\nüéØ Criando estrat√©gia ensemble...\")\n",
    "    ensemble_signals = create_ensemble_signals(results, voting='soft')\n",
    "    print(f\"  Sinais ensemble criados: {len(ensemble_signals)}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Demonstra√ß√£o conclu√≠da!\")\n",
    "    \n",
    "    return results, backtest_results\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de demonstra√ß√£o definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9474a",
   "metadata": {},
   "source": [
    "## 11. Como Usar o Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f8a1ed1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                        GUIA DE USO DO PIPELINE                              ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "1. CONFIGURA√á√ÉO INICIAL:\n",
      "   ```python\n",
      "   # Configurar ambiente determin√≠stico\n",
      "   setup_deterministic_environment(seed=42)\n",
      "\n",
      "   # Configurar projeto\n",
      "   config = ProjectConfig()\n",
      "   config.create_directories()\n",
      "   ```\n",
      "\n",
      "2. CARREGAR SEUS DADOS:\n",
      "   ```python\n",
      "   # Op√ß√£o 1: Dados locais\n",
      "   df = pd.read_csv('seu_arquivo.csv', index_col='timestamp', parse_dates=True)\n",
      "\n",
      "   # Op√ß√£o 2: API (se dispon√≠vel)\n",
      "   from src.data.binance_loader import CryptoDataLoader\n",
      "   loader = CryptoDataLoader()\n",
      "   df = loader.fetch_ohlcv('BTCUSDT', '15m', limit=10000)\n",
      "   ```\n",
      "\n",
      "3. CRIAR FEATURES:\n",
      "   ```python\n",
      "   # Features b√°sicas\n",
      "   features = create_sample_features(df)\n",
      "\n",
      "   # Adicionar features crypto 24/7\n",
      "   crypto_features = Crypto24x7Features()\n",
      "   # Definir per√≠odo de funding baseado no contrato (480 min para maioria, 60 min para alguns)\n",
      "   funding_period = 480  # Ajustar conforme s√≠mbolo/exchange\n",
      "   features = pd.concat([\n",
      "       features,\n",
      "       crypto_features.create_calendar_features(df),\n",
      "       crypto_features.create_session_features(df),\n",
      "       crypto_features.create_funding_features(df, funding_period_minutes=funding_period)\n",
      "   ], axis=1)\n",
      "   ```\n",
      "\n",
      "4. TREINAR MODELOS MULTI-HORIZONTE:\n",
      "   ```python\n",
      "   results = run_multi_horizon_pipeline(\n",
      "       df=df,\n",
      "       features=features,\n",
      "       horizons=['15m', '30m', '60m', '120m'],\n",
      "       test_size=0.2,\n",
      "       val_size=0.2,\n",
      "       n_trials=50  # Aumentar para produ√ß√£o\n",
      "   )\n",
      "   ```\n",
      "\n",
      "5. EXECUTAR BACKTEST:\n",
      "   ```python\n",
      "   backtest_results = run_multi_horizon_backtest(\n",
      "       df=df,\n",
      "       results=results,\n",
      "       initial_capital=100000,\n",
      "       fee_bps=5,\n",
      "       slippage_bps=10\n",
      "   )\n",
      "   ```\n",
      "\n",
      "6. CRIAR ESTRAT√âGIA ENSEMBLE:\n",
      "   ```python\n",
      "   # Combinar sinais de m√∫ltiplos horizontes\n",
      "   ensemble_signals = create_ensemble_signals(\n",
      "       results,\n",
      "       weights={'15m': 0.2, '30m': 0.3, '60m': 0.3, '120m': 0.2},\n",
      "       voting='soft'\n",
      "   )\n",
      "   ```\n",
      "\n",
      "7. DEMO R√ÅPIDA:\n",
      "   ```python\n",
      "   # Executar demonstra√ß√£o completa com dados sint√©ticos\n",
      "   results, backtest_results = demo_multi_horizon_pipeline()\n",
      "   ```\n",
      "\n",
      "NOTAS IMPORTANTES:\n",
      "- Sempre use dados de 15 minutos como base\n",
      "- Horizontes s√£o m√∫ltiplos de 15min (15m, 30m, 60m, 120m)\n",
      "- PR-AUC √© a m√©trica principal (n√£o ROC-AUC)\n",
      "- Calibra√ß√£o de probabilidades √© obrigat√≥ria\n",
      "- Backtest usa execu√ß√£o t+1 (sinal em t, execu√ß√£o em t+1)\n",
      "- Custos incluem fees e slippage\n",
      "\n",
      "Para mais informa√ß√µes, consulte a documenta√ß√£o em docs/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                        GUIA DE USO DO PIPELINE                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "1. CONFIGURA√á√ÉO INICIAL:\n",
    "   ```python\n",
    "   # Configurar ambiente determin√≠stico\n",
    "   setup_deterministic_environment(seed=42)\n",
    "   \n",
    "   # Configurar projeto\n",
    "   config = ProjectConfig()\n",
    "   config.create_directories()\n",
    "   ```\n",
    "\n",
    "2. CARREGAR SEUS DADOS:\n",
    "   ```python\n",
    "   # Op√ß√£o 1: Dados locais\n",
    "   df = pd.read_csv('seu_arquivo.csv', index_col='timestamp', parse_dates=True)\n",
    "   \n",
    "   # Op√ß√£o 2: API (se dispon√≠vel)\n",
    "   from src.data.binance_loader import CryptoDataLoader\n",
    "   loader = CryptoDataLoader()\n",
    "   df = loader.fetch_ohlcv('BTCUSDT', '15m', limit=10000)\n",
    "   ```\n",
    "\n",
    "3. CRIAR FEATURES:\n",
    "   ```python\n",
    "   # Features b√°sicas\n",
    "   features = create_sample_features(df)\n",
    "   \n",
    "   # Adicionar features crypto 24/7\n",
    "   crypto_features = Crypto24x7Features()\n",
    "   # Definir per√≠odo de funding baseado no contrato (480 min para maioria, 60 min para alguns)\n",
    "   funding_period = 480  # Ajustar conforme s√≠mbolo/exchange\n",
    "   features = pd.concat([\n",
    "       features,\n",
    "       crypto_features.create_calendar_features(df),\n",
    "       crypto_features.create_session_features(df),\n",
    "       crypto_features.create_funding_features(df, funding_period_minutes=funding_period)\n",
    "   ], axis=1)\n",
    "   ```\n",
    "\n",
    "4. TREINAR MODELOS MULTI-HORIZONTE:\n",
    "   ```python\n",
    "   results = run_multi_horizon_pipeline(\n",
    "       df=df,\n",
    "       features=features,\n",
    "       horizons=['15m', '30m', '60m', '120m'],\n",
    "       test_size=0.2,\n",
    "       val_size=0.2,\n",
    "       n_trials=50  # Aumentar para produ√ß√£o\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. EXECUTAR BACKTEST:\n",
    "   ```python\n",
    "   backtest_results = run_multi_horizon_backtest(\n",
    "       df=df,\n",
    "       results=results,\n",
    "       initial_capital=100000,\n",
    "       fee_bps=5,\n",
    "       slippage_bps=10\n",
    "   )\n",
    "   ```\n",
    "\n",
    "6. CRIAR ESTRAT√âGIA ENSEMBLE:\n",
    "   ```python\n",
    "   # Combinar sinais de m√∫ltiplos horizontes\n",
    "   ensemble_signals = create_ensemble_signals(\n",
    "       results,\n",
    "       weights={'15m': 0.2, '30m': 0.3, '60m': 0.3, '120m': 0.2},\n",
    "       voting='soft'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "7. DEMO R√ÅPIDA:\n",
    "   ```python\n",
    "   # Executar demonstra√ß√£o completa com dados sint√©ticos\n",
    "   results, backtest_results = demo_multi_horizon_pipeline()\n",
    "   ```\n",
    "\n",
    "NOTAS IMPORTANTES:\n",
    "- Sempre use dados de 15 minutos como base\n",
    "- Horizontes s√£o m√∫ltiplos de 15min (15m, 30m, 60m, 120m)\n",
    "- PR-AUC √© a m√©trica principal (n√£o ROC-AUC)\n",
    "- Calibra√ß√£o de probabilidades √© obrigat√≥ria\n",
    "- Backtest usa execu√ß√£o t+1 (sinal em t, execu√ß√£o em t+1)\n",
    "- Custos incluem fees e slippage\n",
    "\n",
    "Para mais informa√ß√µes, consulte a documenta√ß√£o em docs/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709541f",
   "metadata": {},
   "source": [
    "## 12. Executar Demonstra√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d74b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ DEMONSTRA√á√ÉO DO PIPELINE MULTI-HORIZONTE\n",
      "================================================================================\n",
      "\n",
      "üìä Gerando dados sint√©ticos...\n",
      "  Dados gerados: 10000 barras de 15min\n",
      "  Per√≠odo: 2023-01-01 00:00:00 a 2023-04-15 03:45:00\n",
      "\n",
      "üîß Criando features...\n",
      "  Features criadas: 41\n",
      "  Features: return_1, return_5, return_10, return_20, return_50, ma_10, ma_20, ma_50, ma_100, volume_ratio...\n",
      "\n",
      "üéØ Executando pipeline multi-horizonte...\n",
      "================================================================================\n",
      "üöÄ INICIANDO PIPELINE MULTI-HORIZONTE\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Optuna n√£o est√° dispon√≠vel",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Para executar a demonstra√ß√£o, descomente a linha abaixo:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results, backtest_results = \u001b[43mdemo_multi_horizon_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mdemo_multi_horizon_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# 3. Executar pipeline multi-horizonte\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ Executando pipeline multi-horizonte...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m results = \u001b[43mrun_multi_horizon_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m15m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m30m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m60m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m120m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduzido para demo\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# 4. Executar backtest\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Executando backtest multi-horizonte...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_multi_horizon_pipeline\u001b[39m\u001b[34m(df, features, horizons, test_size, val_size, n_trials, k_range)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mXGBoost n√£o est√° dispon√≠vel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OPTUNA_AVAILABLE:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOptuna n√£o est√° dispon√≠vel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MLFLOW_AVAILABLE:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è MLflow n√£o dispon√≠vel - resultados n√£o ser√£o tracked\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Optuna n√£o est√° dispon√≠vel"
     ]
    }
   ],
   "source": [
    "# Para executar a demonstra√ß√£o, descomente a linha abaixo:\n",
    "# results, backtest_results = demo_multi_horizon_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0f653",
   "metadata": {},
   "source": [
    "## 13. Su√≠te Completa de Testes - Valida√ß√£o de Requisitos PRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTestSuite:\n",
    "    \"\"\"\n",
    "    Su√≠te completa de testes para validar todos os requisitos dos PRDs\n",
    "    Inclui testes de integridade, performance e requisitos econ√¥micos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProjectConfig = None):\n",
    "        self.config = config or ProjectConfig()\n",
    "        self.test_results = {}\n",
    "        \n",
    "    def test_temporal_leakage(self, df: pd.DataFrame, features: pd.DataFrame, \n",
    "                              labels: pd.Series) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de vazamento temporal - CR√çTICO\n",
    "        Requisito PRD: Sem vazamento temporal comprovado\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando vazamento temporal...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        # Verificar se features usam informa√ß√£o futura\n",
    "        for col in features.columns:\n",
    "            if 'future' in col.lower() or 'next' in col.lower():\n",
    "                passed = False\n",
    "                metrics[f'feature_{col}'] = 'SUSPEITA: nome sugere informa√ß√£o futura'\n",
    "        \n",
    "        # Verificar correla√ß√£o com retornos futuros n√£o shiftados\n",
    "        future_returns = df['close'].pct_change().shift(-1)  # Retorno futuro\n",
    "        \n",
    "        for col in features.columns:\n",
    "            if features[col].notna().sum() > 100:  # S√≥ testar se tiver dados suficientes\n",
    "                corr = features[col].corr(future_returns)\n",
    "                if abs(corr) > 0.95:  # Correla√ß√£o muito alta √© suspeita\n",
    "                    passed = False\n",
    "                    metrics[f'correlation_{col}'] = f'ALTA: {corr:.3f}'\n",
    "        \n",
    "        # Verificar alinhamento temporal de labels\n",
    "        label_returns = labels.shift(-1)  # Labels devem estar no futuro\n",
    "        label_feature_corr = labels.corr(features.mean(axis=1))\n",
    "        \n",
    "        if abs(label_feature_corr) > 0.8:\n",
    "            passed = False\n",
    "            metrics['label_alignment'] = f'PROBLEMA: correla√ß√£o {label_feature_corr:.3f}'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_data_quality(self, df: pd.DataFrame, features: pd.DataFrame) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de qualidade de dados\n",
    "        Requisito PRD: Dados limpos e consistentes\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando qualidade de dados...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        # Verificar NaN\n",
    "        nan_ratio = features.isna().sum().sum() / (len(features) * len(features.columns))\n",
    "        metrics['nan_ratio'] = nan_ratio\n",
    "        if nan_ratio > 0.1:  # Mais de 10% NaN √© problem√°tico\n",
    "            passed = False\n",
    "        \n",
    "        # Verificar consist√™ncia OHLC\n",
    "        ohlc_errors = 0\n",
    "        ohlc_errors += (df['high'] < df['low']).sum()\n",
    "        ohlc_errors += (df['high'] < df['open']).sum()\n",
    "        ohlc_errors += (df['high'] < df['close']).sum()\n",
    "        ohlc_errors += (df['low'] > df['open']).sum()\n",
    "        ohlc_errors += (df['low'] > df['close']).sum()\n",
    "        \n",
    "        metrics['ohlc_errors'] = ohlc_errors\n",
    "        if ohlc_errors > 0:\n",
    "            passed = False\n",
    "        \n",
    "        # Verificar outliers extremos (> 10 desvios padr√£o)\n",
    "        outliers = 0\n",
    "        for col in features.select_dtypes(include=[np.number]).columns:\n",
    "            z_scores = np.abs(stats.zscore(features[col].dropna()))\n",
    "            outliers += (z_scores > 10).sum()\n",
    "        \n",
    "        metrics['extreme_outliers'] = outliers\n",
    "        if outliers > len(features) * 0.01:  # Mais de 1% outliers extremos\n",
    "            passed = False\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_model_performance(self, results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de performance do modelo\n",
    "        Requisito PRD: PR-AUC acima do baseline, F1 adequado\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando performance dos modelos...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        baseline_pr_auc = 0.5  # Baseline aleat√≥rio\n",
    "        min_acceptable_f1 = 0.3  # M√≠nimo aceit√°vel para crypto\n",
    "        \n",
    "        for horizon, result in results.items():\n",
    "            horizon_metrics = result['metrics']\n",
    "            \n",
    "            # PR-AUC deve ser melhor que baseline\n",
    "            pr_auc = horizon_metrics['pr_auc']\n",
    "            metrics[f'{horizon}_pr_auc'] = pr_auc\n",
    "            if pr_auc <= baseline_pr_auc:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_pr_auc_status'] = 'FAIL: abaixo do baseline'\n",
    "            \n",
    "            # F1 score m√≠nimo\n",
    "            f1 = horizon_metrics['f1']\n",
    "            metrics[f'{horizon}_f1'] = f1\n",
    "            if f1 < min_acceptable_f1:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_f1_status'] = 'FAIL: F1 muito baixo'\n",
    "            \n",
    "            # MCC (Matthews Correlation Coefficient)\n",
    "            mcc = horizon_metrics['mcc']\n",
    "            metrics[f'{horizon}_mcc'] = mcc\n",
    "            if mcc < 0:  # MCC negativo indica pior que aleat√≥rio\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_mcc_status'] = 'FAIL: MCC negativo'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_calibration(self, results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de calibra√ß√£o de probabilidades\n",
    "        Requisito PRD: Calibra√ß√£o dentro de ¬±2 p.p.\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando calibra√ß√£o de probabilidades...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        for horizon, result in results.items():\n",
    "            predictions = result['predictions']['calibrated']\n",
    "            labels = result['labels']\n",
    "            \n",
    "            # Calcular ECE (Expected Calibration Error)\n",
    "            n_bins = 10\n",
    "            bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "            bin_lowers = bin_boundaries[:-1]\n",
    "            bin_uppers = bin_boundaries[1:]\n",
    "            \n",
    "            ece = 0\n",
    "            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "                in_bin = (predictions > bin_lower) & (predictions <= bin_upper)\n",
    "                prop_in_bin = in_bin.mean()\n",
    "                \n",
    "                if prop_in_bin > 0:\n",
    "                    accuracy_in_bin = labels[in_bin].mean()\n",
    "                    avg_confidence_in_bin = predictions[in_bin].mean()\n",
    "                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "            \n",
    "            metrics[f'{horizon}_ece'] = ece\n",
    "            \n",
    "            # ECE deve ser < 0.02 (2%)\n",
    "            if ece > 0.02:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_ece_status'] = f'FAIL: ECE {ece:.3f} > 0.02'\n",
    "            \n",
    "            # Brier Score (menor √© melhor)\n",
    "            brier = brier_score_loss(labels, predictions)\n",
    "            metrics[f'{horizon}_brier'] = brier\n",
    "            \n",
    "            if brier > 0.25:  # Brier > 0.25 indica m√° calibra√ß√£o\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_brier_status'] = f'FAIL: Brier {brier:.3f} > 0.25'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_economic_metrics(self, backtest_results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de m√©tricas econ√¥micas\n",
    "        Requisito PRD: Sharpe > 1.0, DSR > 0.8\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando m√©tricas econ√¥micas...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        min_sharpe = 1.0  # Requisito PRD\n",
    "        min_dsr = 0.8     # Requisito PRD\n",
    "        max_acceptable_drawdown = 0.25  # Max 25% drawdown\n",
    "        \n",
    "        for horizon, result in backtest_results.items():\n",
    "            horizon_metrics = result['metrics']\n",
    "            \n",
    "            # Sharpe Ratio\n",
    "            sharpe = horizon_metrics['sharpe_ratio']\n",
    "            metrics[f'{horizon}_sharpe'] = sharpe\n",
    "            if sharpe < min_sharpe:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_sharpe_status'] = f'FAIL: Sharpe {sharpe:.2f} < {min_sharpe}'\n",
    "            \n",
    "            # Maximum Drawdown\n",
    "            mdd = abs(horizon_metrics['max_drawdown'])\n",
    "            metrics[f'{horizon}_max_drawdown'] = mdd\n",
    "            if mdd > max_acceptable_drawdown:\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_mdd_status'] = f'FAIL: MDD {mdd:.2%} > {max_acceptable_drawdown:.0%}'\n",
    "            \n",
    "            # Calmar Ratio\n",
    "            calmar = horizon_metrics['calmar_ratio']\n",
    "            metrics[f'{horizon}_calmar'] = calmar\n",
    "            \n",
    "            # Win Rate\n",
    "            win_rate = horizon_metrics['win_rate']\n",
    "            metrics[f'{horizon}_win_rate'] = win_rate\n",
    "            if win_rate < 0.45:  # Win rate muito baixo\n",
    "                metrics[f'{horizon}_win_rate_status'] = f'WARNING: Win rate {win_rate:.1%} baixo'\n",
    "            \n",
    "            # Calcular DSR (Deflated Sharpe Ratio) simplificado\n",
    "            # DSR = Sharpe * sqrt(T) / sqrt(1 + skew^2/4 + (kurt-3)^2/24)\n",
    "            returns = result.get('returns', pd.Series([0]))\n",
    "            if len(returns) > 30:\n",
    "                skew = returns.skew()\n",
    "                kurt = returns.kurt()\n",
    "                T = len(returns) / (365 * 24 * 4)  # Anos de dados\n",
    "                dsr = sharpe * np.sqrt(T) / np.sqrt(1 + skew**2/4 + (kurt-3)**2/24)\n",
    "                metrics[f'{horizon}_dsr'] = dsr\n",
    "                \n",
    "                if dsr < min_dsr:\n",
    "                    passed = False\n",
    "                    metrics[f'{horizon}_dsr_status'] = f'FAIL: DSR {dsr:.2f} < {min_dsr}'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_feature_importance_stability(self, results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de estabilidade das feature importances\n",
    "        Requisito PRD: Feature importances consistentes\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando estabilidade de feature importance...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        # Coletar top features de cada horizonte\n",
    "        top_features_by_horizon = {}\n",
    "        for horizon, result in results.items():\n",
    "            top_10 = result['feature_importance'].head(10)['feature'].tolist()\n",
    "            top_features_by_horizon[horizon] = set(top_10)\n",
    "        \n",
    "        # Verificar overlap entre horizontes\n",
    "        horizons = list(results.keys())\n",
    "        for i in range(len(horizons)-1):\n",
    "            h1, h2 = horizons[i], horizons[i+1]\n",
    "            overlap = len(top_features_by_horizon[h1] & top_features_by_horizon[h2])\n",
    "            overlap_ratio = overlap / 10\n",
    "            \n",
    "            metrics[f'overlap_{h1}_{h2}'] = overlap_ratio\n",
    "            \n",
    "            if overlap_ratio < 0.3:  # Menos de 30% de overlap √© suspeito\n",
    "                passed = False\n",
    "                metrics[f'overlap_{h1}_{h2}_status'] = f'FAIL: apenas {overlap_ratio:.1%} overlap'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def test_execution_realism(self, backtest_results: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        Teste de realismo da execu√ß√£o\n",
    "        Requisito: Execu√ß√£o t+1, custos aplicados\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Testando realismo da execu√ß√£o...\")\n",
    "        \n",
    "        passed = True\n",
    "        metrics = {}\n",
    "        \n",
    "        for horizon, result in backtest_results.items():\n",
    "            # Verificar turnover\n",
    "            turnover = result['metrics'].get('turnover', 0)\n",
    "            metrics[f'{horizon}_turnover'] = turnover\n",
    "            \n",
    "            if turnover > 10:  # Turnover muito alto √© irrealista\n",
    "                passed = False\n",
    "                metrics[f'{horizon}_turnover_status'] = f'FAIL: turnover {turnover:.1f} muito alto'\n",
    "            \n",
    "            # Verificar se custos foram aplicados\n",
    "            if 'outperformance' in result['metrics']:\n",
    "                outperf = result['metrics']['outperformance']\n",
    "                if outperf > 0.5:  # Outperformance > 50% √© suspeito\n",
    "                    metrics[f'{horizon}_outperf_warning'] = f'WARNING: outperformance {outperf:.1%} muito alto'\n",
    "        \n",
    "        metrics['status'] = 'PASS' if passed else 'FAIL'\n",
    "        print(f\"  Resultado: {'‚úÖ PASS' if passed else '‚ùå FAIL'}\")\n",
    "        \n",
    "        return passed, metrics\n",
    "    \n",
    "    def run_all_tests(self, df: pd.DataFrame, features: pd.DataFrame, \n",
    "                      results: Dict, backtest_results: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Executa todos os testes e gera relat√≥rio completo\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üß™ EXECUTANDO SU√çTE COMPLETA DE TESTES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Preparar labels para teste (usar do primeiro horizonte)\n",
    "        first_horizon = list(results.keys())[0]\n",
    "        labeler = results[first_horizon]['labeler']\n",
    "        labels = labeler.create_labels(df)\n",
    "        \n",
    "        # 1. Teste de vazamento temporal\n",
    "        passed, metrics = self.test_temporal_leakage(df, features, labels)\n",
    "        all_results['temporal_leakage'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 2. Teste de qualidade de dados\n",
    "        passed, metrics = self.test_data_quality(df, features)\n",
    "        all_results['data_quality'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 3. Teste de performance do modelo\n",
    "        passed, metrics = self.test_model_performance(results)\n",
    "        all_results['model_performance'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 4. Teste de calibra√ß√£o\n",
    "        passed, metrics = self.test_calibration(results)\n",
    "        all_results['calibration'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 5. Teste de m√©tricas econ√¥micas\n",
    "        passed, metrics = self.test_economic_metrics(backtest_results)\n",
    "        all_results['economic_metrics'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 6. Teste de estabilidade de features\n",
    "        passed, metrics = self.test_feature_importance_stability(results)\n",
    "        all_results['feature_stability'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # 7. Teste de realismo de execu√ß√£o\n",
    "        passed, metrics = self.test_execution_realism(backtest_results)\n",
    "        all_results['execution_realism'] = {'passed': passed, 'metrics': metrics}\n",
    "        \n",
    "        # Resumo final\n",
    "        self.print_test_summary(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def print_test_summary(self, results: Dict):\n",
    "        \"\"\"\n",
    "        Imprime resumo dos testes\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä RESUMO DOS TESTES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        total_tests = len(results)\n",
    "        passed_tests = sum(1 for r in results.values() if r['passed'])\n",
    "        \n",
    "        print(f\"\\nTotal de testes: {total_tests}\")\n",
    "        print(f\"Testes aprovados: {passed_tests}\")\n",
    "        print(f\"Taxa de aprova√ß√£o: {passed_tests/total_tests:.1%}\")\n",
    "        \n",
    "        print(\"\\nüìã Detalhes por teste:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for test_name, result in results.items():\n",
    "            status = \"‚úÖ PASS\" if result['passed'] else \"‚ùå FAIL\"\n",
    "            print(f\"{test_name:25s}: {status}\")\n",
    "            \n",
    "            # Mostrar m√©tricas cr√≠ticas se falhou\n",
    "            if not result['passed']:\n",
    "                for key, value in result['metrics'].items():\n",
    "                    if 'FAIL' in str(value) or 'status' in key:\n",
    "                        print(f\"  ‚îî‚îÄ {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Verifica√ß√£o dos requisitos PRD\n",
    "        print(\"\\nüéØ REQUISITOS PRD:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Requisitos cr√≠ticos\n",
    "        requirements = {\n",
    "            'Sem vazamento temporal': results['temporal_leakage']['passed'],\n",
    "            'PR-AUC acima do baseline': 'model_performance' in results and results['model_performance']['passed'],\n",
    "            'Sharpe > 1.0': False,  # Verificar nas m√©tricas\n",
    "            'DSR > 0.8': False,  # Verificar nas m√©tricas\n",
    "            'Calibra√ß√£o < 2%': results.get('calibration', {}).get('passed', False),\n",
    "            'Features est√°veis': results.get('feature_stability', {}).get('passed', False)\n",
    "        }\n",
    "        \n",
    "        # Verificar Sharpe e DSR\n",
    "        if 'economic_metrics' in results:\n",
    "            metrics = results['economic_metrics']['metrics']\n",
    "            # Verificar se algum horizonte passou no Sharpe\n",
    "            sharpe_passed = any(\n",
    "                metrics.get(f'{h}_sharpe', 0) >= 1.0 \n",
    "                for h in ['15m', '30m', '60m', '120m']\n",
    "            )\n",
    "            requirements['Sharpe > 1.0'] = sharpe_passed\n",
    "            \n",
    "            # Verificar DSR\n",
    "            dsr_passed = any(\n",
    "                metrics.get(f'{h}_dsr', 0) >= 0.8\n",
    "                for h in ['15m', '30m', '60m', '120m']\n",
    "                if f'{h}_dsr' in metrics\n",
    "            )\n",
    "            requirements['DSR > 0.8'] = dsr_passed\n",
    "        \n",
    "        for req, passed in requirements.items():\n",
    "            status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "            print(f\"{status} {req}\")\n",
    "        \n",
    "        # Conclus√£o\n",
    "        all_requirements_met = all(requirements.values())\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        if all_requirements_met:\n",
    "            print(\"üéâ TODOS OS REQUISITOS PRD FORAM ATENDIDOS!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è ALGUNS REQUISITOS PRD N√ÉO FORAM ATENDIDOS\")\n",
    "            print(\"   Revise os testes falhados e ajuste o modelo\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Fun√ß√£o para executar os testes\n",
    "def run_model_tests(df: pd.DataFrame = None, features: pd.DataFrame = None,\n",
    "                    results: Dict = None, backtest_results: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Executa a su√≠te completa de testes\n",
    "    \n",
    "    Se n√£o fornecer dados, executa com dados de demo\n",
    "    \"\"\"\n",
    "    if df is None or features is None or results is None:\n",
    "        print(\"Gerando dados de demonstra√ß√£o para testes...\")\n",
    "        df = generate_sample_data(10000)\n",
    "        features = create_sample_features(df)\n",
    "        \n",
    "        # Adicionar features crypto\n",
    "        crypto_features = Crypto24x7Features()\n",
    "        features = pd.concat([\n",
    "            features,\n",
    "            crypto_features.create_calendar_features(df),\n",
    "            crypto_features.create_session_features(df)\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Executar pipeline\n",
    "        print(\"Executando pipeline para gerar resultados...\")\n",
    "        results = run_multi_horizon_pipeline(\n",
    "            df, features, \n",
    "            horizons=['15m', '30m'],  # Menos horizontes para teste r√°pido\n",
    "            n_trials=5  # Poucos trials para teste\n",
    "        )\n",
    "        \n",
    "        # Executar backtest\n",
    "        print(\"Executando backtest...\")\n",
    "        backtest_results = run_multi_horizon_backtest(df, results)\n",
    "    \n",
    "    # Executar testes\n",
    "    test_suite = ModelTestSuite()\n",
    "    test_results = test_suite.run_all_tests(df, features, results, backtest_results)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "\n",
    "# Para executar os testes:\n",
    "# test_results = run_model_tests()\n",
    "\n",
    "print(\"‚úÖ Su√≠te de testes definida\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
