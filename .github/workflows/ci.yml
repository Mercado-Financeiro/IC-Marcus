name: CI Pipeline

on:
  push:
    branches: [main, develop, 'feature/**']
  pull_request:
    branches: [main, develop]
  schedule:
    # Run daily at 2 AM UTC for dependency checks
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION_DEFAULT: '3.11'
  MLFLOW_TRACKING_URI: 'file://./artifacts/mlruns'
  PYTHONHASHSEED: 0
  CUBLAS_WORKSPACE_CONFIG: ':4096:8'
  CUDA_LAUNCH_BLOCKING: 1

jobs:
  # Code Quality Checks
  lint-and-format:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff black mypy bandit[toml] safety
      
      - name: Run Ruff linter
        run: ruff check src tests --output-format=github
      
      - name: Check Black formatting
        run: black --check src tests
      
      - name: Run MyPy type checking
        run: mypy src --ignore-missing-imports
        continue-on-error: true  # Don't fail build on type errors initially

  # Security Checks
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit[toml] safety pip-audit
      
      - name: Run Bandit security linter
        run: bandit -r src -f json -o bandit-report.json || true
        
      - name: Upload Bandit results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json
      
      - name: Run Safety check
        run: safety check --json --output safety-report.json || true
        continue-on-error: true
        
      - name: Run pip-audit
        run: |
          pip install -r requirements.txt
          pip-audit --desc --fix --dry-run
        continue-on-error: true
      
      - name: Check for secrets with Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Unit Tests
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist hypothesis
      
      - name: Create test directories
        run: |
          mkdir -p artifacts/models artifacts/reports artifacts/mlruns
          mkdir -p data/raw data/processed data/cache
      
      - name: Run unit tests with coverage
        run: |
          pytest tests/unit -v \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --junitxml=junit/test-results-${{ matrix.python-version }}.xml \
            -n auto
        env:
          PYTHONPATH: ${{ github.workspace }}
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: Python ${{ matrix.python-version }}
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: pytest-results-${{ matrix.python-version }}
          path: junit/test-results-${{ matrix.python-version }}.xml

  # Integration Tests
  integration-test:
    needs: [lint-and-format, security]
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-timeout
      
      - name: Create required directories
        run: |
          mkdir -p artifacts/models artifacts/reports artifacts/mlruns
          mkdir -p data/raw data/processed data/cache
      
      - name: Run integration tests
        run: |
          pytest tests/integration -v --timeout=300
        env:
          PYTHONPATH: ${{ github.workspace }}
        continue-on-error: true

  # ML Model Validation
  ml-validation:
    needs: [test]
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-ml-${{ hashFiles('requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create required directories
        run: |
          mkdir -p artifacts/models artifacts/reports artifacts/mlruns
          mkdir -p data/raw data/processed data/cache
      
      - name: Download sample data
        run: |
          # Create minimal sample data for testing
          python -c "
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Generate sample OHLCV data
dates = pd.date_range(end=datetime.now(), periods=1000, freq='15min')
np.random.seed(42)
data = pd.DataFrame({
    'timestamp': dates,
    'open': 50000 + np.random.randn(1000) * 1000,
    'high': 51000 + np.random.randn(1000) * 1000,
    'low': 49000 + np.random.randn(1000) * 1000,
    'close': 50000 + np.random.randn(1000) * 1000,
    'volume': np.random.exponential(1000, 1000)
})
data['high'] = data[['open', 'high', 'close']].max(axis=1)
data['low'] = data[['open', 'low', 'close']].min(axis=1)
data.to_parquet('data/raw/BTCUSDT_15m.parquet')
print('Sample data created successfully')
"
      
      - name: Test XGBoost training (smoke test)
        run: |
          python -c "
import sys
import os
sys.path.insert(0, os.getcwd())

# Quick smoke test for XGBoost
from src.models.xgb_optuna import XGBoostOptuna
import pandas as pd
import numpy as np

# Load sample data
data = pd.read_parquet('data/raw/BTCUSDT_15m.parquet')

# Create simple features
data['returns'] = data['close'].pct_change()
data['volume_ma'] = data['volume'].rolling(20).mean()
data = data.dropna()

# Split data
n = len(data)
train_size = int(0.7 * n)
val_size = int(0.15 * n)

X = data[['returns', 'volume_ma']].values
y = (data['returns'].shift(-1) > 0).astype(int).values[:-1]
X = X[:-1]

X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]

print(f'Data shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')

# Train minimal model
model = XGBoostOptuna(n_trials=2, seed=42)
model.optimize(X_train, y_train, X_val, y_val)

# Test prediction
pred = model.predict_proba(X_test)
print(f'Predictions shape: {pred.shape}')
print('XGBoost smoke test passed!')
"
        timeout-minutes: 5
      
      - name: Test MLflow tracking
        run: |
          python -c "
import mlflow
import os

# Set tracking URI
mlflow.set_tracking_uri('file://./artifacts/mlruns')

# Create a test run
with mlflow.start_run(run_name='ci_test_run'):
    mlflow.log_param('test_param', 'test_value')
    mlflow.log_metric('test_metric', 0.95)
    mlflow.set_tag('ci_test', 'true')
    
    # Create a dummy model file
    import joblib
    dummy_model = {'type': 'test', 'value': 42}
    joblib.dump(dummy_model, 'test_model.pkl')
    mlflow.log_artifact('test_model.pkl')
    
    print('MLflow tracking test passed!')
    
# Clean up
os.remove('test_model.pkl')
"
      
      - name: Verify determinism
        run: |
          python -c "
import os
import random
import numpy as np

# Set seeds
os.environ['PYTHONHASHSEED'] = '0'
random.seed(42)
np.random.seed(42)

# Test determinism
results = []
for _ in range(3):
    random.seed(42)
    np.random.seed(42)
    r1 = random.random()
    r2 = np.random.random()
    results.append((r1, r2))

# Verify all results are identical
assert len(set(results)) == 1, f'Determinism check failed: {results}'
print('Determinism verification passed!')
"

  # Docker Build Test
  docker-build:
    needs: [lint-and-format]
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./deployment/Dockerfile
          push: false
          tags: crypto-ml:test
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_DATE=${{ github.event.repository.updated_at }}
            VCS_REF=${{ github.sha }}
            MODEL_VERSION=test
      
      - name: Test Docker image
        run: |
          docker run --rm crypto-ml:test python -c "import sys; print('Docker test successful'); sys.exit(0)"

  # Performance Regression Check
  performance-check:
    needs: [ml-validation]
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for comparison
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scikit-learn xgboost
      
      - name: Check for performance regression
        run: |
          # This is a placeholder for actual performance regression testing
          # In production, you would compare metrics from the PR branch vs main
          echo "Performance regression check placeholder"
          echo "TODO: Implement actual performance comparison"
        continue-on-error: true

  # Dependency License Check
  license-check:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install pip-licenses
        run: |
          python -m pip install --upgrade pip
          pip install pip-licenses
      
      - name: Check licenses
        run: |
          pip install -r requirements.txt
          pip-licenses --format=json --output-file=licenses.json
          pip-licenses --fail-on="GPL;LGPL;AGPL;SSPL" || echo "Warning: Found copyleft licenses"
      
      - name: Upload license report
        uses: actions/upload-artifact@v3
        with:
          name: license-report
          path: licenses.json

  # Summary Job
  ci-success:
    needs: [lint-and-format, security, test, integration-test, ml-validation, docker-build]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Check CI Status
        run: |
          if [ "${{ needs.lint-and-format.result }}" == "failure" ] || \
             [ "${{ needs.security.result }}" == "failure" ] || \
             [ "${{ needs.test.result }}" == "failure" ]; then
            echo "CI failed! Check the logs above."
            exit 1
          else
            echo "CI passed successfully!"
          fi