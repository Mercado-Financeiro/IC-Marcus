#!/usr/bin/env python3
"""
Script para testar e demonstrar o MLflow Model Registry com Champion/Challenger.
"""

import sys
import os
from pathlib import Path

# Adicionar src ao path
sys.path.append(str(Path(__file__).parent.parent))

import json
import pandas as pd
from datetime import datetime
from src.mlops.model_registry import ModelRegistry


def demo_model_registry():
    """Demonstra funcionalidades do Model Registry."""
    
    print("üèÜ DEMONSTRA√á√ÉO DO MODEL REGISTRY")
    print("=" * 60)
    
    # Criar inst√¢ncia do registry
    registry = ModelRegistry()
    
    # Simular registro de modelos com diferentes m√©tricas
    print("\n1. Simulando registro de modelos...")
    
    models_data = [
        {
            "name": "crypto_xgboost_BTCUSDT",
            "metrics": {
                "f1_score": 0.62,
                "pr_auc": 0.58, 
                "roc_auc": 0.59,
                "brier_score": 0.24,
                "sharpe_ratio": 1.2,
                "max_drawdown": -0.15,
                "total_return": 0.18
            },
            "tags": {
                "model_type": "xgboost",
                "symbol": "BTCUSDT",
                "timeframe": "15m"
            }
        },
        {
            "name": "crypto_lstm_BTCUSDT", 
            "metrics": {
                "f1_score": 0.58,
                "pr_auc": 0.61,
                "roc_auc": 0.60,
                "brier_score": 0.26,
                "sharpe_ratio": 0.9,
                "max_drawdown": -0.22,
                "total_return": 0.12
            },
            "tags": {
                "model_type": "lstm",
                "symbol": "BTCUSDT", 
                "timeframe": "15m"
            }
        }
    ]
    
    # Para demonstra√ß√£o, simular que temos modelos registrados
    print("‚úÖ Modelos simulados:")
    for model in models_data:
        print(f"  - {model['name']}: F1={model['metrics']['f1_score']:.3f}, Sharpe={model['metrics']['sharpe_ratio']:.2f}")
    
    print("\n2. Testando compara√ß√£o de modelos...")
    
    # Simular compara√ß√£o
    champion_metrics = models_data[0]["metrics"]
    challenger_metrics = {
        "f1_score": 0.65,      # Melhor
        "pr_auc": 0.63,        # Melhor
        "roc_auc": 0.61,       # Melhor
        "brier_score": 0.22,   # Melhor (menor)
        "sharpe_ratio": 1.4,   # Melhor  
        "max_drawdown": -0.12, # Melhor (menos negativo)
        "total_return": 0.22   # Melhor
    }
    
    # Simular compara√ß√£o manual
    comparison_result = simulate_comparison(champion_metrics, challenger_metrics)
    
    print("üìä Compara√ß√£o Champion vs Challenger:")
    print(f"  Champion F1: {champion_metrics['f1_score']:.3f}")
    print(f"  Challenger F1: {challenger_metrics['f1_score']:.3f}")
    print(f"  Improvement: +{(challenger_metrics['f1_score'] - champion_metrics['f1_score']):.3f}")
    
    print(f"\n  Champion Sharpe: {champion_metrics['sharpe_ratio']:.2f}")
    print(f"  Challenger Sharpe: {challenger_metrics['sharpe_ratio']:.2f}") 
    print(f"  Improvement: +{(challenger_metrics['sharpe_ratio'] - champion_metrics['sharpe_ratio']):.2f}")
    
    print(f"\nüìà Recomenda√ß√£o: {comparison_result['recommendation']}")
    
    print("\n3. Testando thresholds de promo√ß√£o...")
    
    # Testar valida√ß√£o de m√©tricas
    valid_metrics = registry._validate_promotion_metrics(challenger_metrics)
    print(f"‚úÖ M√©tricas atendem aos thresholds: {valid_metrics}")
    
    if valid_metrics:
        print("üéâ Challenger qualificado para promo√ß√£o!")
    else:
        print("‚ö†Ô∏è Challenger n√£o atende aos crit√©rios m√≠nimos")
    
    print("\n4. Demonstra√ß√£o de Model Registry workflow...")
    
    workflow_steps = [
        "1. Novo modelo treinado com Optuna",
        "2. Modelo registrado no MLflow Registry", 
        "3. Modelo promovido para Staging (Challenger)",
        "4. Compara√ß√£o autom√°tica com Champion",
        "5. Promo√ß√£o autom√°tica se m√©tricas melhores",
        "6. Rollback dispon√≠vel se problemas detectados"
    ]
    
    for step in workflow_steps:
        print(f"  {step}")
    
    print("\n5. Exemplo de configura√ß√£o em produ√ß√£o...")
    
    production_config = {
        "auto_promotion_enabled": True,
        "promotion_schedule": "daily",
        "minimum_trials": 50,
        "metrics_improvement_threshold": 0.05,
        "rollback_on_alerts": True,
        "notification_channels": ["slack", "email"]
    }
    
    print("‚öôÔ∏è Configura√ß√£o de produ√ß√£o sugerida:")
    for key, value in production_config.items():
        print(f"  {key}: {value}")
    
    print("\n" + "=" * 60)
    print("‚úÖ DEMONSTRA√á√ÉO COMPLETA!")
    print("üîß Para usar em produ√ß√£o:")
    print("  1. Configure MLflow server remoto")
    print("  2. Implemente monitoramento de m√©tricas")
    print("  3. Configure alertas autom√°ticos") 
    print("  4. Teste workflows de rollback")
    
    return True


def simulate_comparison(champion_metrics, challenger_metrics):
    """Simula compara√ß√£o entre champion e challenger."""
    
    better_count = 0
    total_metrics = 0
    
    comparison = {}
    
    for metric in champion_metrics.keys():
        if metric in challenger_metrics:
            champion_value = champion_metrics[metric]
            challenger_value = challenger_metrics[metric]
            
            # Determinar se challenger √© melhor
            if metric in ["brier_score", "max_drawdown"]:
                is_better = challenger_value < champion_value
            else:
                is_better = challenger_value > champion_value
            
            comparison[metric] = {
                "champion": champion_value,
                "challenger": challenger_value, 
                "better": is_better
            }
            
            if is_better:
                better_count += 1
            total_metrics += 1
    
    improvement_ratio = better_count / total_metrics if total_metrics > 0 else 0
    
    if improvement_ratio >= 0.7:
        recommendation = "PROMOTE"
    elif improvement_ratio >= 0.5:
        recommendation = "EVALUATE"
    else:
        recommendation = "REJECT"
    
    return {
        "comparison": comparison,
        "improvement_ratio": improvement_ratio,
        "recommendation": recommendation
    }


def main():
    """Fun√ß√£o principal."""
    
    try:
        demo_model_registry()
        return 0
    except Exception as e:
        print(f"‚ùå Erro na demonstra√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())